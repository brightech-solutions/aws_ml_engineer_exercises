{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker Factorization Machines Exercise\n",
    "\n",
    "This notebook demonstrates Amazon SageMaker's **Factorization Machines** algorithm for classification and regression with sparse data.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare sparse data for Factorization Machines\n",
    "2. How to train a model for recommendation-style problems\n",
    "3. How to interpret predictions\n",
    "\n",
    "## What are Factorization Machines?\n",
    "\n",
    "Factorization Machines are a **supervised** algorithm that:\n",
    "- Captures feature interactions efficiently\n",
    "- Works well with high-dimensional sparse data\n",
    "- Combines linear regression with factorized interaction terms\n",
    "\n",
    "**Key Formula:**\n",
    "```\n",
    "ŷ = w₀ + Σ wᵢxᵢ + Σ Σ <vᵢ, vⱼ>xᵢxⱼ\n",
    "```\n",
    "- w₀: Global bias\n",
    "- wᵢ: Linear feature weights\n",
    "- <vᵢ, vⱼ>: Factorized pairwise interaction\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Application | Description |\n",
    "|-------------|-------------|\n",
    "| Recommendation | Predict user-item interactions |\n",
    "| Click prediction | Ad click-through rate |\n",
    "| Rating prediction | Movie/product ratings |\n",
    "| Sparse classification | High-dimensional categorical data |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"factorization-machines\"\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_USERS = 500\n",
    "NUM_ITEMS = 200\n",
    "NUM_INTERACTIONS = 10000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Generate Synthetic Recommendation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendation_data(num_users=500, num_items=200, num_interactions=10000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic user-item interaction data.\n",
    "    \n",
    "    Creates a binary classification problem (click/no-click).\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate latent factors for users and items\n",
    "    num_factors = 10\n",
    "    user_factors = np.random.randn(num_users, num_factors) * 0.5\n",
    "    item_factors = np.random.randn(num_items, num_factors) * 0.5\n",
    "    \n",
    "    # Generate biases\n",
    "    user_bias = np.random.randn(num_users) * 0.2\n",
    "    item_bias = np.random.randn(num_items) * 0.2\n",
    "    \n",
    "    interactions = []\n",
    "    \n",
    "    for _ in range(num_interactions):\n",
    "        user_id = np.random.randint(0, num_users)\n",
    "        item_id = np.random.randint(0, num_items)\n",
    "        \n",
    "        # Compute interaction score\n",
    "        score = (\n",
    "            user_bias[user_id] + \n",
    "            item_bias[item_id] + \n",
    "            np.dot(user_factors[user_id], item_factors[item_id])\n",
    "        )\n",
    "        \n",
    "        # Convert to probability and sample\n",
    "        prob = 1 / (1 + np.exp(-score))\n",
    "        label = 1 if np.random.random() < prob else 0\n",
    "        \n",
    "        interactions.append({\n",
    "            'user_id': user_id,\n",
    "            'item_id': item_id,\n",
    "            'label': label\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(interactions)\n",
    "\n",
    "# Generate data\n",
    "df = generate_recommendation_data(NUM_USERS, NUM_ITEMS, NUM_INTERACTIONS, RANDOM_STATE)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "np.random.seed(RANDOM_STATE)\n",
    "indices = np.random.permutation(len(df))\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "train_idx = indices[:train_size]\n",
    "test_idx = indices[train_size:]\n",
    "\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "test_df = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for Factorization Machines\n",
    "\n",
    "Factorization Machines expect **sparse** feature representation with one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_features(df, num_users, num_items):\n",
    "    \"\"\"\n",
    "    Create sparse feature matrix with one-hot encoded users and items.\n",
    "    \n",
    "    Feature space: [user_0, ..., user_N, item_0, ..., item_M]\n",
    "    \"\"\"\n",
    "    num_samples = len(df)\n",
    "    num_features = num_users + num_items\n",
    "    \n",
    "    # Create sparse matrix\n",
    "    X = np.zeros((num_samples, num_features), dtype=np.float32)\n",
    "    y = df['label'].values.astype(np.float32)\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        user_idx = int(row['user_id'])\n",
    "        item_idx = num_users + int(row['item_id'])\n",
    "        \n",
    "        X[i, user_idx] = 1.0\n",
    "        X[i, item_idx] = 1.0\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Create features\n",
    "X_train, y_train = create_sparse_features(train_df, NUM_USERS, NUM_ITEMS)\n",
    "X_test, y_test = create_sparse_features(test_df, NUM_USERS, NUM_ITEMS)\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"Feature dimensionality: {X_train.shape[1]}\")\n",
    "print(f\"Sparsity: {1 - np.count_nonzero(X_train) / X_train.size:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV with label first\n",
    "os.makedirs('data/fm', exist_ok=True)\n",
    "\n",
    "train_data = np.column_stack([y_train, X_train])\n",
    "test_data = np.column_stack([y_test, X_test])\n",
    "\n",
    "np.savetxt('data/fm/train.csv', train_data, delimiter=',')\n",
    "np.savetxt('data/fm/test.csv', test_data, delimiter=',')\n",
    "\n",
    "print(\"Data files created:\")\n",
    "for f in os.listdir('data/fm'):\n",
    "    size = os.path.getsize(f'data/fm/{f}') / 1024 / 1024\n",
    "    print(f\"  data/fm/{f} ({size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    s3_key = f\"{PREFIX}/{split}/{split}.csv\"\n",
    "    s3_client.upload_file(f'data/fm/{split}.csv', BUCKET_NAME, s3_key)\n",
    "    print(f\"Uploaded: s3://{BUCKET_NAME}/{s3_key}\")\n",
    "\n",
    "train_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/train\"\n",
    "test_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 4: Train Factorization Machines Model\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `num_factors` | Dimension of factorized interaction | 64 |\n",
    "| `predictor_type` | `binary_classifier` or `regressor` | Required |\n",
    "| `feature_dim` | Number of features | Required |\n",
    "| `epochs` | Training epochs | 1 |\n",
    "| `mini_batch_size` | Batch size | 1000 |\n",
    "| `bias_lr` | Learning rate for bias | 0.1 |\n",
    "| `linear_lr` | Learning rate for linear terms | 0.001 |\n",
    "| `factors_lr` | Learning rate for factor terms | 0.0001 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Factorization Machines container image\n",
    "fm_image = retrieve(\n",
    "    framework='factorization-machines',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"Factorization Machines Image URI: {fm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Factorization Machines estimator\n",
    "fm_estimator = Estimator(\n",
    "    image_uri=fm_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='factorization-machines'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "hyperparameters = {\n",
    "    \"num_factors\": 64,\n",
    "    \"predictor_type\": \"binary_classifier\",\n",
    "    \"feature_dim\": NUM_USERS + NUM_ITEMS,\n",
    "    \"epochs\": 20,\n",
    "    \"mini_batch_size\": 200,\n",
    "    \"bias_lr\": 0.1,\n",
    "    \"linear_lr\": 0.01,\n",
    "    \"factors_lr\": 0.001,\n",
    "    \"bias_init_method\": \"normal\",\n",
    "    \"bias_init_scale\": 0.1,\n",
    "    \"linear_init_method\": \"normal\",\n",
    "    \"linear_init_scale\": 0.1,\n",
    "    \"factors_init_method\": \"normal\",\n",
    "    \"factors_init_scale\": 0.1,\n",
    "}\n",
    "\n",
    "fm_estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "print(\"Factorization Machines hyperparameters:\")\n",
    "for k, v in hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting Factorization Machines training job...\")\n",
    "print(\"This will take approximately 5-10 minutes.\\n\")\n",
    "\n",
    "fm_estimator.fit(\n",
    "    {\n",
    "        'train': train_uri,\n",
    "        'test': test_uri\n",
    "    },\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "job_name = fm_estimator.latest_training_job.name\n",
    "print(f\"Training job completed: {job_name}\")\n",
    "print(f\"Model artifacts: {fm_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 5: Deploy and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "print(\"Deploying Factorization Machines model...\")\n",
    "print(\"This will take approximately 5-7 minutes.\\n\")\n",
    "\n",
    "fm_predictor = fm_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=f'fm-{datetime.now().strftime(\"%Y%m%d%H%M\")}'\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint deployed: {fm_predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Configure predictor\n",
    "fm_predictor.serializer = CSVSerializer()\n",
    "fm_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "def predict(data, predictor, batch_size=500):\n",
    "    \"\"\"\n",
    "    Get predictions.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        response = predictor.predict(batch)\n",
    "        \n",
    "        for pred in response['predictions']:\n",
    "            scores.append(pred['score'])\n",
    "            labels.append(pred['predicted_label'])\n",
    "    \n",
    "    return np.array(scores), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "print(\"Getting predictions...\")\n",
    "scores, y_pred = predict(X_test, fm_predictor)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, scores)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC-ROC:  {auc:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.hist(scores[y_test == 0], bins=50, alpha=0.5, label='Negative', color='blue')\n",
    "ax.hist(scores[y_test == 1], bins=50, alpha=0.5, label='Positive', color='red')\n",
    "ax.axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
    "ax.set_xlabel('Prediction Score')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Score Distribution by True Label')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Step 6: Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "print(f\"Deleting endpoint: {fm_predictor.endpoint_name}\")\n",
    "fm_predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **Data Format**: CSV with label first, sparse one-hot features\n",
    "\n",
    "2. **Key Hyperparameters**:\n",
    "   - `num_factors`: Dimensionality of factorization\n",
    "   - `predictor_type`: binary_classifier or regressor\n",
    "   - Learning rates for bias, linear, and factor terms\n",
    "\n",
    "3. **Output**:\n",
    "   - Classification: score and predicted_label\n",
    "   - Regression: score (predicted value)\n",
    "\n",
    "### Factorization Machines vs Other Algorithms\n",
    "\n",
    "| Aspect | FM | Logistic Regression | Neural Network |\n",
    "|--------|----|--------------------|----------------|\n",
    "| Feature interactions | Automatic | Manual | Learned |\n",
    "| Sparse data | Excellent | Good | OK |\n",
    "| Training speed | Fast | Very fast | Slower |\n",
    "| Interpretability | Medium | High | Low |\n",
    "\n",
    "### Instance Recommendations\n",
    "\n",
    "| Task | Instance Types |\n",
    "|------|----------------|\n",
    "| Training | ml.c5.xlarge, ml.m5.large (CPU recommended) |\n",
    "| Inference | ml.c5.large, ml.m5.large |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Use one-hot encoding for categorical features\n",
    "- Normalize continuous features\n",
    "- Start with `num_factors=64`\n",
    "- Use separate learning rates for different terms\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try regression for rating prediction\n",
    "- Add additional features (user demographics, item attributes)\n",
    "- Compare with other recommendation algorithms\n",
    "- Use for click-through rate prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
