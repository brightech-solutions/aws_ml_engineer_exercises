{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Object Detection Exercise\n",
    "\n",
    "This notebook demonstrates Amazon SageMaker's **Object Detection** algorithm for detecting and localizing objects in images.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare image data with bounding box annotations\n",
    "2. How to configure and understand object detection hyperparameters\n",
    "3. How to interpret bounding box predictions\n",
    "4. How to evaluate detection models using IoU and mAP metrics\n",
    "\n",
    "## What is Object Detection?\n",
    "\n",
    "Object Detection identifies and locates objects within images, providing:\n",
    "- **Class labels**: What objects are present\n",
    "- **Bounding boxes**: Where objects are located (x, y, width, height)\n",
    "- **Confidence scores**: How certain the model is\n",
    "\n",
    "**SageMaker provides two implementations:**\n",
    "- **MXNet-based**: Uses Single Shot Detector (SSD) with VGG/ResNet backbone\n",
    "- **TensorFlow-based**: Uses TensorFlow Hub pretrained models\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Industry | Application |\n",
    "|----------|-------------|\n",
    "| Retail | Product detection, shelf inventory |\n",
    "| Automotive | Pedestrian/vehicle detection, ADAS |\n",
    "| Healthcare | Medical imaging, cell detection |\n",
    "| Security | Surveillance, intrusion detection |\n",
    "| Agriculture | Crop monitoring, pest detection |\n",
    "| Manufacturing | Defect detection, quality control |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ Important: Training Cost Warning\n",
    "\n",
    "<div style=\"background-color: #100f0aff; border: 1px solid #ffc107; border-radius: 5px; padding: 15px; margin: 10px 0;\">\n",
    "\n",
    "### GPU Requirements and Costs\n",
    "\n",
    "**Object Detection training requires GPU instances.** Unlike algorithms like Linear Learner or XGBoost that can train on CPU instances, Object Detection is computationally intensive and requires GPUs.\n",
    "\n",
    "| Instance Type | GPU | Memory | On-Demand Price* |\n",
    "|---------------|-----|--------|------------------|\n",
    "| ml.p2.xlarge | 1x K80 | 12 GB | ~$1.26/hour |\n",
    "| ml.p3.2xlarge | 1x V100 | 16 GB | ~$3.83/hour |\n",
    "| ml.p3.8xlarge | 4x V100 | 64 GB | ~$14.69/hour |\n",
    "| ml.g4dn.xlarge | 1x T4 | 16 GB | ~$0.74/hour |\n",
    "| ml.g4dn.2xlarge | 1x T4 | 32 GB | ~$1.05/hour |\n",
    "| ml.g5.xlarge | 1x A10G | 24 GB | ~$1.41/hour |\n",
    "\n",
    "*Prices are approximate for us-west-2 and subject to change. Check [AWS SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/) for current rates.\n",
    "\n",
    "### Cost Estimation Example\n",
    "\n",
    "Training a typical object detection model:\n",
    "- **30 epochs** with **10,000 images**: ~2-4 hours on ml.p3.2xlarge\n",
    "- **Estimated cost**: $7.66 - $15.32 for training\n",
    "\n",
    "### Cost-Saving Recommendations\n",
    "\n",
    "1. **Use Spot Instances**: Can save up to 70% - add `use_spot_instances=True` to Estimator\n",
    "2. **Start with ml.g4dn.xlarge**: Most cost-effective GPU option (~$0.74/hour)\n",
    "3. **Reduce epochs for experimentation**: Use 5-10 epochs to validate setup before full training\n",
    "4. **Use pretrained models**: `use_pretrained_model=1` requires fewer epochs\n",
    "5. **Monitor training**: Stop early if loss plateaus\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"object-detection\"\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understand Data Formats\n",
    "\n",
    "SageMaker Object Detection supports multiple data formats. Understanding these is critical for successful training.\n",
    "\n",
    "### Format 1: RecordIO (Recommended for Large Datasets)\n",
    "\n",
    "Binary format that packs images and annotations together. Most efficient for training but requires preprocessing.\n",
    "\n",
    "```bash\n",
    "# RecordIO files\n",
    "train.rec\n",
    "train.idx\n",
    "validation.rec\n",
    "validation.idx\n",
    "```\n",
    "\n",
    "### Format 2: Image + JSON Annotation (Easiest to Understand)\n",
    "\n",
    "Separate folders for images and corresponding JSON annotation files.\n",
    "\n",
    "```\n",
    "train/\n",
    "  image001.jpg\n",
    "  image002.jpg\n",
    "train_annotation/\n",
    "  image001.json\n",
    "  image002.json\n",
    "validation/\n",
    "validation_annotation/\n",
    "```\n",
    "\n",
    "### Format 3: Augmented Manifest (For Ground Truth Integration)\n",
    "\n",
    "JSON Lines format with S3 references - ideal when using SageMaker Ground Truth for labeling.\n",
    "\n",
    "```json\n",
    "{\"source-ref\": \"s3://bucket/image.jpg\", \"bounding-box\": {\"annotations\": [...], \"image_size\": [...]}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Annotation Format Deep Dive\n",
    "\n",
    "Each annotation JSON file contains:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"file\": \"image001.jpg\",\n",
    "  \"image_size\": [\n",
    "    {\"width\": 800, \"height\": 600, \"depth\": 3}\n",
    "  ],\n",
    "  \"annotations\": [\n",
    "    {\n",
    "      \"class_id\": 0,\n",
    "      \"left\": 100,    // x-coordinate of top-left corner\n",
    "      \"top\": 200,     // y-coordinate of top-left corner\n",
    "      \"width\": 150,   // box width in pixels\n",
    "      \"height\": 100   // box height in pixels\n",
    "    }\n",
    "  ],\n",
    "  \"categories\": [\n",
    "    {\"class_id\": 0, \"name\": \"dog\"},\n",
    "    {\"class_id\": 1, \"name\": \"cat\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- `class_id` is **0-indexed** (first class is 0, not 1)\n",
    "- Coordinates are in **pixel values**, not normalized (0-1)\n",
    "- `depth` is typically 3 for RGB images\n",
    "- Each image can have multiple annotations (multiple objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Synthetic Data - Limitations and Purpose\n",
    "\n",
    "<div style=\"background-color: #d1ecf1; border: 1px solid #0c5460; border-radius: 5px; padding: 15px; margin: 10px 0;\">\n",
    "\n",
    "### ⚠️ Important: Why We Can't Truly Simulate Object Detection\n",
    "\n",
    "Unlike algorithms like Linear Learner or XGBoost where we can generate synthetic tabular data that follows known patterns, **object detection requires real images** with actual visual features.\n",
    "\n",
    "**Why synthetic data doesn't work for training:**\n",
    "1. **Neural networks learn visual features**: Edges, textures, shapes that exist in real photos\n",
    "2. **Random noise or shapes** don't contain learnable patterns that transfer to real images\n",
    "3. **Bounding boxes are meaningless** without corresponding visual content\n",
    "\n",
    "**What we CAN demonstrate:**\n",
    "- ✅ Annotation format structure\n",
    "- ✅ Data preparation pipeline\n",
    "- ✅ Evaluation metric calculations\n",
    "- ✅ Output parsing and visualization\n",
    "- ✅ Hyperparameter configuration\n",
    "\n",
    "**For actual training, you need:**\n",
    "- Real images with manually labeled bounding boxes\n",
    "- Public datasets like COCO, Pascal VOC, Open Images\n",
    "- SageMaker Ground Truth for custom labeling\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_annotations(num_images=100, num_classes=5, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic object detection annotations to demonstrate the format.\n",
    "    \n",
    "    NOTE: These annotations are for FORMAT DEMONSTRATION ONLY.\n",
    "    Real training requires actual images with meaningful visual content.\n",
    "    \n",
    "    Args:\n",
    "        num_images: Number of synthetic annotation files to generate\n",
    "        num_classes: Number of object classes\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        annotations: List of annotation dictionaries\n",
    "        class_names: List of class names\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    class_names = ['person', 'car', 'dog', 'cat', 'bicycle']\n",
    "    \n",
    "    annotations = []\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Random image dimensions (common sizes)\n",
    "        width = np.random.choice([640, 800, 1024, 1280])\n",
    "        height = np.random.choice([480, 600, 768, 720])\n",
    "        \n",
    "        # Random number of objects per image (1-5)\n",
    "        num_objects = np.random.randint(1, 6)\n",
    "        \n",
    "        objects = []\n",
    "        for _ in range(num_objects):\n",
    "            # Random class assignment\n",
    "            class_id = np.random.randint(0, num_classes)\n",
    "            \n",
    "            # Random bounding box (ensuring it fits within image bounds)\n",
    "            # Typical object sizes range from 10% to 50% of image dimension\n",
    "            box_width = np.random.randint(int(width * 0.1), int(width * 0.4))\n",
    "            box_height = np.random.randint(int(height * 0.1), int(height * 0.4))\n",
    "            left = np.random.randint(0, max(1, width - box_width))\n",
    "            top = np.random.randint(0, max(1, height - box_height))\n",
    "            \n",
    "            objects.append({\n",
    "                \"class_id\": class_id,\n",
    "                \"left\": left,\n",
    "                \"top\": top,\n",
    "                \"width\": box_width,\n",
    "                \"height\": box_height\n",
    "            })\n",
    "        \n",
    "        annotation = {\n",
    "            \"file\": f\"image_{i:04d}.jpg\",\n",
    "            \"image_size\": [{\"width\": width, \"height\": height, \"depth\": 3}],\n",
    "            \"annotations\": objects,\n",
    "            \"categories\": [{\"class_id\": j, \"name\": class_names[j]} for j in range(num_classes)]\n",
    "        }\n",
    "        annotations.append(annotation)\n",
    "    \n",
    "    return annotations, class_names\n",
    "\n",
    "# Generate sample annotations\n",
    "annotations, class_names = generate_synthetic_annotations()\n",
    "\n",
    "print(f\"Generated {len(annotations)} sample annotations\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"\\nSample annotation structure:\")\n",
    "print(json.dumps(annotations[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_annotation(annotation, class_names, title=None):\n",
    "    \"\"\"\n",
    "    Visualize bounding boxes on a canvas representing the image dimensions.\n",
    "    \n",
    "    In real applications, you would overlay these boxes on actual images.\n",
    "    \"\"\"\n",
    "    img_size = annotation['image_size'][0]\n",
    "    width, height = img_size['width'], img_size['height']\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(10, 8))\n",
    "    \n",
    "    # Create representation of image area\n",
    "    ax.set_xlim(0, width)\n",
    "    ax.set_ylim(height, 0)  # Inverted for image coordinates (0,0 at top-left)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_facecolor('#f0f0f0')\n",
    "    \n",
    "    # Color map for different classes\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "    \n",
    "    # Draw each bounding box\n",
    "    for obj in annotation['annotations']:\n",
    "        class_id = obj['class_id']\n",
    "        \n",
    "        # Create rectangle patch\n",
    "        rect = patches.Rectangle(\n",
    "            (obj['left'], obj['top']),\n",
    "            obj['width'],\n",
    "            obj['height'],\n",
    "            linewidth=3,\n",
    "            edgecolor=colors[class_id],\n",
    "            facecolor=colors[class_id],\n",
    "            alpha=0.3\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label\n",
    "        label = f\"{class_names[class_id]}\"\n",
    "        ax.text(\n",
    "            obj['left'], obj['top'] - 5,\n",
    "            label,\n",
    "            color='white',\n",
    "            fontsize=11,\n",
    "            fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor=colors[class_id], alpha=0.8)\n",
    "        )\n",
    "    \n",
    "    # Add legend\n",
    "    legend_patches = [patches.Patch(color=colors[i], label=class_names[i]) \n",
    "                      for i in range(len(class_names))]\n",
    "    ax.legend(handles=legend_patches, loc='upper right')\n",
    "    \n",
    "    ax.set_xlabel('X (pixels)')\n",
    "    ax.set_ylabel('Y (pixels)')\n",
    "    ax.set_title(title or f\"{annotation['file']} ({width}x{height})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize sample annotations\n",
    "visualize_annotation(annotations[0], class_names, \"Sample Annotation Visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of our synthetic annotations\n",
    "def analyze_annotations(annotations, class_names):\n",
    "    \"\"\"Analyze annotation statistics.\"\"\"\n",
    "    \n",
    "    # Count objects per class\n",
    "    class_counts = defaultdict(int)\n",
    "    objects_per_image = []\n",
    "    box_sizes = []\n",
    "    \n",
    "    for ann in annotations:\n",
    "        objects_per_image.append(len(ann['annotations']))\n",
    "        img_area = ann['image_size'][0]['width'] * ann['image_size'][0]['height']\n",
    "        \n",
    "        for obj in ann['annotations']:\n",
    "            class_counts[class_names[obj['class_id']]] += 1\n",
    "            box_area = obj['width'] * obj['height']\n",
    "            box_sizes.append(box_area / img_area * 100)  # As percentage of image\n",
    "    \n",
    "    return class_counts, objects_per_image, box_sizes\n",
    "\n",
    "class_counts, objects_per_image, box_sizes = analyze_annotations(annotations, class_names)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Class distribution\n",
    "axes[0].bar(class_counts.keys(), class_counts.values(), color=plt.cm.tab10(np.linspace(0, 1, 5)))\n",
    "axes[0].set_title('Objects per Class')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Objects per image\n",
    "axes[1].hist(objects_per_image, bins=range(1, 8), edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Objects per Image')\n",
    "axes[1].set_xlabel('Number of Objects')\n",
    "axes[1].set_ylabel('Image Count')\n",
    "\n",
    "# Box sizes\n",
    "axes[2].hist(box_sizes, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[2].set_title('Bounding Box Sizes')\n",
    "axes[2].set_xlabel('Box Area (% of image)')\n",
    "axes[2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total annotations: {sum(class_counts.values())}\")\n",
    "print(f\"Average objects per image: {np.mean(objects_per_image):.1f}\")\n",
    "print(f\"Average box size: {np.mean(box_sizes):.1f}% of image area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Training Configuration and Hyperparameters\n",
    "\n",
    "### Understanding Object Detection Hyperparameters\n",
    "\n",
    "SageMaker's Object Detection algorithm has many hyperparameters. Understanding each one is crucial for successful training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Required Parameters\n",
    "\n",
    "**num_classes** (Required)\n",
    "- The number of distinct object classes to detect\n",
    "- Must match the number of classes in your annotation files\n",
    "- Does NOT include background (handled automatically)\n",
    "- Example: For person, car, dog → `num_classes=3`\n",
    "\n",
    "**num_training_samples** (Required)\n",
    "- Total number of training images\n",
    "- Used for learning rate scheduling and progress tracking\n",
    "- Must match your actual training dataset size\n",
    "- Example: If you have 5000 training images → `num_training_samples=5000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture Parameters\n",
    "\n",
    "**base_network**\n",
    "- The backbone CNN that extracts features from images\n",
    "- Options: `vgg-16`, `resnet-50`\n",
    "- `vgg-16`: Older architecture, faster but less accurate\n",
    "- `resnet-50`: Modern architecture with skip connections, recommended for most cases\n",
    "- ResNet-50 is 50 layers deep with residual connections that help with gradient flow\n",
    "- Default: `vgg-16`\n",
    "- Recommendation: Use `resnet-50` unless you have memory constraints\n",
    "\n",
    "**use_pretrained_model**\n",
    "- Whether to initialize with ImageNet pretrained weights\n",
    "- `1`: Yes - **highly recommended**, especially for smaller datasets\n",
    "- `0`: No - train from scratch (requires much more data and time)\n",
    "- Pretrained models have already learned general visual features (edges, shapes, textures)\n",
    "- Transfer learning: You're fine-tuning these features for your specific objects\n",
    "- Default: `1`\n",
    "\n",
    "**image_shape**\n",
    "- Input image size (images are resized to this)\n",
    "- Options: `300`, `512`\n",
    "- `300`: Faster training/inference, less detail\n",
    "- `512`: Better for detecting small objects, more memory usage\n",
    "- Trade-off: Larger images = better accuracy but slower and more memory\n",
    "- Default: `300`\n",
    "- Recommendation: Use `512` if detecting small objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters\n",
    "\n",
    "**epochs**\n",
    "- Number of complete passes through the training data\n",
    "- More epochs = more learning, but risk of overfitting\n",
    "- Typical range: 10-100 (30 is a good starting point)\n",
    "- With pretrained models, you often need fewer epochs\n",
    "- Monitor validation mAP to detect overfitting (training improves but validation doesn't)\n",
    "- Default: `30`\n",
    "\n",
    "**mini_batch_size**\n",
    "- Number of images processed before updating weights\n",
    "- Larger batches: More stable gradients, better GPU utilization\n",
    "- Smaller batches: More frequent updates, may generalize better\n",
    "- Limited by GPU memory (reduce if you get OOM errors)\n",
    "- Typical range: 8-32 depending on image size and GPU memory\n",
    "- Default: `32`\n",
    "- Rule of thumb: With `image_shape=512`, use batch size 8-16\n",
    "\n",
    "**learning_rate**\n",
    "- How much to adjust weights on each update\n",
    "- Too high: Training oscillates or diverges (loss spikes)\n",
    "- Too low: Training is very slow, may get stuck\n",
    "- With pretrained models, use lower learning rate (0.001) to preserve learned features\n",
    "- From scratch, can use higher learning rate (0.01)\n",
    "- Default: `0.001`\n",
    "\n",
    "**lr_scheduler_step**\n",
    "- Epochs at which to reduce learning rate\n",
    "- Format: comma-separated epoch numbers (e.g., `\"10,20\"`)\n",
    "- Reducing learning rate helps fine-tune as training progresses\n",
    "- Common pattern: reduce at 1/3 and 2/3 of total epochs\n",
    "- Example: For 30 epochs → `\"10,20\"`\n",
    "\n",
    "**lr_scheduler_factor**\n",
    "- Factor to multiply learning rate by at each step\n",
    "- Value of `0.1` means learning rate becomes 10% of previous value\n",
    "- Default: `0.1`\n",
    "- Example: If LR=0.001 and factor=0.1, after first step LR=0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Parameters\n",
    "\n",
    "**optimizer**\n",
    "- Algorithm for updating weights based on gradients\n",
    "- Options: `sgd`, `adam`, `rmsprop`, `adadelta`\n",
    "- `sgd`: Stochastic Gradient Descent - simple, effective with momentum\n",
    "- `adam`: Adaptive learning rates per parameter, often converges faster\n",
    "- Default: `sgd` (recommended with proper learning rate schedule)\n",
    "\n",
    "**momentum**\n",
    "- Used with SGD optimizer\n",
    "- Helps accelerate training by maintaining velocity in consistent directions\n",
    "- Typical value: `0.9`\n",
    "- Higher momentum (0.9-0.99): Faster convergence, may overshoot\n",
    "- Lower momentum (0.5-0.9): More stable, slower\n",
    "- Default: `0.9`\n",
    "\n",
    "**weight_decay**\n",
    "- L2 regularization to prevent overfitting\n",
    "- Adds penalty for large weights: `loss + weight_decay * sum(weights^2)`\n",
    "- Helps model generalize by keeping weights small\n",
    "- Typical range: `0.0001` to `0.001`\n",
    "- Default: `0.0005`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection-Specific Parameters\n",
    "\n",
    "**nms_threshold** (Non-Maximum Suppression Threshold)\n",
    "- Controls how overlapping detections are merged\n",
    "- During inference, model may predict multiple boxes for same object\n",
    "- NMS removes redundant boxes based on IoU overlap\n",
    "- If two boxes overlap more than this threshold, the lower-confidence one is removed\n",
    "- Value range: 0.0 to 1.0\n",
    "- Lower values (0.3): More aggressive suppression, fewer boxes\n",
    "- Higher values (0.7): Keep more overlapping boxes\n",
    "- Default: `0.45`\n",
    "- Use lower values if getting many duplicate detections\n",
    "\n",
    "**overlap_threshold** (Training IoU Threshold)\n",
    "- Minimum IoU between anchor box and ground truth to be considered a match\n",
    "- Used during training to assign labels to anchor boxes\n",
    "- Anchor boxes with IoU ≥ threshold → positive (object)\n",
    "- Anchor boxes with IoU < threshold → negative (background)\n",
    "- Default: `0.5`\n",
    "- Lower values: More lenient matching, may include imprecise boxes\n",
    "- Higher values: Stricter matching, may miss some objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Parameters\n",
    "\n",
    "**kv_store**\n",
    "- Key-value store for distributed training\n",
    "- `device`: Store on GPU (for single GPU)\n",
    "- `dist_sync`: Distributed synchronous (for multi-GPU)\n",
    "- Default: `device`\n",
    "\n",
    "**early_stopping**\n",
    "- Whether to stop training if validation metric stops improving\n",
    "- Helps prevent overfitting and saves training time\n",
    "- Default: `False`\n",
    "\n",
    "**early_stopping_patience**\n",
    "- Number of epochs to wait for improvement before stopping\n",
    "- Only used if `early_stopping=True`\n",
    "- Default: `5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Object Detection container image\n",
    "object_detection_image = retrieve(\n",
    "    framework='object-detection',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"Object Detection Image URI: {object_detection_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete hyperparameter configuration with explanations\n",
    "hyperparameters = {\n",
    "    # === REQUIRED PARAMETERS ===\n",
    "    \"num_classes\": 5,                    # Number of object classes (person, car, dog, cat, bicycle)\n",
    "    \"num_training_samples\": 1000,        # Total training images\n",
    "    \n",
    "    # === NETWORK ARCHITECTURE ===\n",
    "    \"base_network\": \"resnet-50\",         # Feature extractor backbone\n",
    "    \"use_pretrained_model\": 1,           # Transfer learning from ImageNet\n",
    "    \"image_shape\": 512,                  # Input image size (300 or 512)\n",
    "    \n",
    "    # === TRAINING PARAMETERS ===\n",
    "    \"epochs\": 30,                        # Training epochs\n",
    "    \"mini_batch_size\": 16,               # Batch size (reduce if OOM)\n",
    "    \"learning_rate\": 0.001,              # Initial learning rate\n",
    "    \"lr_scheduler_step\": \"10,20\",        # Reduce LR at these epochs\n",
    "    \"lr_scheduler_factor\": 0.1,          # LR multiplier at each step\n",
    "    \n",
    "    # === OPTIMIZER ===\n",
    "    \"optimizer\": \"sgd\",                  # Optimizer algorithm\n",
    "    \"momentum\": 0.9,                     # SGD momentum\n",
    "    \"weight_decay\": 0.0005,              # L2 regularization\n",
    "    \n",
    "    # === DETECTION PARAMETERS ===\n",
    "    \"nms_threshold\": 0.45,               # Non-max suppression threshold\n",
    "    \"overlap_threshold\": 0.5,            # IoU threshold for training\n",
    "}\n",
    "\n",
    "print(\"Object Detection Hyperparameters:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Estimator Configuration\n",
    "# NOTE: Do NOT run training without actual image data!\n",
    "\n",
    "print(\"\"\"\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "                    EXAMPLE ESTIMATOR CONFIGURATION\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "⚠️  WARNING: Running this training job will incur GPU costs!\n",
    "    Estimated cost: $3-15 depending on epochs and instance type.\n",
    "\n",
    "# Standard training (On-Demand)\n",
    "object_detection_estimator = Estimator(\n",
    "    image_uri=object_detection_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',  # GPU required!\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='object-detection',\n",
    "    max_run=3600 * 4,  # 4 hour max runtime\n",
    ")\n",
    "\n",
    "# Cost-saving alternative with Spot Instances (up to 70% savings)\n",
    "object_detection_estimator_spot = Estimator(\n",
    "    image_uri=object_detection_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',  # Most cost-effective GPU\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='object-detection-spot',\n",
    "    use_spot_instances=True,         # Enable Spot pricing\n",
    "    max_wait=3600 * 5,               # Max time to wait for spot capacity\n",
    "    max_run=3600 * 4,                # Max training time\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "object_detection_estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "# Data channels configuration\n",
    "# train: s3://bucket/prefix/train/  (images)\n",
    "# train_annotation: s3://bucket/prefix/train_annotation/  (JSON files)\n",
    "# validation: s3://bucket/prefix/validation/  (images)\n",
    "# validation_annotation: s3://bucket/prefix/validation_annotation/  (JSON files)\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Understanding Model Output\n",
    "\n",
    "The model outputs detections as a NumPy array where each row represents one detection:\n",
    "\n",
    "```\n",
    "[class_id, confidence, x_min, y_min, x_max, y_max]\n",
    "```\n",
    "\n",
    "**Important Notes:**\n",
    "- Coordinates are **normalized** (0.0 to 1.0), not pixel values\n",
    "- `x_min, y_min`: Top-left corner of bounding box\n",
    "- `x_max, y_max`: Bottom-right corner of bounding box\n",
    "- Multiple detections may exist for the same object (before NMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_detection_output(detections, class_names, image_width, image_height, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Parse and filter detection output from SageMaker Object Detection.\n",
    "    \n",
    "    Args:\n",
    "        detections: Model output array, shape (N, 6)\n",
    "                   Each row: [class_id, confidence, x_min, y_min, x_max, y_max]\n",
    "        class_names: List of class names\n",
    "        image_width: Original image width in pixels\n",
    "        image_height: Original image height in pixels\n",
    "        threshold: Minimum confidence score to keep detection\n",
    "    \n",
    "    Returns:\n",
    "        List of detection dictionaries with denormalized coordinates\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for det in detections:\n",
    "        class_id = int(det[0])\n",
    "        confidence = float(det[1])\n",
    "        \n",
    "        # Filter by confidence threshold\n",
    "        if confidence >= threshold:\n",
    "            # Convert normalized coordinates (0-1) to pixel values\n",
    "            x_min = int(det[2] * image_width)\n",
    "            y_min = int(det[3] * image_height)\n",
    "            x_max = int(det[4] * image_width)\n",
    "            y_max = int(det[5] * image_height)\n",
    "            \n",
    "            results.append({\n",
    "                'class_id': class_id,\n",
    "                'class_name': class_names[class_id] if class_id < len(class_names) else f'class_{class_id}',\n",
    "                'confidence': confidence,\n",
    "                'bbox': {\n",
    "                    'x_min': x_min,\n",
    "                    'y_min': y_min,\n",
    "                    'x_max': x_max,\n",
    "                    'y_max': y_max,\n",
    "                    'width': x_max - x_min,\n",
    "                    'height': y_max - y_min\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Sort by confidence descending\n",
    "    results.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Simulate detection output (as model would return)\n",
    "np.random.seed(42)\n",
    "sample_detections = np.array([\n",
    "    [0, 0.95, 0.10, 0.20, 0.35, 0.60],   # person, high confidence\n",
    "    [1, 0.87, 0.50, 0.30, 0.80, 0.65],   # car, good confidence\n",
    "    [2, 0.45, 0.20, 0.50, 0.35, 0.75],   # dog, low confidence (below threshold)\n",
    "    [0, 0.72, 0.60, 0.10, 0.85, 0.45],   # another person\n",
    "    [3, 0.63, 0.05, 0.70, 0.20, 0.90],   # cat\n",
    "    [4, 0.58, 0.40, 0.75, 0.55, 0.95],   # bicycle\n",
    "])\n",
    "\n",
    "# Parse with 0.5 threshold\n",
    "parsed_detections = parse_detection_output(\n",
    "    sample_detections, \n",
    "    class_names, \n",
    "    image_width=800, \n",
    "    image_height=600,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"Parsed detections (confidence threshold = 0.5):\")\n",
    "print(\"=\" * 60)\n",
    "for det in parsed_detections:\n",
    "    bbox = det['bbox']\n",
    "    print(f\"  {det['class_name']:10s} | conf: {det['confidence']:.2f} | \"\n",
    "          f\"box: ({bbox['x_min']}, {bbox['y_min']}) to ({bbox['x_max']}, {bbox['y_max']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(detections, image_width, image_height, class_names, title=\"Detections\"):\n",
    "    \"\"\"\n",
    "    Visualize detection results with bounding boxes and confidence scores.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    \n",
    "    # Create canvas\n",
    "    ax.set_xlim(0, image_width)\n",
    "    ax.set_ylim(image_height, 0)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_facecolor('#e8e8e8')\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "    \n",
    "    for det in detections:\n",
    "        bbox = det['bbox']\n",
    "        class_id = det['class_id']\n",
    "        \n",
    "        # Draw bounding box\n",
    "        rect = patches.Rectangle(\n",
    "            (bbox['x_min'], bbox['y_min']),\n",
    "            bbox['width'],\n",
    "            bbox['height'],\n",
    "            linewidth=3,\n",
    "            edgecolor=colors[class_id],\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label with confidence\n",
    "        label = f\"{det['class_name']}: {det['confidence']:.2f}\"\n",
    "        ax.text(\n",
    "            bbox['x_min'], bbox['y_min'] - 5,\n",
    "            label,\n",
    "            color='white',\n",
    "            fontsize=11,\n",
    "            fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor=colors[class_id], alpha=0.9)\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('X (pixels)')\n",
    "    ax.set_ylabel('Y (pixels)')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Legend\n",
    "    legend_patches = [patches.Patch(color=colors[i], label=class_names[i]) \n",
    "                      for i in range(len(class_names))]\n",
    "    ax.legend(handles=legend_patches, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_detections(parsed_detections, 800, 600, class_names, \"Sample Detection Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Evaluation Metrics Deep Dive\n",
    "\n",
    "Object detection evaluation is more complex than classification because we must consider both:\n",
    "1. **Classification accuracy**: Is the predicted class correct?\n",
    "2. **Localization accuracy**: Is the bounding box position correct?\n",
    "\n",
    "### Key Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union (IoU)\n",
    "\n",
    "IoU measures how well a predicted box overlaps with the ground truth box.\n",
    "\n",
    "```\n",
    "IoU = Area of Intersection / Area of Union\n",
    "```\n",
    "\n",
    "- **IoU = 1.0**: Perfect overlap (identical boxes)\n",
    "- **IoU = 0.5**: Common threshold for \"correct\" detection\n",
    "- **IoU = 0.0**: No overlap at all\n",
    "\n",
    "**Standard Thresholds:**\n",
    "- IoU ≥ 0.5: Traditional PASCAL VOC threshold\n",
    "- IoU ≥ 0.75: Stricter threshold (COCO challenge uses this too)\n",
    "- IoU @ 0.5:0.95: Average over multiple thresholds (COCO primary metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union between two bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        box1, box2: Dicts with x_min, y_min, x_max, y_max\n",
    "    \n",
    "    Returns:\n",
    "        IoU value between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    # Calculate intersection coordinates\n",
    "    x_left = max(box1['x_min'], box2['x_min'])\n",
    "    y_top = max(box1['y_min'], box2['y_min'])\n",
    "    x_right = min(box1['x_max'], box2['x_max'])\n",
    "    y_bottom = min(box1['y_max'], box2['y_max'])\n",
    "    \n",
    "    # Check if boxes actually intersect\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate areas\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    \n",
    "    box1_area = (box1['x_max'] - box1['x_min']) * (box1['y_max'] - box1['y_min'])\n",
    "    box2_area = (box2['x_max'] - box2['x_min']) * (box2['y_max'] - box2['y_min'])\n",
    "    \n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    return intersection_area / union_area if union_area > 0 else 0.0\n",
    "\n",
    "\n",
    "def visualize_iou(box1, box2, title=\"IoU Visualization\"):\n",
    "    \"\"\"\n",
    "    Visualize two bounding boxes and their IoU.\n",
    "    \"\"\"\n",
    "    iou = calculate_iou(box1, box2)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Determine canvas size\n",
    "    all_x = [box1['x_min'], box1['x_max'], box2['x_min'], box2['x_max']]\n",
    "    all_y = [box1['y_min'], box1['y_max'], box2['y_min'], box2['y_max']]\n",
    "    padding = 50\n",
    "    \n",
    "    ax.set_xlim(min(all_x) - padding, max(all_x) + padding)\n",
    "    ax.set_ylim(max(all_y) + padding, min(all_y) - padding)  # Inverted\n",
    "    \n",
    "    # Draw ground truth (blue)\n",
    "    rect1 = patches.Rectangle(\n",
    "        (box1['x_min'], box1['y_min']),\n",
    "        box1['x_max'] - box1['x_min'],\n",
    "        box1['y_max'] - box1['y_min'],\n",
    "        linewidth=3, edgecolor='blue', facecolor='blue', alpha=0.3,\n",
    "        label='Ground Truth'\n",
    "    )\n",
    "    ax.add_patch(rect1)\n",
    "    \n",
    "    # Draw prediction (red)\n",
    "    rect2 = patches.Rectangle(\n",
    "        (box2['x_min'], box2['y_min']),\n",
    "        box2['x_max'] - box2['x_min'],\n",
    "        box2['y_max'] - box2['y_min'],\n",
    "        linewidth=3, edgecolor='red', facecolor='red', alpha=0.3,\n",
    "        label='Prediction'\n",
    "    )\n",
    "    ax.add_patch(rect2)\n",
    "    \n",
    "    # Highlight intersection (green)\n",
    "    x_left = max(box1['x_min'], box2['x_min'])\n",
    "    y_top = max(box1['y_min'], box2['y_min'])\n",
    "    x_right = min(box1['x_max'], box2['x_max'])\n",
    "    y_bottom = min(box1['y_max'], box2['y_max'])\n",
    "    \n",
    "    if x_right > x_left and y_bottom > y_top:\n",
    "        rect_inter = patches.Rectangle(\n",
    "            (x_left, y_top),\n",
    "            x_right - x_left,\n",
    "            y_bottom - y_top,\n",
    "            linewidth=2, edgecolor='green', facecolor='green', alpha=0.5,\n",
    "            label='Intersection'\n",
    "        )\n",
    "        ax.add_patch(rect_inter)\n",
    "    \n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xlabel('X (pixels)')\n",
    "    ax.set_ylabel('Y (pixels)')\n",
    "    ax.set_title(f\"{title}\\nIoU = {iou:.4f}\")\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return iou\n",
    "\n",
    "# Example: Good detection (high IoU)\n",
    "gt_box = {'x_min': 100, 'y_min': 100, 'x_max': 300, 'y_max': 250}\n",
    "pred_box_good = {'x_min': 110, 'y_min': 95, 'x_max': 305, 'y_max': 255}\n",
    "\n",
    "print(\"Example 1: Good Detection\")\n",
    "iou_good = visualize_iou(gt_box, pred_box_good, \"Good Detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Poor detection (low IoU)\n",
    "pred_box_poor = {'x_min': 200, 'y_min': 150, 'x_max': 350, 'y_max': 300}\n",
    "\n",
    "print(\"Example 2: Poor Detection\")\n",
    "iou_poor = visualize_iou(gt_box, pred_box_poor, \"Poor Detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: No overlap\n",
    "pred_box_miss = {'x_min': 400, 'y_min': 100, 'x_max': 500, 'y_max': 200}\n",
    "\n",
    "print(\"Example 3: Missed Detection (No Overlap)\")\n",
    "iou_miss = visualize_iou(gt_box, pred_box_miss, \"Missed Detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall for Object Detection\n",
    "\n",
    "In object detection context:\n",
    "\n",
    "**True Positive (TP)**: Detection with correct class AND IoU ≥ threshold\n",
    "**False Positive (FP)**: Detection that doesn't match any ground truth\n",
    "**False Negative (FN)**: Ground truth object that wasn't detected\n",
    "\n",
    "```\n",
    "Precision = TP / (TP + FP) = \"Of all detections, how many are correct?\"\n",
    "Recall = TP / (TP + FN) = \"Of all ground truth objects, how many did we find?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(predictions, ground_truths, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall for object detection.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of prediction dicts with 'class_id', 'confidence', 'bbox'\n",
    "        ground_truths: List of ground truth dicts with 'class_id', 'bbox'\n",
    "        iou_threshold: Minimum IoU to consider a detection correct\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with TP, FP, FN counts and precision/recall values\n",
    "    \"\"\"\n",
    "    # Sort predictions by confidence (descending)\n",
    "    predictions = sorted(predictions, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # Track which ground truths have been matched\n",
    "    gt_matched = [False] * len(ground_truths)\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for pred in predictions:\n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        # Find best matching ground truth (same class, highest IoU)\n",
    "        for gt_idx, gt in enumerate(ground_truths):\n",
    "            if gt_matched[gt_idx]:\n",
    "                continue  # Already matched\n",
    "            if pred['class_id'] != gt['class_id']:\n",
    "                continue  # Different class\n",
    "            \n",
    "            iou = calculate_iou(pred['bbox'], gt['bbox'])\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = gt_idx\n",
    "        \n",
    "        # Check if this is a valid detection\n",
    "        if best_iou >= iou_threshold:\n",
    "            tp += 1\n",
    "            gt_matched[best_gt_idx] = True\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    # False negatives = unmatched ground truths\n",
    "    fn = sum(1 for matched in gt_matched if not matched)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    }\n",
    "\n",
    "# Example: Simulate predictions and ground truths\n",
    "ground_truths = [\n",
    "    {'class_id': 0, 'bbox': {'x_min': 100, 'y_min': 100, 'x_max': 200, 'y_max': 200}},\n",
    "    {'class_id': 0, 'bbox': {'x_min': 300, 'y_min': 150, 'x_max': 400, 'y_max': 280}},\n",
    "    {'class_id': 1, 'bbox': {'x_min': 500, 'y_min': 200, 'x_max': 650, 'y_max': 350}},\n",
    "]\n",
    "\n",
    "predictions = [\n",
    "    {'class_id': 0, 'confidence': 0.95, 'bbox': {'x_min': 105, 'y_min': 98, 'x_max': 205, 'y_max': 205}},  # Good match\n",
    "    {'class_id': 0, 'confidence': 0.80, 'bbox': {'x_min': 310, 'y_min': 155, 'x_max': 395, 'y_max': 275}},  # Good match\n",
    "    {'class_id': 1, 'confidence': 0.75, 'bbox': {'x_min': 510, 'y_min': 210, 'x_max': 640, 'y_max': 340}},  # Good match\n",
    "    {'class_id': 2, 'confidence': 0.60, 'bbox': {'x_min': 50, 'y_min': 400, 'x_max': 150, 'y_max': 500}},   # False positive\n",
    "]\n",
    "\n",
    "metrics = calculate_precision_recall(predictions, ground_truths, iou_threshold=0.5)\n",
    "\n",
    "print(\"Detection Metrics (IoU threshold = 0.5):\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"  True Positives:  {metrics['tp']}\")\n",
    "print(f\"  False Positives: {metrics['fp']}\")\n",
    "print(f\"  False Negatives: {metrics['fn']}\")\n",
    "print(f\"  Precision:       {metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:          {metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:        {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision (mAP)\n",
    "\n",
    "**mAP is THE primary metric for object detection.**\n",
    "\n",
    "It combines precision and recall across all confidence thresholds:\n",
    "\n",
    "1. **AP (Average Precision)**: Area under the Precision-Recall curve for one class\n",
    "2. **mAP (Mean AP)**: Average of AP across all classes\n",
    "\n",
    "**Common mAP Variants:**\n",
    "- **mAP@0.5**: Using IoU threshold of 0.5 (PASCAL VOC style)\n",
    "- **mAP@0.75**: Stricter IoU threshold\n",
    "- **mAP@[0.5:0.95]**: Average over IoU thresholds 0.5 to 0.95 (COCO primary metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ap(predictions, ground_truths, class_id, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Average Precision for a single class.\n",
    "    \n",
    "    Uses the 11-point interpolation method (PASCAL VOC style).\n",
    "    \"\"\"\n",
    "    # Filter predictions and ground truths for this class\n",
    "    class_preds = [p for p in predictions if p['class_id'] == class_id]\n",
    "    class_gt = [g for g in ground_truths if g['class_id'] == class_id]\n",
    "    \n",
    "    if len(class_gt) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Sort by confidence\n",
    "    class_preds = sorted(class_preds, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    gt_matched = [False] * len(class_gt)\n",
    "    \n",
    "    # Calculate precision-recall pairs\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    tp_cumsum = 0\n",
    "    fp_cumsum = 0\n",
    "    \n",
    "    for pred in class_preds:\n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        for gt_idx, gt in enumerate(class_gt):\n",
    "            if gt_matched[gt_idx]:\n",
    "                continue\n",
    "            iou = calculate_iou(pred['bbox'], gt['bbox'])\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = gt_idx\n",
    "        \n",
    "        if best_iou >= iou_threshold:\n",
    "            tp_cumsum += 1\n",
    "            gt_matched[best_gt_idx] = True\n",
    "        else:\n",
    "            fp_cumsum += 1\n",
    "        \n",
    "        precision = tp_cumsum / (tp_cumsum + fp_cumsum)\n",
    "        recall = tp_cumsum / len(class_gt)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    # 11-point interpolation\n",
    "    ap = 0.0\n",
    "    for t in np.arange(0, 1.1, 0.1):\n",
    "        # Find max precision at recall >= t\n",
    "        prec_at_t = [p for p, r in zip(precisions, recalls) if r >= t]\n",
    "        if prec_at_t:\n",
    "            ap += max(prec_at_t) / 11\n",
    "    \n",
    "    return ap, precisions, recalls\n",
    "\n",
    "\n",
    "def calculate_map(predictions, ground_truths, class_names, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision across all classes.\n",
    "    \"\"\"\n",
    "    aps = []\n",
    "    ap_per_class = {}\n",
    "    \n",
    "    for class_id, class_name in enumerate(class_names):\n",
    "        result = calculate_ap(predictions, ground_truths, class_id, iou_threshold)\n",
    "        if isinstance(result, tuple):\n",
    "            ap = result[0]\n",
    "        else:\n",
    "            ap = result\n",
    "        ap_per_class[class_name] = ap\n",
    "        aps.append(ap)\n",
    "    \n",
    "    # Filter out classes with no ground truth\n",
    "    valid_aps = [ap for ap in aps if ap > 0]\n",
    "    mAP = np.mean(valid_aps) if valid_aps else 0.0\n",
    "    \n",
    "    return mAP, ap_per_class\n",
    "\n",
    "# Generate more comprehensive test data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate ground truths across multiple images\n",
    "all_ground_truths = []\n",
    "all_predictions = []\n",
    "\n",
    "for img_idx in range(20):  # 20 images\n",
    "    # 1-3 objects per image\n",
    "    num_objects = np.random.randint(1, 4)\n",
    "    \n",
    "    for _ in range(num_objects):\n",
    "        class_id = np.random.randint(0, len(class_names))\n",
    "        x_min = np.random.randint(50, 400)\n",
    "        y_min = np.random.randint(50, 300)\n",
    "        \n",
    "        gt = {\n",
    "            'image_id': img_idx,\n",
    "            'class_id': class_id,\n",
    "            'bbox': {\n",
    "                'x_min': x_min,\n",
    "                'y_min': y_min,\n",
    "                'x_max': x_min + np.random.randint(50, 150),\n",
    "                'y_max': y_min + np.random.randint(50, 150)\n",
    "            }\n",
    "        }\n",
    "        all_ground_truths.append(gt)\n",
    "        \n",
    "        # Simulate prediction (with some noise)\n",
    "        if np.random.random() > 0.1:  # 90% detection rate\n",
    "            noise = np.random.randint(-20, 20, 4)\n",
    "            pred = {\n",
    "                'image_id': img_idx,\n",
    "                'class_id': class_id,\n",
    "                'confidence': np.random.uniform(0.5, 0.99),\n",
    "                'bbox': {\n",
    "                    'x_min': max(0, gt['bbox']['x_min'] + noise[0]),\n",
    "                    'y_min': max(0, gt['bbox']['y_min'] + noise[1]),\n",
    "                    'x_max': gt['bbox']['x_max'] + noise[2],\n",
    "                    'y_max': gt['bbox']['y_max'] + noise[3]\n",
    "                }\n",
    "            }\n",
    "            all_predictions.append(pred)\n",
    "    \n",
    "    # Add some false positives\n",
    "    if np.random.random() > 0.7:\n",
    "        false_pos = {\n",
    "            'image_id': img_idx,\n",
    "            'class_id': np.random.randint(0, len(class_names)),\n",
    "            'confidence': np.random.uniform(0.3, 0.6),\n",
    "            'bbox': {\n",
    "                'x_min': np.random.randint(400, 600),\n",
    "                'y_min': np.random.randint(300, 500),\n",
    "                'x_max': np.random.randint(500, 700),\n",
    "                'y_max': np.random.randint(400, 600)\n",
    "            }\n",
    "        }\n",
    "        all_predictions.append(false_pos)\n",
    "\n",
    "# Calculate mAP\n",
    "mAP, ap_per_class = calculate_map(all_predictions, all_ground_truths, class_names, iou_threshold=0.5)\n",
    "\n",
    "print(\"Mean Average Precision Results (IoU=0.5):\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"\\n  mAP@0.5: {mAP:.4f}\\n\")\n",
    "print(\"  Per-class AP:\")\n",
    "for class_name, ap in ap_per_class.items():\n",
    "    print(f\"    {class_name:10s}: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class AP\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "bars = ax.barh(list(ap_per_class.keys()), list(ap_per_class.values()), color=colors)\n",
    "\n",
    "ax.set_xlabel('Average Precision')\n",
    "ax.set_title(f'Per-Class Average Precision (mAP@0.5 = {mAP:.4f})')\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, ap in zip(bars, ap_per_class.values()):\n",
    "    ax.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "           f'{ap:.3f}', va='center')\n",
    "\n",
    "# Add mAP line\n",
    "ax.axvline(x=mAP, color='red', linestyle='--', linewidth=2, label=f'mAP = {mAP:.3f}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Non-Maximum Suppression (NMS) Explained\n",
    "\n",
    "**Problem**: Neural networks often predict multiple overlapping boxes for the same object.\n",
    "\n",
    "**Solution**: NMS removes redundant detections by:\n",
    "1. Sort detections by confidence (highest first)\n",
    "2. Keep the highest confidence detection\n",
    "3. Remove all other detections that overlap significantly (IoU > threshold)\n",
    "4. Repeat for remaining detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_maximum_suppression(detections, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Apply Non-Maximum Suppression to remove overlapping detections.\n",
    "    \n",
    "    Args:\n",
    "        detections: List of dicts with 'class_id', 'confidence', 'bbox'\n",
    "        iou_threshold: Detections with IoU > this are suppressed\n",
    "    \n",
    "    Returns:\n",
    "        List of kept detections after NMS\n",
    "    \"\"\"\n",
    "    if len(detections) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Sort by confidence (descending)\n",
    "    sorted_dets = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    kept = []\n",
    "    \n",
    "    while sorted_dets:\n",
    "        # Keep the highest confidence detection\n",
    "        best = sorted_dets.pop(0)\n",
    "        kept.append(best)\n",
    "        \n",
    "        # Remove detections that overlap too much with 'best'\n",
    "        remaining = []\n",
    "        for det in sorted_dets:\n",
    "            # Only compare same class\n",
    "            if det['class_id'] != best['class_id']:\n",
    "                remaining.append(det)\n",
    "            else:\n",
    "                iou = calculate_iou(det['bbox'], best['bbox'])\n",
    "                if iou < iou_threshold:\n",
    "                    remaining.append(det)  # Keep - not overlapping enough\n",
    "                # else: suppress (don't add to remaining)\n",
    "        \n",
    "        sorted_dets = remaining\n",
    "    \n",
    "    return kept\n",
    "\n",
    "# Demonstrate NMS\n",
    "# Simulate multiple overlapping detections for the same object\n",
    "before_nms = [\n",
    "    {'class_id': 0, 'confidence': 0.95, 'bbox': {'x_min': 100, 'y_min': 100, 'x_max': 200, 'y_max': 200}},\n",
    "    {'class_id': 0, 'confidence': 0.85, 'bbox': {'x_min': 105, 'y_min': 95, 'x_max': 205, 'y_max': 205}},   # Overlaps with first\n",
    "    {'class_id': 0, 'confidence': 0.75, 'bbox': {'x_min': 110, 'y_min': 105, 'x_max': 210, 'y_max': 210}},  # Overlaps with first\n",
    "    {'class_id': 1, 'confidence': 0.90, 'bbox': {'x_min': 400, 'y_min': 150, 'x_max': 550, 'y_max': 300}},   # Different class\n",
    "    {'class_id': 1, 'confidence': 0.70, 'bbox': {'x_min': 410, 'y_min': 155, 'x_max': 545, 'y_max': 295}},  # Overlaps with car\n",
    "]\n",
    "\n",
    "after_nms = non_maximum_suppression(before_nms, iou_threshold=0.5)\n",
    "\n",
    "print(f\"Before NMS: {len(before_nms)} detections\")\n",
    "print(f\"After NMS:  {len(after_nms)} detections\")\n",
    "print(\"\\nKept detections:\")\n",
    "for det in after_nms:\n",
    "    print(f\"  {class_names[det['class_id']]:10s}: confidence {det['confidence']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NMS effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "\n",
    "for ax, detections, title in [(axes[0], before_nms, f\"Before NMS ({len(before_nms)} boxes)\"),\n",
    "                               (axes[1], after_nms, f\"After NMS ({len(after_nms)} boxes)\")]:\n",
    "    ax.set_xlim(0, 700)\n",
    "    ax.set_ylim(400, 0)\n",
    "    ax.set_facecolor('#f0f0f0')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    for det in detections:\n",
    "        bbox = det['bbox']\n",
    "        rect = patches.Rectangle(\n",
    "            (bbox['x_min'], bbox['y_min']),\n",
    "            bbox['x_max'] - bbox['x_min'],\n",
    "            bbox['y_max'] - bbox['y_min'],\n",
    "            linewidth=3,\n",
    "            edgecolor=colors[det['class_id']],\n",
    "            facecolor=colors[det['class_id']],\n",
    "            alpha=0.3\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Label\n",
    "        ax.text(bbox['x_min'], bbox['y_min'] - 5,\n",
    "               f\"{class_names[det['class_id']]}: {det['confidence']:.2f}\",\n",
    "               fontsize=9, color='white',\n",
    "               bbox=dict(boxstyle='round', facecolor=colors[det['class_id']], alpha=0.8))\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNMS removes redundant overlapping boxes of the same class,\")\n",
    "print(\"keeping only the highest confidence detection for each object.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: CloudWatch Training Metrics\n",
    "\n",
    "During training, SageMaker Object Detection emits these metrics to CloudWatch:\n",
    "\n",
    "| Metric | Description | Good Values |\n",
    "|--------|-------------|-------------|\n",
    "| `mAP` | Mean Average Precision on validation set | Higher is better (0-1) |\n",
    "| `smooth_l1` | Bounding box regression loss | Lower is better |\n",
    "| `cross_entropy` | Classification loss | Lower is better |\n",
    "| `total_loss` | Combined loss (smooth_l1 + cross_entropy) | Should decrease over time |\n",
    "\n",
    "### What to Watch For\n",
    "\n",
    "**Healthy Training:**\n",
    "- `total_loss` decreasing over epochs\n",
    "- `mAP` increasing on validation set\n",
    "- Gap between training and validation metrics is small\n",
    "\n",
    "**Overfitting Signs:**\n",
    "- Training loss keeps decreasing but validation mAP plateaus or decreases\n",
    "- Large gap between training and validation performance\n",
    "\n",
    "**Underfitting Signs:**\n",
    "- Both training and validation metrics are poor\n",
    "- Loss decreases very slowly\n",
    "\n",
    "**Learning Rate Issues:**\n",
    "- Loss oscillates wildly → Learning rate too high\n",
    "- Loss decreases very slowly → Learning rate too low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training metrics over epochs\n",
    "np.random.seed(42)\n",
    "epochs = 30\n",
    "\n",
    "# Simulate healthy training curve\n",
    "base_loss = 5.0\n",
    "training_loss = [base_loss * np.exp(-0.1 * e) + np.random.normal(0, 0.1) for e in range(epochs)]\n",
    "validation_loss = [base_loss * np.exp(-0.08 * e) + np.random.normal(0, 0.15) + 0.3 for e in range(epochs)]\n",
    "\n",
    "base_map = 0.1\n",
    "validation_map = [min(0.85, base_map + 0.025 * e + np.random.normal(0, 0.02)) for e in range(epochs)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(range(1, epochs + 1), training_loss, 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0].plot(range(1, epochs + 1), validation_loss, 'r--', label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Total Loss')\n",
    "axes[0].set_title('Training Progress: Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mark LR reduction points\n",
    "for lr_step in [10, 20]:\n",
    "    axes[0].axvline(x=lr_step, color='green', linestyle=':', alpha=0.7, label='LR Reduction' if lr_step == 10 else '')\n",
    "\n",
    "# mAP plot\n",
    "axes[1].plot(range(1, epochs + 1), validation_map, 'g-', label='Validation mAP', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('mAP')\n",
    "axes[1].set_title('Training Progress: Mean Average Precision')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Mark LR reduction points\n",
    "for lr_step in [10, 20]:\n",
    "    axes[1].axvline(x=lr_step, color='green', linestyle=':', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final validation mAP: {validation_map[-1]:.4f}\")\n",
    "print(f\"Final training loss: {training_loss[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "### 1. Data Formats\n",
    "- **RecordIO**: Binary format, most efficient for large datasets\n",
    "- **Image + JSON**: Separate images and annotation files\n",
    "- **Augmented Manifest**: JSON Lines with S3 references\n",
    "\n",
    "### 2. Annotation Structure\n",
    "- Bounding boxes: `left`, `top`, `width`, `height` (pixel values)\n",
    "- Class IDs are 0-indexed\n",
    "- Each image can have multiple objects\n",
    "\n",
    "### 3. Key Hyperparameters\n",
    "| Category | Parameters |\n",
    "|----------|------------|\n",
    "| Architecture | `base_network`, `use_pretrained_model`, `image_shape` |\n",
    "| Training | `epochs`, `mini_batch_size`, `learning_rate` |\n",
    "| Optimizer | `optimizer`, `momentum`, `weight_decay` |\n",
    "| Detection | `nms_threshold`, `overlap_threshold` |\n",
    "\n",
    "### 4. Model Output\n",
    "- Format: `[class_id, confidence, x_min, y_min, x_max, y_max]`\n",
    "- Coordinates are **normalized** (0-1 range)\n",
    "- Apply confidence threshold to filter weak detections\n",
    "\n",
    "### 5. Evaluation Metrics\n",
    "- **IoU**: Measures bounding box overlap quality\n",
    "- **Precision/Recall**: Classification + localization accuracy\n",
    "- **mAP**: Primary metric, averaged across classes and thresholds\n",
    "\n",
    "### 6. Non-Maximum Suppression\n",
    "- Removes overlapping detections for the same object\n",
    "- Controlled by `nms_threshold` hyperparameter\n",
    "\n",
    "### Instance Requirements\n",
    "\n",
    "| Task | Instance Types | Notes |\n",
    "|------|----------------|-------|\n",
    "| Training | ml.g4dn.xlarge, ml.p3.2xlarge, ml.p3.8xlarge | **GPU required** |\n",
    "| Inference | ml.m5.large (CPU), ml.g4dn.xlarge (GPU) | GPU for real-time |\n",
    "\n",
    "### Cost Considerations\n",
    "- Training costs: $5-50+ depending on dataset size and epochs\n",
    "- Use Spot Instances for up to 70% savings\n",
    "- Start with ml.g4dn.xlarge (~$0.74/hour) for cost efficiency\n",
    "\n",
    "### Next Steps\n",
    "1. Obtain real labeled image data (COCO, Pascal VOC, or custom)\n",
    "2. Use SageMaker Ground Truth for custom dataset labeling\n",
    "3. Experiment with different `base_network` and `image_shape` settings\n",
    "4. Monitor CloudWatch metrics during training\n",
    "5. Tune `nms_threshold` based on your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [SageMaker Object Detection Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html)\n",
    "- [Object Detection Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-api-config.html)\n",
    "- [COCO Dataset](https://cocodataset.org/) - Standard object detection benchmark\n",
    "- [Pascal VOC Dataset](http://host.robots.ox.ac.uk/pascal/VOC/) - Classic detection dataset\n",
    "- [SageMaker Ground Truth](https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html) - For custom labeling\n",
    "- [AWS Pricing Calculator](https://calculator.aws/) - Estimate training costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
