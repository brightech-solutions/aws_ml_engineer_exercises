{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker Object Detection Exercise\n",
    "\n",
    "This notebook demonstrates Amazon SageMaker's **Object Detection** algorithm for detecting and localizing objects in images.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare image data for object detection\n",
    "2. How to train an object detection model\n",
    "3. How to interpret bounding box predictions\n",
    "\n",
    "## What is Object Detection?\n",
    "\n",
    "Object Detection identifies and locates objects within images, providing:\n",
    "- **Class labels**: What objects are present\n",
    "- **Bounding boxes**: Where objects are located (x, y, width, height)\n",
    "- **Confidence scores**: How certain the model is\n",
    "\n",
    "**SageMaker provides two implementations:**\n",
    "- **MXNet-based**: Uses Single Shot Detector (SSD) with VGG/ResNet backbone\n",
    "- **TensorFlow-based**: Uses TensorFlow Hub pretrained models\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Industry | Application |\n",
    "|----------|-------------|\n",
    "| Retail | Product detection, inventory management |\n",
    "| Automotive | Pedestrian/vehicle detection |\n",
    "| Healthcare | Medical imaging, cell detection |\n",
    "| Security | Surveillance, intrusion detection |\n",
    "| Agriculture | Crop monitoring, pest detection |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"object-detection\"\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Understand Data Format\n",
    "\n",
    "SageMaker Object Detection supports multiple data formats:\n",
    "\n",
    "### RecordIO Format (Recommended)\n",
    "Binary format with images and annotations packed together.\n",
    "\n",
    "### Image + JSON Annotation Format\n",
    "```\n",
    "train/\n",
    "  image001.jpg\n",
    "  image002.jpg\n",
    "train_annotation/\n",
    "  image001.json\n",
    "  image002.json\n",
    "```\n",
    "\n",
    "### JSON Annotation Format\n",
    "```json\n",
    "{\n",
    "  \"file\": \"image001.jpg\",\n",
    "  \"image_size\": [{\"width\": 800, \"height\": 600, \"depth\": 3}],\n",
    "  \"annotations\": [\n",
    "    {\"class_id\": 0, \"left\": 100, \"top\": 200, \"width\": 150, \"height\": 100},\n",
    "    {\"class_id\": 1, \"left\": 400, \"top\": 300, \"width\": 200, \"height\": 150}\n",
    "  ],\n",
    "  \"categories\": [{\"class_id\": 0, \"name\": \"dog\"}, {\"class_id\": 1, \"name\": \"cat\"}]\n",
    "}\n",
    "```\n",
    "\n",
    "### Augmented Manifest Format\n",
    "JSON Lines with S3 references:\n",
    "```json\n",
    "{\"source-ref\": \"s3://bucket/image.jpg\", \"annotations\": {\"annotations\": [{...}], \"image_size\": [...]}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_annotations(num_images=100, num_classes=5, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic object detection annotations.\n",
    "    \n",
    "    In a real scenario, you would use actual images with labeled bounding boxes.\n",
    "    This demonstrates the annotation format.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    annotations = []\n",
    "    class_names = ['person', 'car', 'dog', 'cat', 'bicycle']\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Random image size\n",
    "        width = np.random.choice([640, 800, 1024])\n",
    "        height = np.random.choice([480, 600, 768])\n",
    "        \n",
    "        # Random number of objects\n",
    "        num_objects = np.random.randint(1, 6)\n",
    "        \n",
    "        objects = []\n",
    "        for _ in range(num_objects):\n",
    "            # Random class\n",
    "            class_id = np.random.randint(0, num_classes)\n",
    "            \n",
    "            # Random bounding box (ensuring it fits in image)\n",
    "            box_width = np.random.randint(50, min(300, width - 50))\n",
    "            box_height = np.random.randint(50, min(300, height - 50))\n",
    "            left = np.random.randint(0, width - box_width)\n",
    "            top = np.random.randint(0, height - box_height)\n",
    "            \n",
    "            objects.append({\n",
    "                \"class_id\": class_id,\n",
    "                \"left\": left,\n",
    "                \"top\": top,\n",
    "                \"width\": box_width,\n",
    "                \"height\": box_height\n",
    "            })\n",
    "        \n",
    "        annotation = {\n",
    "            \"file\": f\"image_{i:04d}.jpg\",\n",
    "            \"image_size\": [{\"width\": width, \"height\": height, \"depth\": 3}],\n",
    "            \"annotations\": objects,\n",
    "            \"categories\": [{\"class_id\": j, \"name\": class_names[j]} for j in range(num_classes)]\n",
    "        }\n",
    "        annotations.append(annotation)\n",
    "    \n",
    "    return annotations, class_names\n",
    "\n",
    "# Generate sample annotations\n",
    "annotations, class_names = generate_synthetic_annotations()\n",
    "\n",
    "print(f\"Generated {len(annotations)} annotations\")\n",
    "print(f\"\\nClasses: {class_names}\")\n",
    "print(f\"\\nSample annotation:\")\n",
    "print(json.dumps(annotations[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_annotation(annotation, class_names):\n",
    "    \"\"\"\n",
    "    Visualize bounding boxes on a blank canvas.\n",
    "    \"\"\"\n",
    "    img_size = annotation['image_size'][0]\n",
    "    width, height = img_size['width'], img_size['height']\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(10, 8))\n",
    "    \n",
    "    # Create blank image\n",
    "    ax.set_xlim(0, width)\n",
    "    ax.set_ylim(height, 0)  # Inverted for image coordinates\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_facecolor('lightgray')\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "    \n",
    "    for obj in annotation['annotations']:\n",
    "        class_id = obj['class_id']\n",
    "        rect = patches.Rectangle(\n",
    "            (obj['left'], obj['top']),\n",
    "            obj['width'],\n",
    "            obj['height'],\n",
    "            linewidth=2,\n",
    "            edgecolor=colors[class_id],\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(\n",
    "            obj['left'], obj['top'] - 5,\n",
    "            class_names[class_id],\n",
    "            color=colors[class_id],\n",
    "            fontsize=12,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    ax.set_title(f\"Sample: {annotation['file']} ({width}x{height})\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a sample\n",
    "visualize_annotation(annotations[0], class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Training Configuration\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `num_classes` | Number of object classes | Required |\n",
    "| `num_training_samples` | Number of training images | Required |\n",
    "| `base_network` | Feature extractor: `vgg-16` or `resnet-50` | vgg-16 |\n",
    "| `use_pretrained_model` | Use ImageNet pretrained weights | 1 |\n",
    "| `epochs` | Training epochs | 30 |\n",
    "| `learning_rate` | Learning rate | 0.001 |\n",
    "| `mini_batch_size` | Batch size | 32 |\n",
    "| `image_shape` | Input image size | 300 |\n",
    "| `nms_threshold` | Non-max suppression threshold | 0.45 |\n",
    "| `overlap_threshold` | IoU threshold for matching | 0.5 |\n",
    "\n",
    "### Instance Requirements\n",
    "\n",
    "**Object Detection requires GPU instances:**\n",
    "- Training: P2, P3, G4dn, G5 families\n",
    "- Inference: CPU (C5, M5) or GPU (P2, P3, G4dn, G5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Object Detection container image\n",
    "object_detection_image = retrieve(\n",
    "    framework='object-detection',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"Object Detection Image URI: {object_detection_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example estimator configuration (for reference - requires actual image data)\n",
    "print(\"\"\"\n",
    "Object Detection Estimator Configuration:\n",
    "=========================================\n",
    "\n",
    "object_detection_estimator = Estimator(\n",
    "    image_uri=object_detection_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',  # GPU required\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='object-detection'\n",
    ")\n",
    "\n",
    "hyperparameters = {\n",
    "    \"num_classes\": 5,\n",
    "    \"num_training_samples\": 1000,\n",
    "    \"base_network\": \"resnet-50\",\n",
    "    \"use_pretrained_model\": 1,\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"lr_scheduler_step\": \"10,20\",\n",
    "    \"lr_scheduler_factor\": 0.1,\n",
    "    \"mini_batch_size\": 16,\n",
    "    \"image_shape\": 512,\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"nms_threshold\": 0.45,\n",
    "    \"overlap_threshold\": 0.5,\n",
    "}\n",
    "\n",
    "Data channels:\n",
    "- train: Training images and annotations\n",
    "- validation: Validation images and annotations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Understanding Model Output\n",
    "\n",
    "The model outputs detections as a list of:\n",
    "```json\n",
    "[\n",
    "  [class_id, confidence, x_min, y_min, x_max, y_max],\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "Coordinates are normalized (0-1 range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_detection_output(detections, class_names, image_width, image_height, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Parse and filter detection output.\n",
    "    \n",
    "    Args:\n",
    "        detections: Model output array\n",
    "        class_names: List of class names\n",
    "        image_width: Original image width\n",
    "        image_height: Original image height\n",
    "        threshold: Confidence threshold\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for det in detections:\n",
    "        class_id = int(det[0])\n",
    "        confidence = det[1]\n",
    "        \n",
    "        if confidence >= threshold:\n",
    "            # Convert normalized coordinates to pixel values\n",
    "            x_min = int(det[2] * image_width)\n",
    "            y_min = int(det[3] * image_height)\n",
    "            x_max = int(det[4] * image_width)\n",
    "            y_max = int(det[5] * image_height)\n",
    "            \n",
    "            results.append({\n",
    "                'class': class_names[class_id],\n",
    "                'confidence': confidence,\n",
    "                'bbox': {'x_min': x_min, 'y_min': y_min, 'x_max': x_max, 'y_max': y_max}\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Simulate detection output\n",
    "sample_detections = np.array([\n",
    "    [0, 0.95, 0.1, 0.2, 0.4, 0.6],   # person, high confidence\n",
    "    [1, 0.87, 0.5, 0.3, 0.8, 0.7],   # car, good confidence\n",
    "    [2, 0.45, 0.2, 0.5, 0.3, 0.7],   # dog, low confidence\n",
    "    [0, 0.72, 0.6, 0.1, 0.9, 0.5],   # another person\n",
    "])\n",
    "\n",
    "parsed = parse_detection_output(\n",
    "    sample_detections, \n",
    "    class_names, \n",
    "    image_width=800, \n",
    "    image_height=600,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"Parsed detections (threshold=0.5):\")\n",
    "for det in parsed:\n",
    "    print(f\"  {det['class']}: {det['confidence']:.2f} at {det['bbox']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation Metrics\n",
    "\n",
    "Object Detection uses these key metrics:\n",
    "\n",
    "### Mean Average Precision (mAP)\n",
    "- Primary metric for object detection\n",
    "- Calculated at different IoU thresholds\n",
    "- mAP@0.5 = mAP at 50% IoU threshold\n",
    "- mAP@[0.5:0.95] = average over IoU 0.5 to 0.95\n",
    "\n",
    "### Intersection over Union (IoU)\n",
    "```\n",
    "IoU = Area of Overlap / Area of Union\n",
    "```\n",
    "\n",
    "### Precision and Recall\n",
    "- **Precision**: How many detections are correct\n",
    "- **Recall**: How many ground truth objects are detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union between two bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        box1, box2: Dicts with x_min, y_min, x_max, y_max\n",
    "    \"\"\"\n",
    "    # Calculate intersection\n",
    "    x_left = max(box1['x_min'], box2['x_min'])\n",
    "    y_top = max(box1['y_min'], box2['y_min'])\n",
    "    x_right = min(box1['x_max'], box2['x_max'])\n",
    "    y_bottom = min(box1['y_max'], box2['y_max'])\n",
    "    \n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    \n",
    "    # Calculate union\n",
    "    box1_area = (box1['x_max'] - box1['x_min']) * (box1['y_max'] - box1['y_min'])\n",
    "    box2_area = (box2['x_max'] - box2['x_min']) * (box2['y_max'] - box2['y_min'])\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    return intersection_area / union_area\n",
    "\n",
    "# Example IoU calculation\n",
    "box1 = {'x_min': 100, 'y_min': 100, 'x_max': 200, 'y_max': 200}\n",
    "box2 = {'x_min': 150, 'y_min': 150, 'x_max': 250, 'y_max': 250}\n",
    "\n",
    "iou = calculate_iou(box1, box2)\n",
    "print(f\"IoU between boxes: {iou:.4f}\")\n",
    "\n",
    "# Visualize IoU\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "rect1 = patches.Rectangle((box1['x_min'], box1['y_min']), \n",
    "                          box1['x_max']-box1['x_min'], \n",
    "                          box1['y_max']-box1['y_min'],\n",
    "                          linewidth=2, edgecolor='blue', facecolor='blue', alpha=0.3, label='Box 1')\n",
    "rect2 = patches.Rectangle((box2['x_min'], box2['y_min']), \n",
    "                          box2['x_max']-box2['x_min'], \n",
    "                          box2['y_max']-box2['y_min'],\n",
    "                          linewidth=2, edgecolor='red', facecolor='red', alpha=0.3, label='Box 2')\n",
    "ax.add_patch(rect1)\n",
    "ax.add_patch(rect2)\n",
    "ax.set_xlim(0, 300)\n",
    "ax.set_ylim(0, 300)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend()\n",
    "ax.set_title(f'IoU = {iou:.4f}')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **Data Formats**:\n",
    "   - RecordIO (recommended for large datasets)\n",
    "   - Image + JSON annotations\n",
    "   - Augmented manifest format\n",
    "\n",
    "2. **Annotation Structure**:\n",
    "   - Bounding boxes: left, top, width, height\n",
    "   - Class IDs (0-indexed)\n",
    "   - Image metadata\n",
    "\n",
    "3. **Model Output**:\n",
    "   - [class_id, confidence, x_min, y_min, x_max, y_max]\n",
    "   - Normalized coordinates (0-1)\n",
    "\n",
    "4. **Key Hyperparameters**:\n",
    "   - `base_network`: VGG-16 or ResNet-50\n",
    "   - `use_pretrained_model`: Transfer learning\n",
    "   - `nms_threshold`: Non-max suppression\n",
    "\n",
    "5. **Evaluation Metrics**:\n",
    "   - mAP (Mean Average Precision)\n",
    "   - IoU (Intersection over Union)\n",
    "\n",
    "### Instance Requirements\n",
    "\n",
    "| Task | Instance Types |\n",
    "|------|----------------|\n",
    "| Training | ml.p2.xlarge, ml.p3.2xlarge, ml.g4dn.xlarge |\n",
    "| Inference | ml.m5.large (CPU) or GPU instances |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Prepare real image data with annotations\n",
    "- Use data augmentation for better generalization\n",
    "- Fine-tune hyperparameters for your use case\n",
    "- Consider TensorFlow Object Detection for more model options"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
