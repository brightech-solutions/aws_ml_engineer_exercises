{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Linear Learner Exercise\n",
    "\n",
    "This notebook walks you through training Amazon SageMaker's **Linear Learner** algorithm for both **classification** and **regression** tasks, using **Batch Transform** for predictions.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare data in Linear Learner's required format\n",
    "2. How to configure and train a Linear Learner model for classification\n",
    "3. How to configure and train a Linear Learner model for regression\n",
    "4. How to use **Batch Transform** for predictions (no endpoint deployment)\n",
    "5. How to evaluate model performance with comprehensive metrics\n",
    "\n",
    "## What is Linear Learner?\n",
    "Linear Learner is a supervised learning algorithm that can be used for both **classification** and **regression** problems. Under the hood, it trains multiple models with different hyperparameters in parallel and selects the best one.\n",
    "\n",
    "Key features:\n",
    "- **Binary/Multiclass Classification**: Logistic regression with various loss functions\n",
    "- **Regression**: Linear regression with L1/L2 regularization\n",
    "- **Automatic model tuning**: Trains multiple models in parallel\n",
    "- **Built-in data normalization**: Handles feature scaling automatically\n",
    "\n",
    "## Why Batch Transform?\n",
    "- **No endpoint costs**: Only pay for compute during the transform job\n",
    "- **Better for batch predictions**: Ideal for evaluating models on test sets\n",
    "- **No cleanup required**: Resources automatically terminate after job completes\n",
    "\n",
    "## Prerequisites\n",
    "- SageMaker notebook instance or Studio\n",
    "- IAM role with S3 and SageMaker permissions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if needed)\n",
    "!pip install -q matplotlib pandas numpy sagemaker boto3 scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.image_uris import retrieve\nfrom sagemaker.estimator import Estimator\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.transformer import Transformer\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    # Classification metrics\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n    precision_recall_curve, average_precision_score, log_loss,\n    balanced_accuracy_score, matthews_corrcoef,\n    # Regression metrics\n    mean_squared_error, mean_absolute_error, r2_score,\n    mean_absolute_percentage_error, explained_variance_score,\n    max_error, median_absolute_error\n)\nimport json\nimport os\nfrom datetime import datetime\nimport time\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure AWS session from environment variables\naws_profile = os.getenv('AWS_PROFILE')\naws_region = os.getenv('AWS_REGION', 'us-west-2')\nsagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n\nif aws_profile:\n    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\nelse:\n    boto3.setup_default_session(region_name=aws_region)\n\n# SageMaker session and role\nsagemaker_session = sagemaker.Session()\n\n# Use environment variable for role, or fall back to execution role if running in SageMaker\nif sagemaker_role:\n    role = sagemaker_role\nelse:\n    role = get_execution_role()\n\nregion = sagemaker_session.boto_region_name\n\nprint(f\"AWS Profile: {aws_profile or 'default'}\")\nprint(f\"SageMaker Role: {role}\")\nprint(f\"Region: {region}\")\nprint(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - MODIFY THESE FOR YOUR ENVIRONMENT\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()  # Or specify your bucket\n",
    "PREFIX = \"linear-learner-exercise\"\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_SAMPLES = 5000\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Synthetic Data\n",
    "\n",
    "We'll generate two types of data:\n",
    "1. **Classification**: Customer churn prediction\n",
    "2. **Regression**: House price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(num_samples=5000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic customer churn data for binary classification.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    feature_names = [\n",
    "        'tenure_months', 'monthly_charges', 'total_charges', 'support_tickets',\n",
    "        'contract_type', 'payment_method', 'online_security', 'tech_support',\n",
    "        'internet_service', 'num_products'\n",
    "    ]\n",
    "    \n",
    "    # Generate features\n",
    "    tenure = np.random.exponential(scale=24, size=num_samples).clip(1, 72)\n",
    "    monthly_charges = np.random.normal(65, 30, num_samples).clip(20, 150)\n",
    "    total_charges = tenure * monthly_charges * np.random.uniform(0.8, 1.0, num_samples)\n",
    "    support_tickets = np.random.poisson(2, num_samples)\n",
    "    contract_type = np.random.choice([0, 1, 2], num_samples, p=[0.5, 0.3, 0.2])\n",
    "    payment_method = np.random.choice([0, 1, 2, 3], num_samples)\n",
    "    online_security = np.random.choice([0, 1], num_samples, p=[0.6, 0.4])\n",
    "    tech_support = np.random.choice([0, 1], num_samples, p=[0.5, 0.5])\n",
    "    internet_service = np.random.choice([0, 1, 2], num_samples, p=[0.2, 0.4, 0.4])\n",
    "    num_products = np.random.choice([1, 2, 3, 4, 5], num_samples, p=[0.3, 0.3, 0.2, 0.15, 0.05])\n",
    "    \n",
    "    X = np.column_stack([\n",
    "        tenure, monthly_charges, total_charges, support_tickets,\n",
    "        contract_type, payment_method, online_security, tech_support,\n",
    "        internet_service, num_products\n",
    "    ])\n",
    "    \n",
    "    # Generate labels based on realistic churn patterns\n",
    "    churn_score = (\n",
    "        -0.05 * tenure + 0.02 * monthly_charges + 0.1 * support_tickets +\n",
    "        -0.5 * contract_type - 0.3 * online_security - 0.3 * tech_support +\n",
    "        0.2 * (internet_service == 2) - 0.1 * num_products +\n",
    "        np.random.normal(0, 0.5, num_samples)\n",
    "    )\n",
    "    \n",
    "    churn_prob = 1 / (1 + np.exp(-churn_score))\n",
    "    y = (np.random.random(num_samples) < churn_prob).astype(float)\n",
    "    \n",
    "    return X.astype(np.float32), y.astype(np.float32), feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression_data(num_samples=5000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic house price data for regression.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    feature_names = [\n",
    "        'sqft', 'bedrooms', 'bathrooms', 'lot_size_acres', 'year_built',\n",
    "        'garage_capacity', 'has_pool', 'distance_to_city', 'school_rating', 'crime_rate'\n",
    "    ]\n",
    "    \n",
    "    sqft = np.random.normal(2000, 800, num_samples).clip(500, 6000)\n",
    "    bedrooms = np.random.choice([1, 2, 3, 4, 5, 6], num_samples, p=[0.05, 0.15, 0.35, 0.30, 0.12, 0.03])\n",
    "    bathrooms = np.minimum(bedrooms, np.random.choice([1, 2, 3, 4], num_samples, p=[0.2, 0.4, 0.3, 0.1]))\n",
    "    lot_size = np.random.exponential(0.3, num_samples).clip(0.1, 5.0)\n",
    "    year_built = np.random.normal(1990, 20, num_samples).clip(1920, 2024).astype(int)\n",
    "    garage_capacity = np.random.choice([0, 1, 2, 3], num_samples, p=[0.1, 0.25, 0.50, 0.15])\n",
    "    has_pool = np.random.choice([0, 1], num_samples, p=[0.75, 0.25])\n",
    "    distance_to_city = np.random.exponential(10, num_samples).clip(1, 50)\n",
    "    school_rating = np.random.uniform(3, 10, num_samples)\n",
    "    crime_rate = np.random.exponential(5, num_samples).clip(0.5, 30)\n",
    "    \n",
    "    X = np.column_stack([\n",
    "        sqft, bedrooms, bathrooms, lot_size, year_built,\n",
    "        garage_capacity, has_pool, distance_to_city, school_rating, crime_rate\n",
    "    ])\n",
    "    \n",
    "    # Generate prices\n",
    "    price = (\n",
    "        50000 + 150 * sqft + 15000 * bedrooms + 20000 * bathrooms +\n",
    "        30000 * lot_size + 1000 * (year_built - 1950) + 15000 * garage_capacity +\n",
    "        40000 * has_pool - 2000 * distance_to_city + 10000 * school_rating -\n",
    "        3000 * crime_rate + np.random.normal(0, 30000, num_samples)\n",
    "    )\n",
    "    \n",
    "    y = np.maximum(price, 50000).astype(np.float32)\n",
    "    \n",
    "    return X.astype(np.float32), y, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate both datasets\n",
    "print(\"Generating Classification Data (Customer Churn)...\")\n",
    "X_clf, y_clf, clf_features = generate_classification_data(NUM_SAMPLES)\n",
    "\n",
    "print(\"Generating Regression Data (House Prices)...\")\n",
    "X_reg, y_reg, reg_features = generate_regression_data(NUM_SAMPLES)\n",
    "\n",
    "print(f\"\\nClassification Dataset:\")\n",
    "print(f\"  Shape: {X_clf.shape}\")\n",
    "print(f\"  Churn Rate: {y_clf.mean()*100:.1f}%\")\n",
    "print(f\"  Features: {clf_features}\")\n",
    "\n",
    "print(f\"\\nRegression Dataset:\")\n",
    "print(f\"  Shape: {X_reg.shape}\")\n",
    "print(f\"  Price Range: ${y_reg.min():,.0f} - ${y_reg.max():,.0f}\")\n",
    "print(f\"  Mean Price: ${y_reg.mean():,.0f}\")\n",
    "print(f\"  Features: {reg_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_ratio=0.2, seed=42):\n",
    "    \"\"\"Split data into train and test sets.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(len(y))\n",
    "    test_size = int(len(y) * test_ratio)\n",
    "    \n",
    "    test_idx = indices[:test_size]\n",
    "    train_idx = indices[test_size:]\n",
    "    \n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "# Split classification data\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = split_data(X_clf, y_clf, TEST_RATIO)\n",
    "\n",
    "# Split regression data\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = split_data(X_reg, y_reg, TEST_RATIO)\n",
    "\n",
    "print(f\"Classification - Train: {len(y_clf_train)}, Test: {len(y_clf_test)}\")\n",
    "print(f\"Regression - Train: {len(y_reg_train)}, Test: {len(y_reg_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification data\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Create DataFrame for easier plotting\n",
    "clf_df = pd.DataFrame(X_clf_train, columns=clf_features)\n",
    "clf_df['churn'] = y_clf_train\n",
    "\n",
    "features_to_plot = ['tenure_months', 'monthly_charges', 'support_tickets', \n",
    "                    'contract_type', 'num_products', 'total_charges']\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    ax = axes.flatten()[idx]\n",
    "    \n",
    "    for label, color, name in [(0, 'blue', 'No Churn'), (1, 'red', 'Churn')]:\n",
    "        subset = clf_df[clf_df['churn'] == label][feature]\n",
    "        ax.hist(subset, bins=30, alpha=0.5, color=color, label=name)\n",
    "    \n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{feature} by Churn Status')\n",
    "\n",
    "plt.suptitle('Customer Churn - Feature Distributions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression data\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "reg_df = pd.DataFrame(X_reg_train, columns=reg_features)\n",
    "reg_df['price'] = y_reg_train\n",
    "\n",
    "features_to_plot = ['sqft', 'bedrooms', 'year_built', \n",
    "                    'distance_to_city', 'school_rating', 'lot_size_acres']\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    ax = axes.flatten()[idx]\n",
    "    ax.scatter(reg_df[feature], reg_df['price'], alpha=0.3, s=5)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Price ($)')\n",
    "    ax.set_title(f'{feature} vs Price')\n",
    "\n",
    "plt.suptitle('House Price - Feature Relationships', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Data for SageMaker Linear Learner\n",
    "\n",
    "Linear Learner accepts data in several formats:\n",
    "1. **CSV**: Label in first column, followed by features (for training)\n",
    "2. **CSV**: Features only (for batch transform inference)\n",
    "\n",
    "We'll prepare both formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local data directory\n",
    "os.makedirs('data/classification', exist_ok=True)\n",
    "os.makedirs('data/regression', exist_ok=True)\n",
    "\n",
    "def save_csv_for_training(X, y, filepath):\n",
    "    \"\"\"Save data in SageMaker Linear Learner CSV format (label first) for training.\"\"\"\n",
    "    data = np.column_stack([y.reshape(-1, 1), X])\n",
    "    np.savetxt(filepath, data, delimiter=',', fmt='%.6f')\n",
    "\n",
    "def save_csv_for_inference(X, filepath):\n",
    "    \"\"\"Save features only for batch transform inference.\"\"\"\n",
    "    np.savetxt(filepath, X, delimiter=',', fmt='%.6f')\n",
    "\n",
    "# Save classification data\n",
    "save_csv_for_training(X_clf_train, y_clf_train, 'data/classification/train.csv')\n",
    "save_csv_for_inference(X_clf_test, 'data/classification/test_features.csv')\n",
    "# Also save test labels locally for evaluation\n",
    "np.savetxt('data/classification/test_labels.csv', y_clf_test, delimiter=',', fmt='%.6f')\n",
    "\n",
    "# Save regression data\n",
    "save_csv_for_training(X_reg_train, y_reg_train, 'data/regression/train.csv')\n",
    "save_csv_for_inference(X_reg_test, 'data/regression/test_features.csv')\n",
    "# Also save test labels locally for evaluation\n",
    "np.savetxt('data/regression/test_labels.csv', y_reg_test, delimiter=',', fmt='%.6f')\n",
    "\n",
    "print(\"Data saved locally:\")\n",
    "print(f\"  Classification train: {os.path.getsize('data/classification/train.csv') / 1024:.1f} KB\")\n",
    "print(f\"  Classification test features: {os.path.getsize('data/classification/test_features.csv') / 1024:.1f} KB\")\n",
    "print(f\"  Regression train: {os.path.getsize('data/regression/train.csv') / 1024:.1f} KB\")\n",
    "print(f\"  Regression test features: {os.path.getsize('data/regression/test_features.csv') / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Classification data\n",
    "clf_train_s3 = f\"{PREFIX}/classification/train/train.csv\"\n",
    "clf_test_s3 = f\"{PREFIX}/classification/test/test_features.csv\"\n",
    "\n",
    "s3_client.upload_file('data/classification/train.csv', BUCKET_NAME, clf_train_s3)\n",
    "s3_client.upload_file('data/classification/test_features.csv', BUCKET_NAME, clf_test_s3)\n",
    "\n",
    "# Regression data\n",
    "reg_train_s3 = f\"{PREFIX}/regression/train/train.csv\"\n",
    "reg_test_s3 = f\"{PREFIX}/regression/test/test_features.csv\"\n",
    "\n",
    "s3_client.upload_file('data/regression/train.csv', BUCKET_NAME, reg_train_s3)\n",
    "s3_client.upload_file('data/regression/test_features.csv', BUCKET_NAME, reg_test_s3)\n",
    "\n",
    "print(\"Data uploaded to S3:\")\n",
    "print(f\"  Classification train: s3://{BUCKET_NAME}/{clf_train_s3}\")\n",
    "print(f\"  Classification test: s3://{BUCKET_NAME}/{clf_test_s3}\")\n",
    "print(f\"  Regression train: s3://{BUCKET_NAME}/{reg_train_s3}\")\n",
    "print(f\"  Regression test: s3://{BUCKET_NAME}/{reg_test_s3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Binary Classification (Customer Churn)\n",
    "\n",
    "## Step 5A: Configure and Train the Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Linear Learner container image\n",
    "linear_learner_image = retrieve(\n",
    "    framework='linear-learner',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"Linear Learner Image URI: {linear_learner_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification estimator\n",
    "clf_estimator = Estimator(\n",
    "    image_uri=linear_learner_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/classification/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='linear-learner-churn'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set classification hyperparameters\n",
    "clf_hyperparameters = {\n",
    "    # Task type\n",
    "    \"predictor_type\": \"binary_classifier\",\n",
    "    \n",
    "    # Number of features\n",
    "    \"feature_dim\": X_clf_train.shape[1],\n",
    "    \n",
    "    # Training parameters\n",
    "    \"epochs\": 15,\n",
    "    \"mini_batch_size\": 200,\n",
    "    \n",
    "    # Regularization\n",
    "    \"l1\": 0.001,\n",
    "    \"wd\": 0.0001,  # L2 weight decay\n",
    "    \n",
    "    # Optimization\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \n",
    "    # Model selection\n",
    "    \"num_models\": \"auto\",  # Train multiple models and select best\n",
    "    \n",
    "    # Normalization (important for linear models!)\n",
    "    \"normalize_data\": \"true\",\n",
    "    \"normalize_label\": \"false\",\n",
    "    \n",
    "    # Binary classification specific\n",
    "    \"binary_classifier_model_selection_criteria\": \"precision_at_target_recall\",\n",
    "    \"target_recall\": 0.9,  # Optimize precision while maintaining 90% recall\n",
    "    \"positive_example_weight_mult\": \"balanced\",  # Handle class imbalance\n",
    "}\n",
    "\n",
    "clf_estimator.set_hyperparameters(**clf_hyperparameters)\n",
    "\n",
    "print(\"Classification Hyperparameters:\")\n",
    "for k, v in clf_hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training input\n",
    "clf_train_input = TrainingInput(\n",
    "    s3_data=f's3://{BUCKET_NAME}/{clf_train_s3}',\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "print(\"Starting classification training job...\")\n",
    "print(\"This will take approximately 3-5 minutes.\\n\")\n",
    "\n",
    "clf_estimator.fit({'train': clf_train_input}, wait=True, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "clf_training_job = clf_estimator.latest_training_job.name\n",
    "print(f\"Classification training job completed: {clf_training_job}\")\n",
    "print(f\"Model artifacts: {clf_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6A: Run Batch Transform for Classification Predictions\n",
    "\n",
    "Instead of deploying an endpoint, we use Batch Transform to get predictions on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer for batch predictions\n",
    "clf_transformer = clf_estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/classification/batch-output',\n",
    "    accept='text/csv',\n",
    "    assemble_with='Line'\n",
    ")\n",
    "\n",
    "print(\"Starting batch transform job for classification...\")\n",
    "print(\"This will take approximately 3-5 minutes.\\n\")\n",
    "\n",
    "# Run batch transform\n",
    "clf_transformer.transform(\n",
    "    data=f's3://{BUCKET_NAME}/{clf_test_s3}',\n",
    "    content_type='text/csv',\n",
    "    split_type='Line',\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBatch transform completed!\")\n",
    "print(f\"Output location: {clf_transformer.output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and parse batch transform results\n",
    "clf_output_key = f\"{PREFIX}/classification/batch-output/test_features.csv.out\"\n",
    "\n",
    "# Download results\n",
    "s3_client.download_file(BUCKET_NAME, clf_output_key, 'data/classification/predictions.csv')\n",
    "\n",
    "# Parse predictions (Linear Learner returns JSON per line for classification)\n",
    "clf_predictions = []\n",
    "clf_scores = []\n",
    "\n",
    "with open('data/classification/predictions.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        pred = json.loads(line.strip())\n",
    "        clf_predictions.append(pred['predicted_label'])\n",
    "        clf_scores.append(pred['score'])\n",
    "\n",
    "clf_predictions = np.array(clf_predictions)\n",
    "clf_scores = np.array(clf_scores)\n",
    "\n",
    "print(f\"Loaded {len(clf_predictions)} predictions\")\n",
    "print(f\"Prediction distribution: {np.bincount(clf_predictions.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7A: Comprehensive Classification Model Evaluation\n",
    "\n",
    "We'll calculate all standard classification metrics and create visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_model(y_true, y_pred, y_scores, class_names=['No Churn', 'Churn']):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of a binary classification model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    y_scores : array-like\n",
    "        Prediction probabilities/scores for the positive class\n",
    "    class_names : list\n",
    "        Names of the classes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision_score(y_true, y_pred)\n",
    "    metrics['recall'] = recall_score(y_true, y_pred)\n",
    "    metrics['f1_score'] = f1_score(y_true, y_pred)\n",
    "    metrics['matthews_corrcoef'] = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    # Probability-based metrics\n",
    "    metrics['roc_auc'] = roc_auc_score(y_true, y_scores)\n",
    "    metrics['avg_precision'] = average_precision_score(y_true, y_scores)\n",
    "    metrics['log_loss'] = log_loss(y_true, y_scores)\n",
    "    \n",
    "    # Confusion matrix components\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['false_negatives'] = fn\n",
    "    metrics['true_positives'] = tp\n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['negative_predictive_value'] = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_classification_report(metrics, title=\"Classification Metrics\"):\n",
    "    \"\"\"Print a formatted classification report.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\" {title}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n--- Core Metrics ---\")\n",
    "    print(f\"  Accuracy:           {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy:  {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  Precision:          {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall (Sensitivity): {metrics['recall']:.4f}\")\n",
    "    print(f\"  Specificity:        {metrics['specificity']:.4f}\")\n",
    "    print(f\"  F1 Score:           {metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Probability-Based Metrics ---\")\n",
    "    print(f\"  ROC AUC:            {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  Average Precision:  {metrics['avg_precision']:.4f}\")\n",
    "    print(f\"  Log Loss:           {metrics['log_loss']:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Additional Metrics ---\")\n",
    "    print(f\"  Matthews Corr Coef: {metrics['matthews_corrcoef']:.4f}\")\n",
    "    print(f\"  Neg Pred Value:     {metrics['negative_predictive_value']:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    print(f\"  True Negatives:  {metrics['true_negatives']}\")\n",
    "    print(f\"  False Positives: {metrics['false_positives']}\")\n",
    "    print(f\"  False Negatives: {metrics['false_negatives']}\")\n",
    "    print(f\"  True Positives:  {metrics['true_positives']}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all classification metrics\n",
    "clf_metrics = evaluate_classification_model(y_clf_test, clf_predictions, clf_scores)\n",
    "print_classification_report(clf_metrics, \"Customer Churn Classification Metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sklearn's classification report for additional detail\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_clf_test, clf_predictions, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "cm = confusion_matrix(y_clf_test, clf_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# Normalized (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: ROC Curve and Precision-Recall Curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_clf_test, clf_scores)\n",
    "axes[0].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC Curve (AUC = {clf_metrics[\"roc_auc\"]:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "axes[0].fill_between(fpr, tpr, alpha=0.2)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_clf_test, clf_scores)\n",
    "axes[1].plot(recall_curve, precision_curve, 'b-', linewidth=2, \n",
    "             label=f'PR Curve (AP = {clf_metrics[\"avg_precision\"]:.3f})')\n",
    "axes[1].axhline(y=y_clf_test.mean(), color='k', linestyle='--', linewidth=1, label='Baseline (Positive Rate)')\n",
    "axes[1].fill_between(recall_curve, precision_curve, alpha=0.2)\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend(loc='lower left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Score Distribution and Threshold Analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Score distribution by actual class\n",
    "axes[0].hist(clf_scores[y_clf_test == 0], bins=50, alpha=0.5, label='No Churn (Actual)', color='blue', density=True)\n",
    "axes[0].hist(clf_scores[y_clf_test == 1], bins=50, alpha=0.5, label='Churn (Actual)', color='red', density=True)\n",
    "axes[0].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Default Threshold (0.5)')\n",
    "axes[0].set_xlabel('Prediction Score (Probability of Churn)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Score Distribution by Actual Class')\n",
    "axes[0].legend()\n",
    "\n",
    "# Threshold analysis\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (clf_scores >= thresh).astype(int)\n",
    "    f1_scores.append(f1_score(y_clf_test, y_pred_thresh, zero_division=0))\n",
    "    precisions.append(precision_score(y_clf_test, y_pred_thresh, zero_division=0))\n",
    "    recalls.append(recall_score(y_clf_test, y_pred_thresh, zero_division=0))\n",
    "\n",
    "axes[1].plot(thresholds, f1_scores, 'g-', linewidth=2, label='F1 Score')\n",
    "axes[1].plot(thresholds, precisions, 'b-', linewidth=2, label='Precision')\n",
    "axes[1].plot(thresholds, recalls, 'r-', linewidth=2, label='Recall')\n",
    "axes[1].axvline(x=0.5, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "axes[1].axvline(x=best_thresh, color='green', linestyle=':', linewidth=2, label=f'Best F1 Threshold ({best_thresh:.2f})')\n",
    "axes[1].set_xlabel('Classification Threshold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Metrics vs Classification Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal threshold for F1 Score: {best_thresh:.3f}\")\n",
    "print(f\"F1 Score at optimal threshold: {max(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Regression (House Price Prediction)\n",
    "\n",
    "## Step 5B: Configure and Train the Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regression estimator\n",
    "reg_estimator = Estimator(\n",
    "    image_uri=linear_learner_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/regression/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='linear-learner-housing'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set regression hyperparameters\n",
    "reg_hyperparameters = {\n",
    "    # Task type\n",
    "    \"predictor_type\": \"regressor\",\n",
    "    \n",
    "    # Number of features\n",
    "    \"feature_dim\": X_reg_train.shape[1],\n",
    "    \n",
    "    # Training parameters\n",
    "    \"epochs\": 15,\n",
    "    \"mini_batch_size\": 200,\n",
    "    \n",
    "    # Regularization\n",
    "    \"l1\": 0.001,\n",
    "    \"wd\": 0.0001,  # L2 weight decay\n",
    "    \n",
    "    # Optimization\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \n",
    "    # Model selection\n",
    "    \"num_models\": \"auto\",\n",
    "    \n",
    "    # Normalization (important for linear models!)\n",
    "    \"normalize_data\": \"true\",\n",
    "    \"normalize_label\": \"true\",  # Normalize labels for regression\n",
    "    \n",
    "    # Loss function\n",
    "    \"loss\": \"squared_loss\",  # Standard MSE loss\n",
    "}\n",
    "\n",
    "reg_estimator.set_hyperparameters(**reg_hyperparameters)\n",
    "\n",
    "print(\"Regression Hyperparameters:\")\n",
    "for k, v in reg_hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training input\n",
    "reg_train_input = TrainingInput(\n",
    "    s3_data=f's3://{BUCKET_NAME}/{reg_train_s3}',\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "print(\"Starting regression training job...\")\n",
    "print(\"This will take approximately 3-5 minutes.\\n\")\n",
    "\n",
    "reg_estimator.fit({'train': reg_train_input}, wait=True, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "reg_training_job = reg_estimator.latest_training_job.name\n",
    "print(f\"Regression training job completed: {reg_training_job}\")\n",
    "print(f\"Model artifacts: {reg_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6B: Run Batch Transform for Regression Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer for batch predictions\n",
    "reg_transformer = reg_estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/regression/batch-output',\n",
    "    accept='text/csv',\n",
    "    assemble_with='Line'\n",
    ")\n",
    "\n",
    "print(\"Starting batch transform job for regression...\")\n",
    "print(\"This will take approximately 3-5 minutes.\\n\")\n",
    "\n",
    "# Run batch transform\n",
    "reg_transformer.transform(\n",
    "    data=f's3://{BUCKET_NAME}/{reg_test_s3}',\n",
    "    content_type='text/csv',\n",
    "    split_type='Line',\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBatch transform completed!\")\n",
    "print(f\"Output location: {reg_transformer.output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and parse batch transform results\n",
    "reg_output_key = f\"{PREFIX}/regression/batch-output/test_features.csv.out\"\n",
    "\n",
    "# Download results\n",
    "s3_client.download_file(BUCKET_NAME, reg_output_key, 'data/regression/predictions.csv')\n",
    "\n",
    "# Parse predictions (Linear Learner returns JSON per line for regression)\n",
    "reg_predictions = []\n",
    "\n",
    "with open('data/regression/predictions.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        pred = json.loads(line.strip())\n",
    "        reg_predictions.append(pred['score'])\n",
    "\n",
    "reg_predictions = np.array(reg_predictions)\n",
    "\n",
    "print(f\"Loaded {len(reg_predictions)} predictions\")\n",
    "print(f\"Prediction range: ${reg_predictions.min():,.0f} - ${reg_predictions.max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7B: Comprehensive Regression Model Evaluation\n",
    "\n",
    "We'll calculate all standard regression metrics and create visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of a regression model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True values\n",
    "    y_pred : array-like\n",
    "        Predicted values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Error metrics\n",
    "    metrics['mse'] = mean_squared_error(y_true, y_pred)\n",
    "    metrics['rmse'] = np.sqrt(metrics['mse'])\n",
    "    metrics['mae'] = mean_absolute_error(y_true, y_pred)\n",
    "    metrics['median_ae'] = median_absolute_error(y_true, y_pred)\n",
    "    metrics['max_error'] = max_error(y_true, y_pred)\n",
    "    \n",
    "    # Percentage-based metrics\n",
    "    metrics['mape'] = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    \n",
    "    # Symmetric MAPE (handles zeros better)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    denominator = np.where(denominator == 0, 1, denominator)  # Avoid division by zero\n",
    "    metrics['smape'] = np.mean(np.abs(y_true - y_pred) / denominator) * 100\n",
    "    \n",
    "    # Explained variance metrics\n",
    "    metrics['r2'] = r2_score(y_true, y_pred)\n",
    "    metrics['explained_variance'] = explained_variance_score(y_true, y_pred)\n",
    "    \n",
    "    # Adjusted R2 (assuming we know the number of features)\n",
    "    n = len(y_true)\n",
    "    p = 10  # number of features\n",
    "    metrics['adjusted_r2'] = 1 - (1 - metrics['r2']) * (n - 1) / (n - p - 1)\n",
    "    \n",
    "    # Residual statistics\n",
    "    residuals = y_true - y_pred\n",
    "    metrics['residual_mean'] = np.mean(residuals)\n",
    "    metrics['residual_std'] = np.std(residuals)\n",
    "    metrics['residual_median'] = np.median(residuals)\n",
    "    \n",
    "    # Within X% accuracy\n",
    "    pct_errors = np.abs(residuals / y_true) * 100\n",
    "    metrics['within_5pct'] = np.mean(pct_errors <= 5) * 100\n",
    "    metrics['within_10pct'] = np.mean(pct_errors <= 10) * 100\n",
    "    metrics['within_20pct'] = np.mean(pct_errors <= 20) * 100\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_regression_report(metrics, value_prefix='$', title=\"Regression Metrics\"):\n",
    "    \"\"\"Print a formatted regression report.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\" {title}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n--- Error Metrics ---\")\n",
    "    print(f\"  RMSE:               {value_prefix}{metrics['rmse']:,.0f}\")\n",
    "    print(f\"  MAE:                {value_prefix}{metrics['mae']:,.0f}\")\n",
    "    print(f\"  Median AE:          {value_prefix}{metrics['median_ae']:,.0f}\")\n",
    "    print(f\"  Max Error:          {value_prefix}{metrics['max_error']:,.0f}\")\n",
    "    print(f\"  MSE:                {metrics['mse']:,.0f}\")\n",
    "    \n",
    "    print(\"\\n--- Percentage Metrics ---\")\n",
    "    print(f\"  MAPE:               {metrics['mape']:.2f}%\")\n",
    "    print(f\"  SMAPE:              {metrics['smape']:.2f}%\")\n",
    "    \n",
    "    print(\"\\n--- Explained Variance ---\")\n",
    "    print(f\"  R-squared (R2):     {metrics['r2']:.4f}\")\n",
    "    print(f\"  Adjusted R2:        {metrics['adjusted_r2']:.4f}\")\n",
    "    print(f\"  Explained Variance: {metrics['explained_variance']:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Residual Statistics ---\")\n",
    "    print(f\"  Mean Residual:      {value_prefix}{metrics['residual_mean']:,.0f}\")\n",
    "    print(f\"  Residual Std Dev:   {value_prefix}{metrics['residual_std']:,.0f}\")\n",
    "    print(f\"  Median Residual:    {value_prefix}{metrics['residual_median']:,.0f}\")\n",
    "    \n",
    "    print(\"\\n--- Prediction Accuracy ---\")\n",
    "    print(f\"  Within 5%:          {metrics['within_5pct']:.1f}%\")\n",
    "    print(f\"  Within 10%:         {metrics['within_10pct']:.1f}%\")\n",
    "    print(f\"  Within 20%:         {metrics['within_20pct']:.1f}%\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all regression metrics\n",
    "reg_metrics = evaluate_regression_model(y_reg_test, reg_predictions)\n",
    "print_regression_report(reg_metrics, value_prefix='$', title=\"House Price Regression Metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Actual vs Predicted with multiple views\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "residuals = y_reg_test - reg_predictions\n",
    "\n",
    "# 1. Actual vs Predicted scatter plot\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(y_reg_test, reg_predictions, alpha=0.3, s=10)\n",
    "min_val = min(y_reg_test.min(), reg_predictions.min())\n",
    "max_val = max(y_reg_test.max(), reg_predictions.max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax.set_xlabel('Actual Price ($)')\n",
    "ax.set_ylabel('Predicted Price ($)')\n",
    "ax.set_title(f'Actual vs Predicted (R2 = {reg_metrics[\"r2\"]:.4f})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuals vs Predicted\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(reg_predictions, residuals, alpha=0.3, s=10)\n",
    "ax.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax.axhline(y=residuals.mean(), color='g', linestyle=':', linewidth=2, label=f'Mean: ${residuals.mean():,.0f}')\n",
    "ax.fill_between([reg_predictions.min(), reg_predictions.max()], \n",
    "                [-2*residuals.std(), -2*residuals.std()],\n",
    "                [2*residuals.std(), 2*residuals.std()], alpha=0.1, color='blue')\n",
    "ax.set_xlabel('Predicted Price ($)')\n",
    "ax.set_ylabel('Residual ($)')\n",
    "ax.set_title('Residual Plot')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residual distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(residuals, bins=50, edgecolor='black', alpha=0.7, density=True)\n",
    "ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "ax.axvline(x=residuals.mean(), color='green', linestyle=':', linewidth=2, \n",
    "           label=f'Mean: ${residuals.mean():,.0f}')\n",
    "# Add normal distribution overlay\n",
    "x_norm = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "from scipy import stats\n",
    "ax.plot(x_norm, stats.norm.pdf(x_norm, residuals.mean(), residuals.std()), \n",
    "        'r-', linewidth=2, label='Normal Fit')\n",
    "ax.set_xlabel('Prediction Error ($)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Distribution of Residuals')\n",
    "ax.legend()\n",
    "\n",
    "# 4. Percentage error distribution\n",
    "ax = axes[1, 1]\n",
    "pct_errors = (residuals / y_reg_test) * 100\n",
    "ax.hist(pct_errors, bins=50, edgecolor='black', alpha=0.7, range=(-50, 50))\n",
    "ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "ax.axvline(x=-10, color='green', linestyle=':', linewidth=1, alpha=0.7)\n",
    "ax.axvline(x=10, color='green', linestyle=':', linewidth=1, alpha=0.7, label='+/- 10%')\n",
    "ax.set_xlabel('Percentage Error (%)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'Percentage Error Distribution (MAPE = {reg_metrics[\"mape\"]:.2f}%)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional visualization: Error by price range\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Create price bins\n",
    "price_bins = pd.cut(y_reg_test, bins=5)\n",
    "bin_labels = [f'${int(b.left/1000)}K-${int(b.right/1000)}K' for b in price_bins.categories]\n",
    "\n",
    "# MAE by price range\n",
    "mae_by_bin = []\n",
    "mape_by_bin = []\n",
    "counts_by_bin = []\n",
    "\n",
    "for i, bin_label in enumerate(price_bins.categories):\n",
    "    mask = price_bins == bin_label\n",
    "    if mask.sum() > 0:\n",
    "        mae_by_bin.append(mean_absolute_error(y_reg_test[mask], reg_predictions[mask]))\n",
    "        mape_by_bin.append(mean_absolute_percentage_error(y_reg_test[mask], reg_predictions[mask]) * 100)\n",
    "        counts_by_bin.append(mask.sum())\n",
    "    else:\n",
    "        mae_by_bin.append(0)\n",
    "        mape_by_bin.append(0)\n",
    "        counts_by_bin.append(0)\n",
    "\n",
    "x = range(len(bin_labels))\n",
    "\n",
    "ax = axes[0]\n",
    "bars = ax.bar(x, mae_by_bin, color='steelblue', edgecolor='black')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bin_labels, rotation=45, ha='right')\n",
    "ax.set_xlabel('Price Range')\n",
    "ax.set_ylabel('Mean Absolute Error ($)')\n",
    "ax.set_title('MAE by Price Range')\n",
    "# Add count labels\n",
    "for i, (bar, count) in enumerate(zip(bars, counts_by_bin)):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'n={count}', \n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax = axes[1]\n",
    "bars = ax.bar(x, mape_by_bin, color='coral', edgecolor='black')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bin_labels, rotation=45, ha='right')\n",
    "ax.set_xlabel('Price Range')\n",
    "ax.set_ylabel('Mean Absolute Percentage Error (%)')\n",
    "ax.set_title('MAPE by Price Range')\n",
    "for i, (bar, count) in enumerate(zip(bars, counts_by_bin)):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'n={count}', \n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q Plot for residuals (normality check)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
    "ax.set_title('Q-Q Plot of Residuals (Normality Check)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test for normality (on a sample if dataset is large)\n",
    "sample_size = min(5000, len(residuals))\n",
    "sample_residuals = np.random.choice(residuals, size=sample_size, replace=False)\n",
    "stat, p_value = stats.shapiro(sample_residuals)\n",
    "print(f\"Shapiro-Wilk Test for Normality:\")\n",
    "print(f\"  Statistic: {stat:.4f}\")\n",
    "print(f\"  P-value: {p_value:.4f}\")\n",
    "print(f\"  Residuals {'appear' if p_value > 0.05 else 'do not appear'} normally distributed (alpha=0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Clean Up S3 Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete S3 data\n",
    "\n",
    "# import boto3\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket = s3.Bucket(BUCKET_NAME)\n",
    "# bucket.objects.filter(Prefix=PREFIX).delete()\n",
    "# print(f\"Deleted all objects under s3://{BUCKET_NAME}/{PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "### Data Format\n",
    "- Linear Learner accepts CSV format with label in the first column (for training)\n",
    "- For batch transform inference, provide features only (no label column)\n",
    "- Normalization is handled automatically by the algorithm\n",
    "\n",
    "### Batch Transform\n",
    "- More cost-effective than endpoints for batch predictions\n",
    "- No cleanup required - resources terminate after job completes\n",
    "- Ideal for model evaluation and offline scoring\n",
    "\n",
    "### Classification Metrics Covered\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| Accuracy | Overall correctness |\n",
    "| Balanced Accuracy | Average of recall for each class |\n",
    "| Precision | True positives / Predicted positives |\n",
    "| Recall (Sensitivity) | True positives / Actual positives |\n",
    "| Specificity | True negatives / Actual negatives |\n",
    "| F1 Score | Harmonic mean of precision and recall |\n",
    "| ROC AUC | Area under the ROC curve |\n",
    "| Average Precision | Area under the precision-recall curve |\n",
    "| Log Loss | Logarithmic loss (probabilistic) |\n",
    "| Matthews Correlation | Correlation between predicted and actual |\n",
    "\n",
    "### Regression Metrics Covered\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| RMSE | Root Mean Square Error |\n",
    "| MAE | Mean Absolute Error |\n",
    "| Median AE | Median Absolute Error (robust to outliers) |\n",
    "| Max Error | Worst case error |\n",
    "| MAPE | Mean Absolute Percentage Error |\n",
    "| SMAPE | Symmetric MAPE |\n",
    "| R-squared | Coefficient of determination |\n",
    "| Adjusted R2 | R2 adjusted for number of predictors |\n",
    "| Explained Variance | Proportion of variance explained |\n",
    "\n",
    "### Key Hyperparameters\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `predictor_type` | binary_classifier, multiclass_classifier, or regressor |\n",
    "| `feature_dim` | Number of features (required) |\n",
    "| `num_models` | Number of parallel models to train |\n",
    "| `normalize_data` | Normalize features (recommended) |\n",
    "| `l1` | L1 regularization strength |\n",
    "| `wd` | L2 regularization (weight decay) |\n",
    "| `learning_rate` | Learning rate |\n",
    "\n",
    "### Best Practices\n",
    "1. **Always normalize data** - Set `normalize_data: true`\n",
    "2. **Use regularization** - Helps prevent overfitting\n",
    "3. **Train multiple models** - Set `num_models: auto` for automatic tuning\n",
    "4. **Use Batch Transform** - More cost-effective than endpoints for evaluation\n",
    "5. **Check multiple metrics** - Don't rely on a single metric for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Linear Learner Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html)\n",
    "- [Linear Learner Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html)\n",
    "- [SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)\n",
    "- [SageMaker Python SDK](https://sagemaker.readthedocs.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}