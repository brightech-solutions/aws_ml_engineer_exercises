{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker BlazingText Exercise\n",
    "\n",
    "This notebook walks you through Amazon SageMaker's **BlazingText** algorithm for both **Word2Vec embeddings** and **text classification**.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to train Word2Vec embeddings using BlazingText\n",
    "2. How to train a text classifier using BlazingText's supervised mode\n",
    "3. How to prepare data in the required formats\n",
    "4. How to use the trained models for inference\n",
    "\n",
    "## What is BlazingText?\n",
    "BlazingText is a highly optimized implementation of:\n",
    "- **Word2Vec**: Creates semantic word embeddings (unsupervised)\n",
    "- **Text Classification**: Fast text classification based on fastText (supervised)\n",
    "\n",
    "**Key Features:**\n",
    "- GPU and multi-core CPU acceleration\n",
    "- Train on 1+ billion words in minutes\n",
    "- Subword embeddings for out-of-vocabulary words\n",
    "- Compatible with Gensim and fastText\n",
    "\n",
    "## Modes of Operation\n",
    "\n",
    "| Mode | Type | Description |\n",
    "|------|------|-------------|\n",
    "| `cbow` | Word2Vec | Continuous Bag of Words - predicts word from context |\n",
    "| `skipgram` | Word2Vec | Predicts context from word |\n",
    "| `batch_skipgram` | Word2Vec | Distributed skipgram across multiple CPUs |\n",
    "| `supervised` | Classification | Text classification (like fastText) |\n",
    "\n",
    "## Prerequisites\n",
    "- SageMaker notebook instance or Studio, or local environment with AWS credentials\n",
    "- IAM role with S3 and SageMaker permissions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Use environment variable for role, or fall back to execution role if running in SageMaker\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"blazingtext\"\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_SAMPLES = 10000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Word2Vec Embeddings\n",
    "\n",
    "## Step 2A: Generate Training Corpus\n",
    "\n",
    "For Word2Vec, we need a large text corpus. We'll generate synthetic sentences that simulate real-world text patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word2vec_corpus(num_sentences=10000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate a synthetic corpus for Word2Vec training.\n",
    "    \n",
    "    Creates sentences with semantic relationships:\n",
    "    - Technology domain\n",
    "    - Sports domain\n",
    "    - Food domain\n",
    "    - Business domain\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Domain vocabularies with semantic relationships\n",
    "    domains = {\n",
    "        'technology': {\n",
    "            'subjects': ['the computer', 'the software', 'the application', 'the system', 'the network',\n",
    "                        'the database', 'the server', 'the algorithm', 'the program', 'the code'],\n",
    "            'verbs': ['processes', 'stores', 'analyzes', 'computes', 'transfers',\n",
    "                     'encrypts', 'optimizes', 'executes', 'compiles', 'debugs'],\n",
    "            'objects': ['the data', 'the files', 'the information', 'the queries', 'the requests',\n",
    "                       'the packets', 'the records', 'the transactions', 'the messages', 'the bytes'],\n",
    "            'adjectives': ['fast', 'secure', 'efficient', 'scalable', 'reliable', 'modern', 'advanced']\n",
    "        },\n",
    "        'sports': {\n",
    "            'subjects': ['the player', 'the team', 'the athlete', 'the coach', 'the champion',\n",
    "                        'the runner', 'the goalkeeper', 'the striker', 'the defender', 'the referee'],\n",
    "            'verbs': ['wins', 'scores', 'plays', 'trains', 'competes',\n",
    "                     'defeats', 'leads', 'practices', 'runs', 'kicks'],\n",
    "            'objects': ['the game', 'the match', 'the tournament', 'the championship', 'the race',\n",
    "                       'the ball', 'the goal', 'the medal', 'the trophy', 'the title'],\n",
    "            'adjectives': ['strong', 'fast', 'skilled', 'talented', 'professional', 'competitive', 'athletic']\n",
    "        },\n",
    "        'food': {\n",
    "            'subjects': ['the chef', 'the restaurant', 'the cook', 'the baker', 'the kitchen',\n",
    "                        'the waiter', 'the menu', 'the recipe', 'the dish', 'the meal'],\n",
    "            'verbs': ['prepares', 'serves', 'cooks', 'bakes', 'tastes',\n",
    "                     'seasons', 'grills', 'fries', 'roasts', 'mixes'],\n",
    "            'objects': ['the food', 'the ingredients', 'the sauce', 'the vegetables', 'the meat',\n",
    "                       'the dessert', 'the bread', 'the soup', 'the salad', 'the wine'],\n",
    "            'adjectives': ['delicious', 'fresh', 'organic', 'healthy', 'tasty', 'gourmet', 'homemade']\n",
    "        },\n",
    "        'business': {\n",
    "            'subjects': ['the company', 'the manager', 'the executive', 'the investor', 'the startup',\n",
    "                        'the entrepreneur', 'the corporation', 'the firm', 'the director', 'the ceo'],\n",
    "            'verbs': ['invests', 'manages', 'grows', 'acquires', 'launches',\n",
    "                     'develops', 'markets', 'sells', 'expands', 'profits'],\n",
    "            'objects': ['the business', 'the market', 'the product', 'the service', 'the brand',\n",
    "                       'the revenue', 'the strategy', 'the customers', 'the profit', 'the shares'],\n",
    "            'adjectives': ['successful', 'innovative', 'profitable', 'global', 'growing', 'competitive', 'strategic']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sentences = []\n",
    "    domain_names = list(domains.keys())\n",
    "    \n",
    "    for _ in range(num_sentences):\n",
    "        # Pick a random domain\n",
    "        domain = domains[np.random.choice(domain_names)]\n",
    "        \n",
    "        # Generate sentence with random structure\n",
    "        structure = np.random.choice(['SVO', 'SVO_adj', 'adj_SVO'])\n",
    "        \n",
    "        subj = np.random.choice(domain['subjects'])\n",
    "        verb = np.random.choice(domain['verbs'])\n",
    "        obj = np.random.choice(domain['objects'])\n",
    "        adj = np.random.choice(domain['adjectives'])\n",
    "        \n",
    "        if structure == 'SVO':\n",
    "            sentence = f\"{subj} {verb} {obj}\"\n",
    "        elif structure == 'SVO_adj':\n",
    "            sentence = f\"{subj} {verb} {adj} {obj.split()[-1]}\"\n",
    "        else:\n",
    "            sentence = f\"{adj} {subj.split()[-1]} {verb} {obj}\"\n",
    "        \n",
    "        sentences.append(sentence.lower())\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word2Vec corpus\n",
    "print(\"Generating Word2Vec training corpus...\")\n",
    "w2v_corpus = generate_word2vec_corpus(NUM_SAMPLES, RANDOM_STATE)\n",
    "\n",
    "print(f\"\\nGenerated {len(w2v_corpus)} sentences\")\n",
    "print(f\"\\nSample sentences:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {w2v_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary\n",
    "all_words = []\n",
    "for sentence in w2v_corpus:\n",
    "    all_words.extend(sentence.split())\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "print(f\"Total words: {len(all_words)}\")\n",
    "print(f\"Unique words: {len(word_counts)}\")\n",
    "print(f\"\\nMost common words:\")\n",
    "for word, count in word_counts.most_common(15):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3A: Prepare Data for Word2Vec\n",
    "\n",
    "BlazingText Word2Vec expects:\n",
    "- Single text file\n",
    "- One sentence per line\n",
    "- Space-separated tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "os.makedirs('data/word2vec', exist_ok=True)\n",
    "\n",
    "# Save corpus to file\n",
    "with open('data/word2vec/corpus.txt', 'w') as f:\n",
    "    for sentence in w2v_corpus:\n",
    "        f.write(sentence + '\\n')\n",
    "\n",
    "print(f\"Corpus saved: data/word2vec/corpus.txt ({os.path.getsize('data/word2vec/corpus.txt') / 1024:.1f} KB)\")\n",
    "\n",
    "# Show sample of file\n",
    "print(\"\\nFile contents (first 5 lines):\")\n",
    "with open('data/word2vec/corpus.txt', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        print(f\"  {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "w2v_s3_path = f\"{PREFIX}/word2vec/train/corpus.txt\"\n",
    "s3_client.upload_file('data/word2vec/corpus.txt', BUCKET_NAME, w2v_s3_path)\n",
    "\n",
    "w2v_train_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/word2vec/train\"\n",
    "print(f\"Data uploaded to: {w2v_train_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 4A: Train Word2Vec Model\n",
    "\n",
    "### Key Word2Vec Hyperparameters\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `mode` | Architecture: `cbow`, `skipgram`, or `batch_skipgram` | Required |\n",
    "| `vector_dim` | Dimension of word vectors | 100 |\n",
    "| `window_size` | Context window size | 5 |\n",
    "| `epochs` | Training passes through data | 5 |\n",
    "| `learning_rate` | Step size for updates | 0.05 |\n",
    "| `min_count` | Minimum word frequency | 5 |\n",
    "| `negative_samples` | Negative samples per word | 5 |\n",
    "| `subwords` | Learn subword embeddings | False |\n",
    "| `min_char` / `max_char` | Character n-gram range for subwords | 3 / 6 |\n",
    "\n",
    "### CBOW vs Skip-gram\n",
    "\n",
    "| Aspect | CBOW | Skip-gram |\n",
    "|--------|------|----------|\n",
    "| Predicts | Word from context | Context from word |\n",
    "| Speed | Faster | Slower |\n",
    "| Rare words | Less accurate | More accurate |\n",
    "| Best for | Large datasets | Smaller datasets, rare words |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BlazingText container image\n",
    "blazingtext_image = retrieve(\n",
    "    framework='blazingtext',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"BlazingText Image URI: {blazingtext_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word2Vec estimator\n",
    "w2v_estimator = Estimator(\n",
    "    image_uri=blazingtext_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',  # CPU is sufficient for small datasets\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/word2vec/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='blazingtext-word2vec'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Word2Vec hyperparameters\n",
    "w2v_hyperparameters = {\n",
    "    \"mode\": \"skipgram\",         # skipgram works better for smaller datasets\n",
    "    \"vector_dim\": 100,          # Embedding dimension\n",
    "    \"window_size\": 5,           # Context window\n",
    "    \"epochs\": 10,               # Training passes\n",
    "    \"learning_rate\": 0.05,      # Learning rate\n",
    "    \"min_count\": 2,             # Minimum word frequency (lower for small corpus)\n",
    "    \"negative_samples\": 5,      # Negative samples per word\n",
    "    \"subwords\": \"True\",         # Learn subword embeddings for OOV words\n",
    "    \"min_char\": 3,              # Min character n-gram\n",
    "    \"max_char\": 6,              # Max character n-gram\n",
    "    \"evaluation\": \"False\",      # Skip WordSim-353 evaluation (our vocab is synthetic)\n",
    "}\n",
    "\n",
    "w2v_estimator.set_hyperparameters(**w2v_hyperparameters)\n",
    "\n",
    "print(\"Word2Vec hyperparameters:\")\n",
    "for k, v in w2v_hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting Word2Vec training job...\")\n",
    "print(\"This will take approximately 3-5 minutes.\\n\")\n",
    "\n",
    "w2v_estimator.fit({'train': w2v_train_uri}, wait=True, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "w2v_job_name = w2v_estimator.latest_training_job.name\n",
    "print(f\"Training job completed: {w2v_job_name}\")\n",
    "print(f\"Model artifacts: {w2v_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 5A: Download and Explore Word Vectors\n",
    "\n",
    "BlazingText produces:\n",
    "- `vectors.txt`: Human-readable word vectors (Gensim compatible)\n",
    "- `vectors.bin`: Binary vectors for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model artifacts\n",
    "import tarfile\n",
    "\n",
    "model_path = w2v_estimator.model_data\n",
    "model_key = model_path.replace(f's3://{BUCKET_NAME}/', '')\n",
    "\n",
    "os.makedirs('models/word2vec', exist_ok=True)\n",
    "s3_client.download_file(BUCKET_NAME, model_key, 'models/word2vec/model.tar.gz')\n",
    "\n",
    "# Extract\n",
    "with tarfile.open('models/word2vec/model.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall('models/word2vec')\n",
    "\n",
    "print(\"Model files:\")\n",
    "for f in os.listdir('models/word2vec'):\n",
    "    size = os.path.getsize(f'models/word2vec/{f}') / 1024\n",
    "    print(f\"  {f} ({size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectors using Gensim (if available)\n",
    "try:\n",
    "    from gensim.models import KeyedVectors\n",
    "    GENSIM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GENSIM_AVAILABLE = False\n",
    "    print(\"Gensim not available. Install with: pip install gensim\")\n",
    "\n",
    "if GENSIM_AVAILABLE and os.path.exists('models/word2vec/vectors.txt'):\n",
    "    # Load word vectors\n",
    "    word_vectors = KeyedVectors.load_word2vec_format('models/word2vec/vectors.txt', binary=False)\n",
    "    \n",
    "    print(f\"Loaded {len(word_vectors)} word vectors\")\n",
    "    print(f\"Vector dimension: {word_vectors.vector_size}\")\n",
    "    \n",
    "    # Show sample vectors\n",
    "    print(\"\\nSample words in vocabulary:\")\n",
    "    for word in list(word_vectors.key_to_index.keys())[:10]:\n",
    "        print(f\"  {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENSIM_AVAILABLE and os.path.exists('models/word2vec/vectors.txt'):\n",
    "    # Test semantic relationships\n",
    "    print(\"Semantic Similarity Tests:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Similar words\n",
    "    test_words = ['computer', 'player', 'chef', 'company']\n",
    "    \n",
    "    for word in test_words:\n",
    "        if word in word_vectors:\n",
    "            similar = word_vectors.most_similar(word, topn=5)\n",
    "            print(f\"\\nWords similar to '{word}':\")\n",
    "            for sim_word, score in similar:\n",
    "                print(f\"  {sim_word}: {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n'{word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENSIM_AVAILABLE and os.path.exists('models/word2vec/vectors.txt'):\n",
    "    # Word analogies (if vocabulary supports it)\n",
    "    print(\"\\nWord Analogy Tests:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Try some analogies\n",
    "    analogies = [\n",
    "        ('computer', 'data', 'chef'),      # computer:data :: chef:?\n",
    "        ('player', 'game', 'chef'),        # player:game :: chef:?\n",
    "        ('company', 'business', 'team'),   # company:business :: team:?\n",
    "    ]\n",
    "    \n",
    "    for w1, w2, w3 in analogies:\n",
    "        if all(w in word_vectors for w in [w1, w2, w3]):\n",
    "            try:\n",
    "                result = word_vectors.most_similar(positive=[w2, w3], negative=[w1], topn=3)\n",
    "                print(f\"\\n{w1}:{w2} :: {w3}:?\")\n",
    "                for word, score in result:\n",
    "                    print(f\"  {word}: {score:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error with analogy: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Text Classification\n",
    "\n",
    "## Step 2B: Generate Classification Dataset\n",
    "\n",
    "We'll create a sentiment classification dataset with labeled text samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(num_samples=5000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic text classification data.\n",
    "    \n",
    "    Labels:\n",
    "    - 1: Positive sentiment\n",
    "    - 2: Negative sentiment\n",
    "    - 3: Neutral/Informational\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Templates for each class\n",
    "    templates = {\n",
    "        1: [  # Positive\n",
    "            \"this is an excellent {noun} that {verb} perfectly\",\n",
    "            \"i absolutely love this {noun} it works great\",\n",
    "            \"amazing {noun} highly recommend to everyone\",\n",
    "            \"best {noun} i have ever used truly outstanding\",\n",
    "            \"fantastic {noun} exceeded all my expectations\",\n",
    "            \"wonderful experience with this {noun} very happy\",\n",
    "            \"great quality {noun} worth every penny spent\",\n",
    "            \"superb {noun} will definitely buy again soon\",\n",
    "            \"perfect {noun} exactly what i was looking for\",\n",
    "            \"brilliant {noun} makes everything so much easier\",\n",
    "        ],\n",
    "        2: [  # Negative\n",
    "            \"terrible {noun} completely waste of money\",\n",
    "            \"this {noun} is awful do not buy it\",\n",
    "            \"worst {noun} i have ever purchased very disappointed\",\n",
    "            \"horrible experience with this {noun} avoid at all costs\",\n",
    "            \"poor quality {noun} broke after one week\",\n",
    "            \"disappointing {noun} does not work as advertised\",\n",
    "            \"awful {noun} returning it immediately for refund\",\n",
    "            \"bad {noun} very unhappy with this purchase\",\n",
    "            \"useless {noun} complete waste of time and money\",\n",
    "            \"defective {noun} stopped working after few days\",\n",
    "        ],\n",
    "        3: [  # Neutral\n",
    "            \"the {noun} arrived yesterday and seems okay\",\n",
    "            \"received the {noun} it is as described\",\n",
    "            \"the {noun} has standard features nothing special\",\n",
    "            \"average {noun} meets basic requirements only\",\n",
    "            \"the {noun} works but nothing impressive\",\n",
    "            \"got the {noun} it does what it should\",\n",
    "            \"standard {noun} no complaints no praises\",\n",
    "            \"the {noun} is acceptable for the price\",\n",
    "            \"ordinary {noun} serves its purpose adequately\",\n",
    "            \"the {noun} functions as expected nothing more\",\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    nouns = ['product', 'service', 'item', 'device', 'software', 'application',\n",
    "             'tool', 'equipment', 'gadget', 'purchase', 'order', 'delivery']\n",
    "    \n",
    "    data = []\n",
    "    labels = [1, 2, 3]\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        label = np.random.choice(labels, p=[0.4, 0.3, 0.3])  # Slight positive bias\n",
    "        template = np.random.choice(templates[label])\n",
    "        noun = np.random.choice(nouns)\n",
    "        \n",
    "        text = template.format(noun=noun, verb='works')\n",
    "        data.append((label, text))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "print(\"Generating text classification data...\")\n",
    "classification_data = generate_classification_data(NUM_SAMPLES, RANDOM_STATE)\n",
    "\n",
    "# Split into train and validation\n",
    "np.random.seed(RANDOM_STATE)\n",
    "np.random.shuffle(classification_data)\n",
    "\n",
    "val_size = int(len(classification_data) * 0.1)\n",
    "val_data = classification_data[:val_size]\n",
    "train_data = classification_data[val_size:]\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Show class distribution\n",
    "train_labels = [d[0] for d in train_data]\n",
    "label_counts = Counter(train_labels)\n",
    "print(f\"\\nClass distribution (training):\")\n",
    "label_names = {1: 'Positive', 2: 'Negative', 3: 'Neutral'}\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"  {label_names[label]}: {count} ({100*count/len(train_data):.1f}%)\")\n",
    "\n",
    "# Show samples\n",
    "print(f\"\\nSample data:\")\n",
    "for label, text in train_data[:6]:\n",
    "    print(f\"  [{label_names[label]}] {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Step 3B: Prepare Data for Text Classification\n",
    "\n",
    "BlazingText supervised mode expects:\n",
    "- One sample per line\n",
    "- Label prefixed with `__label__`\n",
    "- Space-separated tokens\n",
    "\n",
    "Format: `__label__<label> token1 token2 token3 ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "os.makedirs('data/classification', exist_ok=True)\n",
    "\n",
    "def save_blazingtext_classification(data, filepath):\n",
    "    \"\"\"\n",
    "    Save data in BlazingText classification format.\n",
    "    Format: __label__<label> token1 token2 ...\n",
    "    \"\"\"\n",
    "    with open(filepath, 'w') as f:\n",
    "        for label, text in data:\n",
    "            # Normalize text\n",
    "            text = text.lower().strip()\n",
    "            # Write in BlazingText format\n",
    "            f.write(f\"__label__{label} {text}\\n\")\n",
    "\n",
    "# Save training and validation data\n",
    "save_blazingtext_classification(train_data, 'data/classification/train.txt')\n",
    "save_blazingtext_classification(val_data, 'data/classification/validation.txt')\n",
    "\n",
    "print(\"Data saved:\")\n",
    "print(f\"  - data/classification/train.txt ({os.path.getsize('data/classification/train.txt') / 1024:.1f} KB)\")\n",
    "print(f\"  - data/classification/validation.txt ({os.path.getsize('data/classification/validation.txt') / 1024:.1f} KB)\")\n",
    "\n",
    "# Show sample of file\n",
    "print(\"\\nFile contents (first 5 lines):\")\n",
    "with open('data/classification/train.txt', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        print(f\"  {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "clf_train_path = f\"{PREFIX}/classification/train/train.txt\"\n",
    "clf_val_path = f\"{PREFIX}/classification/validation/validation.txt\"\n",
    "\n",
    "s3_client.upload_file('data/classification/train.txt', BUCKET_NAME, clf_train_path)\n",
    "s3_client.upload_file('data/classification/validation.txt', BUCKET_NAME, clf_val_path)\n",
    "\n",
    "clf_train_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/classification/train\"\n",
    "clf_val_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/classification/validation\"\n",
    "\n",
    "print(\"Data uploaded to S3:\")\n",
    "print(f\"  Train: {clf_train_uri}\")\n",
    "print(f\"  Validation: {clf_val_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Step 4B: Train Text Classification Model\n",
    "\n",
    "### Key Classification Hyperparameters\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `mode` | Must be `supervised` | Required |\n",
    "| `vector_dim` | Embedding dimension | 100 |\n",
    "| `epochs` | Training passes | 5 |\n",
    "| `learning_rate` | Step size | 0.05 |\n",
    "| `min_count` | Minimum word frequency | 5 |\n",
    "| `word_ngrams` | N-gram features (1=unigrams, 2=bigrams) | 2 |\n",
    "| `early_stopping` | Stop when validation accuracy plateaus | False |\n",
    "| `patience` | Epochs to wait before early stopping | 4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification estimator\n",
    "clf_estimator = Estimator(\n",
    "    image_uri=blazingtext_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/classification/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='blazingtext-classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set classification hyperparameters\n",
    "clf_hyperparameters = {\n",
    "    \"mode\": \"supervised\",\n",
    "    \"vector_dim\": 100,\n",
    "    \"epochs\": 15,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"min_count\": 2,\n",
    "    \"word_ngrams\": 2,          # Use bigrams for better accuracy\n",
    "    \"early_stopping\": \"True\",\n",
    "    \"patience\": 4,\n",
    "    \"min_epochs\": 5,\n",
    "}\n",
    "\n",
    "clf_estimator.set_hyperparameters(**clf_hyperparameters)\n",
    "\n",
    "print(\"Classification hyperparameters:\")\n",
    "for k, v in clf_hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with validation channel for early stopping\n",
    "print(\"Starting text classification training job...\")\n",
    "print(\"This will take approximately 3-5 minutes.\\n\")\n",
    "\n",
    "clf_estimator.fit(\n",
    "    {\n",
    "        'train': clf_train_uri,\n",
    "        'validation': clf_val_uri\n",
    "    },\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "clf_job_name = clf_estimator.latest_training_job.name\n",
    "print(f\"Training job completed: {clf_job_name}\")\n",
    "print(f\"Model artifacts: {clf_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Step 5B: Deploy and Test Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "print(\"Deploying classification model...\")\n",
    "print(\"This will take approximately 5-7 minutes.\\n\")\n",
    "\n",
    "clf_predictor = clf_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=f'blazingtext-clf-{datetime.now().strftime(\"%Y%m%d%H%M\")}'\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint deployed: {clf_predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Configure predictor\n",
    "clf_predictor.serializer = JSONSerializer()\n",
    "clf_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "def classify_text(texts, k=1):\n",
    "    \"\"\"\n",
    "    Classify text using the deployed model.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        k: Number of top predictions to return\n",
    "    \"\"\"\n",
    "    # Normalize texts\n",
    "    texts = [t.lower().strip() for t in texts]\n",
    "    \n",
    "    payload = {\n",
    "        \"instances\": texts,\n",
    "        \"configuration\": {\"k\": k}\n",
    "    }\n",
    "    \n",
    "    response = clf_predictor.predict(payload)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classification\n",
    "test_texts = [\n",
    "    \"this product is absolutely amazing i love it\",\n",
    "    \"terrible quality waste of money do not buy\",\n",
    "    \"the item arrived and it works as expected\",\n",
    "    \"best purchase ever highly recommended to everyone\",\n",
    "    \"awful experience the product broke immediately\",\n",
    "    \"average product nothing special about it\",\n",
    "]\n",
    "\n",
    "label_names = {1: 'Positive', 2: 'Negative', 3: 'Neutral'}\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = classify_text(test_texts, k=3)\n",
    "\n",
    "for text, result in zip(test_texts, results):\n",
    "    labels = result['label']\n",
    "    probs = result['prob']\n",
    "    \n",
    "    # Get top prediction\n",
    "    top_label = int(labels[0].replace('__label__', ''))\n",
    "    top_prob = probs[0]\n",
    "    \n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Prediction: {label_names[top_label]} ({top_prob:.2%})\")\n",
    "    \n",
    "    # Show all probabilities\n",
    "    print(\"  All scores:\")\n",
    "    for lbl, prob in zip(labels, probs):\n",
    "        lbl_num = int(lbl.replace('__label__', ''))\n",
    "        print(f\"    {label_names[lbl_num]}: {prob:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "## Step 6B: Evaluate Classification Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_texts = [d[1] for d in val_data]\n",
    "val_labels = [d[0] for d in val_data]\n",
    "\n",
    "# Get predictions in batches\n",
    "batch_size = 100\n",
    "all_predictions = []\n",
    "\n",
    "print(\"Evaluating on validation set...\")\n",
    "for i in range(0, len(val_texts), batch_size):\n",
    "    batch = val_texts[i:i+batch_size]\n",
    "    results = classify_text(batch, k=1)\n",
    "    \n",
    "    for result in results:\n",
    "        pred_label = int(result['label'][0].replace('__label__', ''))\n",
    "        all_predictions.append(pred_label)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(val_labels, all_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    val_labels, \n",
    "    all_predictions, \n",
    "    target_names=['Positive', 'Negative', 'Neutral']\n",
    "))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(val_labels, all_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## Step 7: Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the classification endpoint\n",
    "print(f\"Deleting endpoint: {clf_predictor.endpoint_name}\")\n",
    "clf_predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally clean up S3 data\n",
    "# Uncomment to delete:\n",
    "\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket = s3.Bucket(BUCKET_NAME)\n",
    "# bucket.objects.filter(Prefix=PREFIX).delete()\n",
    "# print(f\"Deleted all objects under s3://{BUCKET_NAME}/{PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "### Part A: Word2Vec\n",
    "\n",
    "1. **Data Format**: Plain text file, one sentence per line, space-separated tokens\n",
    "\n",
    "2. **Modes**:\n",
    "   - `cbow`: Fast, good for large datasets\n",
    "   - `skipgram`: Better for rare words, smaller datasets\n",
    "   - `batch_skipgram`: Distributed training across CPUs\n",
    "\n",
    "3. **Key Hyperparameters**:\n",
    "   - `vector_dim`: Embedding dimension (50-300 typical)\n",
    "   - `window_size`: Context window (5-10 typical)\n",
    "   - `subwords`: Enable for OOV word handling\n",
    "\n",
    "4. **Output**: `vectors.txt` (Gensim compatible), `vectors.bin` (deployment)\n",
    "\n",
    "### Part B: Text Classification\n",
    "\n",
    "1. **Data Format**: `__label__<label> token1 token2 ...` (one per line)\n",
    "\n",
    "2. **Mode**: `supervised`\n",
    "\n",
    "3. **Key Hyperparameters**:\n",
    "   - `word_ngrams`: N-gram features (2 for bigrams)\n",
    "   - `early_stopping`: Prevent overfitting\n",
    "   - `vector_dim`: Embedding dimension\n",
    "\n",
    "4. **Inference**: Returns top-k labels with probabilities\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "| Use Case | Mode | Description |\n",
    "|----------|------|-------------|\n",
    "| Word Embeddings | skipgram/cbow | Pre-train embeddings for NLP tasks |\n",
    "| Sentiment Analysis | supervised | Classify text sentiment |\n",
    "| Topic Classification | supervised | Categorize documents by topic |\n",
    "| Intent Detection | supervised | Classify user intents in chatbots |\n",
    "| Spam Detection | supervised | Binary classification of spam |\n",
    "\n",
    "### Instance Recommendations\n",
    "\n",
    "| Task | Data Size | Recommended Instance |\n",
    "|------|-----------|---------------------|\n",
    "| Word2Vec | Small | ml.c5.xlarge (CPU) |\n",
    "| Word2Vec | Large | ml.p3.2xlarge (GPU) |\n",
    "| Classification | < 2GB | ml.c5.xlarge (CPU) |\n",
    "| Classification | > 2GB | ml.p3.2xlarge (GPU) |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different Word2Vec architectures (cbow vs skipgram)\n",
    "- Experiment with subword embeddings for rare words\n",
    "- Use pre-trained embeddings for downstream tasks\n",
    "- Try multi-label classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
