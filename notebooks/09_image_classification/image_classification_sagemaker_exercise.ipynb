{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Image Classification Exercise\n",
    "\n",
    "This notebook demonstrates Amazon SageMaker's **Image Classification** algorithm for classifying images into categories.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare image data for classification\n",
    "2. How to configure and understand image classification hyperparameters\n",
    "3. How to train an image classifier with transfer learning\n",
    "4. How to interpret and evaluate classification predictions\n",
    "\n",
    "## What is Image Classification?\n",
    "\n",
    "Image Classification assigns one or more labels to an **entire image**. Unlike object detection, it doesn't localize where objects are - it simply answers \"what's in this image?\"\n",
    "\n",
    "**SageMaker provides two implementations:**\n",
    "- **MXNet-based**: CNN with ResNet architecture (covered in this notebook)\n",
    "- **TensorFlow-based**: Transfer learning from TensorFlow Hub models\n",
    "\n",
    "## Comparison with Object Detection\n",
    "\n",
    "| Aspect | Image Classification | Object Detection |\n",
    "|--------|---------------------|------------------|\n",
    "| Output | Single/multiple labels for whole image | Labels + bounding boxes |\n",
    "| Question | \"What's in this image?\" | \"What and where?\" |\n",
    "| Complexity | Simpler | More complex |\n",
    "| Use case | Product categorization | Inventory counting |\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Industry | Application |\n",
    "|----------|-------------|\n",
    "| E-commerce | Product categorization, visual search |\n",
    "| Healthcare | Medical image diagnosis, X-ray analysis |\n",
    "| Manufacturing | Defect detection, quality control |\n",
    "| Social Media | Content moderation, auto-tagging |\n",
    "| Agriculture | Plant disease identification, crop classification |\n",
    "| Security | Scene classification, activity recognition |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ Important: Training Cost Warning\n",
    "\n",
    "<div style=\"background-color: #060604ff; border: 1px solid #ffc107; border-radius: 5px; padding: 15px; margin: 10px 0;\">\n",
    "\n",
    "### GPU Requirements and Costs\n",
    "\n",
    "**Image Classification training requires GPU instances.** Like Object Detection, this algorithm uses deep neural networks that are computationally intensive.\n",
    "\n",
    "| Instance Type | GPU | Memory | On-Demand Price* |\n",
    "|---------------|-----|--------|------------------|\n",
    "| ml.p2.xlarge | 1x K80 | 12 GB | ~$1.26/hour |\n",
    "| ml.p3.2xlarge | 1x V100 | 16 GB | ~$3.83/hour |\n",
    "| ml.p3.8xlarge | 4x V100 | 64 GB | ~$14.69/hour |\n",
    "| ml.g4dn.xlarge | 1x T4 | 16 GB | ~$0.74/hour |\n",
    "| ml.g4dn.2xlarge | 1x T4 | 32 GB | ~$1.05/hour |\n",
    "| ml.g5.xlarge | 1x A10G | 24 GB | ~$1.41/hour |\n",
    "\n",
    "*Prices are approximate for us-west-2 and subject to change. Check [AWS SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/) for current rates.\n",
    "\n",
    "### Cost Estimation Example\n",
    "\n",
    "Training a typical image classification model:\n",
    "- **30 epochs** with **50,000 images** (CIFAR-10 size): ~1-2 hours on ml.p3.2xlarge\n",
    "- **Estimated cost**: $3.83 - $7.66 for training\n",
    "- With transfer learning (pretrained model): Often converges faster, reducing cost\n",
    "\n",
    "### Cost-Saving Recommendations\n",
    "\n",
    "1. **Use Spot Instances**: Can save up to 70% - add `use_spot_instances=True` to Estimator\n",
    "2. **Start with ml.g4dn.xlarge**: Most cost-effective GPU option (~$0.74/hour)\n",
    "3. **Use transfer learning**: Set `use_pretrained_model=1` - requires fewer epochs\n",
    "4. **Start with fewer epochs**: Use 10 epochs to validate setup before full training\n",
    "5. **Use smaller ResNet**: `num_layers=18` or `50` instead of `152` for faster iteration\n",
    "6. **Enable early stopping**: Stop when validation accuracy plateaus\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"image-classification\"\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understand Data Formats\n",
    "\n",
    "SageMaker Image Classification supports multiple data formats. Choosing the right format depends on your dataset size and workflow.\n",
    "\n",
    "### Format 1: RecordIO (Recommended for Large Datasets)\n",
    "\n",
    "Binary format that packs images and labels together. Most efficient for training but requires preprocessing.\n",
    "\n",
    "```bash\n",
    "# RecordIO files\n",
    "train.rec\n",
    "train.idx\n",
    "validation.rec\n",
    "validation.idx\n",
    "```\n",
    "\n",
    "**Pros**: Fastest training, efficient I/O  \n",
    "**Cons**: Requires preprocessing step to create\n",
    "\n",
    "### Format 2: Image Files + LST File (Easiest to Understand)\n",
    "\n",
    "Plain image files with a list file mapping images to labels.\n",
    "\n",
    "```\n",
    "# Directory structure\n",
    "train/\n",
    "  cat/\n",
    "    image001.jpg\n",
    "    image002.jpg\n",
    "  dog/\n",
    "    image001.jpg\n",
    "    image002.jpg\n",
    "\n",
    "# train.lst format: index \\t label \\t path\n",
    "0\\t0\\ttrain/cat/image001.jpg\n",
    "1\\t0\\ttrain/cat/image002.jpg\n",
    "2\\t1\\ttrain/dog/image001.jpg\n",
    "3\\t1\\ttrain/dog/image002.jpg\n",
    "```\n",
    "\n",
    "**Pros**: Easy to create and inspect  \n",
    "**Cons**: Slower than RecordIO for large datasets\n",
    "\n",
    "### Format 3: Augmented Manifest (For Ground Truth Integration)\n",
    "\n",
    "JSON Lines format with S3 references - ideal when using SageMaker Ground Truth.\n",
    "\n",
    "```json\n",
    "{\"source-ref\": \"s3://bucket/image.jpg\", \"class\": 0}\n",
    "{\"source-ref\": \"s3://bucket/image2.jpg\", \"class\": 1}\n",
    "```\n",
    "\n",
    "**Pros**: Direct Ground Truth integration  \n",
    "**Cons**: Requires S3 paths for all images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LST File Format Deep Dive\n",
    "\n",
    "The LST (list) file format is tab-separated with three columns:\n",
    "\n",
    "```\n",
    "index \\t label \\t image_path\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- `index`: Unique integer ID for each image (typically 0, 1, 2, ...)\n",
    "- `label`: Class ID (0-indexed integer)\n",
    "- `image_path`: Relative path to the image file\n",
    "\n",
    "**For Multi-label Classification:**\n",
    "```\n",
    "# Comma-separated labels\n",
    "0\\t0,2,5\\ttrain/image001.jpg  # Image belongs to classes 0, 2, and 5\n",
    "1\\t1,3\\ttrain/image002.jpg    # Image belongs to classes 1 and 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Synthetic Data - Limitations and Purpose\n",
    "\n",
    "<div style=\"background-color: #d1ecf1; border: 1px solid #0c5460; border-radius: 5px; padding: 15px; margin: 10px 0;\">\n",
    "\n",
    "### ⚠️ Important: Why We Can't Truly Simulate Image Classification\n",
    "\n",
    "Like Object Detection, Image Classification requires **real images** with actual visual features that neural networks can learn from.\n",
    "\n",
    "**Why synthetic data doesn't work for training:**\n",
    "1. **CNNs learn hierarchical visual features**: Edges → Textures → Parts → Objects\n",
    "2. **Random noise or generated patterns** don't contain the visual structure found in real images\n",
    "3. **Labels are meaningless** without corresponding visual content\n",
    "\n",
    "**What we CAN demonstrate:**\n",
    "- ✅ Data format structure (LST files, RecordIO concept)\n",
    "- ✅ Data preparation pipeline\n",
    "- ✅ Evaluation metric calculations (accuracy, confusion matrix, etc.)\n",
    "- ✅ Output parsing and visualization\n",
    "- ✅ Hyperparameter configuration\n",
    "\n",
    "**For actual training, you need:**\n",
    "- Real images organized by class\n",
    "- Public datasets like CIFAR-10, ImageNet, or domain-specific datasets\n",
    "- SageMaker Ground Truth for custom labeling\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_lst_file(num_samples=100, num_classes=5, seed=42):\n",
    "    \"\"\"\n",
    "    Generate a sample LST file content to demonstrate the format.\n",
    "    \n",
    "    NOTE: This is for FORMAT DEMONSTRATION ONLY.\n",
    "    Real training requires actual images.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of sample entries\n",
    "        num_classes: Number of classes\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        lines: List of LST file lines\n",
    "        class_names: List of class names\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'dog']\n",
    "    lines = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        label = np.random.randint(0, num_classes)\n",
    "        class_name = class_names[label]\n",
    "        image_path = f\"train/{class_name}/image_{i:04d}.jpg\"\n",
    "        lines.append(f\"{i}\\t{label}\\t{image_path}\")\n",
    "    \n",
    "    return lines, class_names\n",
    "\n",
    "lst_lines, class_names = generate_sample_lst_file()\n",
    "\n",
    "print(\"Sample LST file content:\")\n",
    "print(\"Format: index\\tlabel\\tpath\")\n",
    "print(\"=\" * 60)\n",
    "for line in lst_lines[:10]:\n",
    "    print(line)\n",
    "print(\"...\")\n",
    "\n",
    "print(f\"\\nClasses: {class_names}\")\n",
    "print(f\"Total samples: {len(lst_lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution in our sample LST file\n",
    "def analyze_lst_distribution(lst_lines, class_names):\n",
    "    \"\"\"Analyze the class distribution in LST file.\"\"\"\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    for line in lst_lines:\n",
    "        parts = line.split('\\t')\n",
    "        label = int(parts[1])\n",
    "        class_counts[class_names[label]] += 1\n",
    "    \n",
    "    return dict(class_counts)\n",
    "\n",
    "class_distribution = analyze_lst_distribution(lst_lines, class_names)\n",
    "\n",
    "# Visualize distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "bars = ax.bar(class_distribution.keys(), class_distribution.values(), color=colors)\n",
    "\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Number of Samples')\n",
    "ax.set_title('Class Distribution in Training Data')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, class_distribution.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "           str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "counts = list(class_distribution.values())\n",
    "imbalance_ratio = max(counts) / min(counts)\n",
    "print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}x\")\n",
    "if imbalance_ratio > 2:\n",
    "    print(\"⚠️  Warning: Significant class imbalance detected. Consider using weighted loss or oversampling.\")\n",
    "else:\n",
    "    print(\"✓ Classes are reasonably balanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Training Configuration and Hyperparameters\n",
    "\n",
    "### Understanding Image Classification Hyperparameters\n",
    "\n",
    "SageMaker's Image Classification algorithm has many hyperparameters. Understanding each one is crucial for successful training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Required Parameters\n",
    "\n",
    "**num_classes** (Required)\n",
    "- The number of distinct categories to classify into\n",
    "- Must match the number of unique labels in your training data\n",
    "- Example: CIFAR-10 has 10 classes → `num_classes=10`\n",
    "\n",
    "**num_training_samples** (Required)\n",
    "- Total number of training images\n",
    "- Used for learning rate scheduling and progress tracking\n",
    "- Must match your actual training dataset size\n",
    "- Example: If you have 50,000 training images → `num_training_samples=50000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture Parameters\n",
    "\n",
    "**num_layers**\n",
    "- Depth of the ResNet architecture\n",
    "- Options: `18`, `34`, `50`, `101`, `152`, `200`\n",
    "- Trade-off: Deeper networks can learn more complex patterns but are slower and need more data\n",
    "\n",
    "| Layers | Parameters | Speed | Accuracy | Best For |\n",
    "|--------|------------|-------|----------|----------|\n",
    "| 18 | ~11M | Fastest | Lower | Quick experiments, simple tasks |\n",
    "| 34 | ~21M | Fast | Good | Medium complexity tasks |\n",
    "| 50 | ~25M | Medium | Very Good | **Recommended default** |\n",
    "| 101 | ~44M | Slow | Excellent | Complex tasks, large datasets |\n",
    "| 152 | ~60M | Very Slow | Best | Maximum accuracy needed |\n",
    "\n",
    "- Default: `152` (but `50` is often the best balance)\n",
    "- Recommendation: Start with `50`, increase only if needed\n",
    "\n",
    "**use_pretrained_model**\n",
    "- Whether to initialize with ImageNet pretrained weights\n",
    "- `1`: Yes - **highly recommended**, especially for smaller datasets\n",
    "- `0`: No - train from scratch (requires much more data: 100K+ images)\n",
    "- Pretrained models have already learned visual features from 1.2M ImageNet images\n",
    "- Transfer learning: Fine-tune these features for your specific task\n",
    "- Default: `1`\n",
    "\n",
    "**image_shape**\n",
    "- Input image dimensions in format `\"channels,height,width\"`\n",
    "- Common values: `\"3,224,224\"` (standard), `\"3,299,299\"` (Inception-style)\n",
    "- All input images are resized to this shape\n",
    "- Larger sizes: Better for fine details, but slower and more memory\n",
    "- Default: `\"3,224,224\"`\n",
    "- Important: Use same shape for training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters\n",
    "\n",
    "**epochs**\n",
    "- Number of complete passes through the training data\n",
    "- More epochs = more learning opportunities, but risk of overfitting\n",
    "- With pretrained models, often converges in 10-30 epochs\n",
    "- From scratch: May need 100+ epochs\n",
    "- Monitor validation accuracy to detect overfitting\n",
    "- Default: `30`\n",
    "\n",
    "**mini_batch_size**\n",
    "- Number of images processed before updating weights\n",
    "- Larger batches: More stable gradients, better GPU utilization\n",
    "- Smaller batches: More frequent updates, may generalize better\n",
    "- Limited by GPU memory (reduce if you get OOM errors)\n",
    "- Typical range: 16-128 depending on image size and GPU\n",
    "- Default: `32`\n",
    "- Rule of thumb: If using `image_shape=\"3,224,224\"`, batch 32-64 works on most GPUs\n",
    "\n",
    "**learning_rate**\n",
    "- How much to adjust weights on each update\n",
    "- Too high: Training oscillates or diverges\n",
    "- Too low: Training is very slow\n",
    "- **Critical for transfer learning**: Use LOWER learning rate (0.001-0.01) to preserve pretrained features\n",
    "- From scratch: Can use higher rate (0.1)\n",
    "- Default: `0.1` (reduce for fine-tuning!)\n",
    "\n",
    "**lr_scheduler_step**\n",
    "- Epochs at which to reduce learning rate\n",
    "- Format: comma-separated epoch numbers (e.g., `\"10,20\"`)\n",
    "- Reducing learning rate helps fine-tune as training progresses\n",
    "- Common pattern: Reduce at 1/3 and 2/3 of total epochs\n",
    "- Example: For 30 epochs → `\"10,20\"`\n",
    "\n",
    "**lr_scheduler_factor**\n",
    "- Factor to multiply learning rate by at each step\n",
    "- Value of `0.1` means learning rate becomes 10% of previous value\n",
    "- Default: `0.1`\n",
    "- Example: If LR=0.01 and factor=0.1, after step LR=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Parameters\n",
    "\n",
    "**optimizer**\n",
    "- Algorithm for updating weights based on gradients\n",
    "- Options: `sgd`, `adam`, `rmsprop`, `nag` (Nesterov Accelerated Gradient)\n",
    "- `sgd`: Simple, effective with proper momentum and learning rate schedule\n",
    "- `adam`: Adaptive learning rates, often converges faster, good default\n",
    "- `nag`: SGD with look-ahead momentum, can escape local minima better\n",
    "- Default: `sgd`\n",
    "- Recommendation: Use `sgd` with momentum for best results on image classification\n",
    "\n",
    "**momentum**\n",
    "- Used with SGD/NAG optimizer\n",
    "- Helps accelerate training by maintaining velocity in consistent directions\n",
    "- Typical value: `0.9`\n",
    "- Higher momentum (0.9-0.99): Faster convergence, may overshoot\n",
    "- Default: `0.9`\n",
    "\n",
    "**weight_decay**\n",
    "- L2 regularization to prevent overfitting\n",
    "- Adds penalty for large weights\n",
    "- Helps model generalize by keeping weights small\n",
    "- Typical range: `0.0001` to `0.001`\n",
    "- Default: `0.0001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Parameters\n",
    "\n",
    "**augmentation_type**\n",
    "- Type of data augmentation to apply during training\n",
    "- Options: `crop`, `crop_color`, `crop_color_transform`\n",
    "\n",
    "| Type | Augmentations Applied | Speed | When to Use |\n",
    "|------|----------------------|-------|-------------|\n",
    "| `crop` | Random crop, horizontal flip | Fast | Large datasets, speed priority |\n",
    "| `crop_color` | + Brightness, saturation, hue jitter | Medium | Moderate datasets |\n",
    "| `crop_color_transform` | + Rotation, shear, aspect ratio | Slower | **Small datasets, prevent overfitting** |\n",
    "\n",
    "- Default: `crop_color_transform`\n",
    "- Recommendation: Use `crop_color_transform` unless you have 100K+ images\n",
    "\n",
    "**resize**\n",
    "- Size to resize images before cropping\n",
    "- Should be larger than `image_shape` to allow random cropping\n",
    "- Default: `256` (for 224x224 images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification-Specific Parameters\n",
    "\n",
    "**top_k**\n",
    "- Report top-k accuracy metrics during training\n",
    "- Top-1: Correct if best prediction matches label\n",
    "- Top-5: Correct if label is in top 5 predictions\n",
    "- Default: `5`\n",
    "- Set to number of classes if you have fewer than 5 classes\n",
    "\n",
    "**multi_label**\n",
    "- Enable multi-label classification mode\n",
    "- `0`: Single-label (each image has exactly one class)\n",
    "- `1`: Multi-label (each image can have multiple classes)\n",
    "- Default: `0`\n",
    "- Changes loss function from softmax to sigmoid\n",
    "\n",
    "**precision_dtype**\n",
    "- Numerical precision for training\n",
    "- `float32`: Standard precision, most compatible\n",
    "- `float16`: Mixed precision, faster on V100/A100 GPUs, may reduce accuracy slightly\n",
    "- Default: `float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Image Classification container image\n",
    "image_classification_image = retrieve(\n",
    "    framework='image-classification',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"Image Classification Image URI: {image_classification_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete hyperparameter configuration with explanations\n",
    "hyperparameters = {\n",
    "    # === REQUIRED PARAMETERS ===\n",
    "    \"num_classes\": 5,                        # Number of classification categories\n",
    "    \"num_training_samples\": 10000,           # Total training images\n",
    "    \n",
    "    # === NETWORK ARCHITECTURE ===\n",
    "    \"num_layers\": 50,                        # ResNet-50 (good balance of speed/accuracy)\n",
    "    \"use_pretrained_model\": 1,               # Transfer learning from ImageNet\n",
    "    \"image_shape\": \"3,224,224\",              # Input image shape (channels, height, width)\n",
    "    \n",
    "    # === TRAINING PARAMETERS ===\n",
    "    \"epochs\": 30,                            # Training epochs\n",
    "    \"mini_batch_size\": 32,                   # Batch size\n",
    "    \"learning_rate\": 0.001,                  # Lower for fine-tuning pretrained model\n",
    "    \"lr_scheduler_step\": \"10,20\",            # Reduce LR at epochs 10 and 20\n",
    "    \"lr_scheduler_factor\": 0.1,              # LR multiplier at each step\n",
    "    \n",
    "    # === OPTIMIZER ===\n",
    "    \"optimizer\": \"sgd\",                      # SGD with momentum\n",
    "    \"momentum\": 0.9,                         # Momentum value\n",
    "    \"weight_decay\": 0.0001,                  # L2 regularization\n",
    "    \n",
    "    # === DATA AUGMENTATION ===\n",
    "    \"augmentation_type\": \"crop_color_transform\",  # Full augmentation\n",
    "    \"resize\": 256,                           # Resize before cropping\n",
    "    \n",
    "    # === CLASSIFICATION SPECIFIC ===\n",
    "    \"top_k\": 5,                              # Report top-k accuracy\n",
    "    \"multi_label\": 0,                        # Single-label classification\n",
    "    \"precision_dtype\": \"float32\",            # Training precision\n",
    "}\n",
    "\n",
    "print(\"Image Classification Hyperparameters:\")\n",
    "print(\"=\" * 55)\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Estimator Configuration\n",
    "# NOTE: Do NOT run training without actual image data!\n",
    "\n",
    "print(\"\"\"\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "                    EXAMPLE ESTIMATOR CONFIGURATION\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "⚠️  WARNING: Running this training job will incur GPU costs!\n",
    "    Estimated cost: $2-10 depending on epochs and instance type.\n",
    "\n",
    "# Standard training (On-Demand)\n",
    "image_classification_estimator = Estimator(\n",
    "    image_uri=image_classification_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',  # GPU required!\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='image-classification',\n",
    "    max_run=3600 * 4,  # 4 hour max runtime\n",
    ")\n",
    "\n",
    "# Cost-saving alternative with Spot Instances (up to 70% savings)\n",
    "image_classification_estimator_spot = Estimator(\n",
    "    image_uri=image_classification_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',  # Most cost-effective GPU\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='image-classification-spot',\n",
    "    use_spot_instances=True,         # Enable Spot pricing\n",
    "    max_wait=3600 * 5,               # Max time to wait for spot capacity\n",
    "    max_run=3600 * 4,                # Max training time\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "image_classification_estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "# Data channels configuration (using LST file format)\n",
    "# train: s3://bucket/prefix/train/  (images)\n",
    "# validation: s3://bucket/prefix/validation/  (images)\n",
    "# train_lst: s3://bucket/prefix/train.lst  (list file)\n",
    "# validation_lst: s3://bucket/prefix/validation.lst  (list file)\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Understanding Model Output\n",
    "\n",
    "The model outputs a **probability distribution** over all classes. Each output value represents the model's confidence that the image belongs to that class.\n",
    "\n",
    "**Output Format:**\n",
    "- Array of probabilities, one per class\n",
    "- All values sum to 1.0 (for single-label classification)\n",
    "- Index corresponds to class ID\n",
    "\n",
    "**Example:** For 5 classes\n",
    "```\n",
    "[0.05, 0.02, 0.85, 0.05, 0.03]\n",
    "      ↑            ↑\n",
    "  class 1    class 2 (highest = prediction)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_classification_output(probabilities, class_names, top_k=5):\n",
    "    \"\"\"\n",
    "    Parse classification output and return top-k predictions.\n",
    "    \n",
    "    Args:\n",
    "        probabilities: Array of class probabilities (sums to 1.0)\n",
    "        class_names: List of class names\n",
    "        top_k: Number of top predictions to return\n",
    "    \n",
    "    Returns:\n",
    "        List of prediction dictionaries sorted by probability\n",
    "    \"\"\"\n",
    "    # Sort by probability descending\n",
    "    top_indices = np.argsort(probabilities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'class_name': class_names[idx],\n",
    "            'class_id': int(idx),\n",
    "            'probability': float(probabilities[idx]),\n",
    "            'percentage': f\"{probabilities[idx] * 100:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Simulate model output - a typical confident prediction\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a realistic output where model is confident about one class\n",
    "sample_probs = np.array([0.02, 0.05, 0.85, 0.05, 0.03])  # Confident about class 2 (bird)\n",
    "\n",
    "predictions = parse_classification_output(sample_probs, class_names)\n",
    "\n",
    "print(\"Sample Classification Output:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nRaw probabilities: {sample_probs}\")\n",
    "print(f\"Sum of probabilities: {sum(sample_probs):.4f} (should be 1.0)\")\n",
    "print(f\"\\nTop-{len(predictions)} predictions:\")\n",
    "for i, pred in enumerate(predictions, 1):\n",
    "    marker = \"← Predicted class\" if i == 1 else \"\"\n",
    "    print(f\"  {i}. {pred['class_name']:12s}: {pred['percentage']:>8s} {marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(probabilities, class_names, true_label=None, title=\"Classification Predictions\"):\n",
    "    \"\"\"\n",
    "    Visualize classification probabilities as a horizontal bar chart.\n",
    "    \n",
    "    Args:\n",
    "        probabilities: Array of class probabilities\n",
    "        class_names: List of class names\n",
    "        true_label: Optional true class index for comparison\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Color bars based on probability magnitude\n",
    "    colors = plt.cm.RdYlGn(probabilities)  # Red to Green colormap\n",
    "    \n",
    "    # Highlight true label if provided\n",
    "    if true_label is not None:\n",
    "        edge_colors = ['green' if i == true_label else 'none' for i in range(len(class_names))]\n",
    "        linewidths = [3 if i == true_label else 0 for i in range(len(class_names))]\n",
    "    else:\n",
    "        edge_colors = ['none'] * len(class_names)\n",
    "        linewidths = [0] * len(class_names)\n",
    "    \n",
    "    bars = ax.barh(class_names, probabilities, color=colors, \n",
    "                   edgecolor=edge_colors, linewidth=linewidths)\n",
    "    \n",
    "    ax.set_xlabel('Probability')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, prob in zip(bars, probabilities):\n",
    "        ax.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "               f'{prob:.4f}', va='center', fontsize=10)\n",
    "    \n",
    "    # Add predicted class indicator\n",
    "    predicted_class = np.argmax(probabilities)\n",
    "    ax.annotate('Predicted', \n",
    "                xy=(probabilities[predicted_class], predicted_class),\n",
    "                xytext=(probabilities[predicted_class] + 0.15, predicted_class),\n",
    "                fontsize=10, color='blue',\n",
    "                arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "    \n",
    "    if true_label is not None:\n",
    "        ax.annotate('True Label', \n",
    "                    xy=(probabilities[true_label], true_label),\n",
    "                    xytext=(0.7, true_label + 0.5),\n",
    "                    fontsize=10, color='green',\n",
    "                    arrowprops=dict(arrowstyle='->', color='green'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize with true label\n",
    "visualize_predictions(sample_probs, class_names, true_label=2, \n",
    "                     title=\"Sample Prediction: Correctly Classified 'bird'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Uncertain prediction (low confidence)\n",
    "uncertain_probs = np.array([0.25, 0.22, 0.20, 0.18, 0.15])\n",
    "\n",
    "print(\"Example: Uncertain Prediction\")\n",
    "print(\"When probabilities are similar, the model is uncertain.\")\n",
    "print(f\"Max probability: {max(uncertain_probs):.2%}\")\n",
    "print(f\"Entropy: {-sum(p * np.log(p) for p in uncertain_probs if p > 0):.4f}\")\n",
    "\n",
    "visualize_predictions(uncertain_probs, class_names, \n",
    "                     title=\"Uncertain Prediction (Low Confidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Evaluation Metrics Deep Dive\n",
    "\n",
    "Image classification uses several metrics depending on whether it's single-label or multi-label.\n",
    "\n",
    "### Single-Label Metrics\n",
    "\n",
    "| Metric | Description | Formula |\n",
    "|--------|-------------|--------|\n",
    "| **Top-1 Accuracy** | Correct if top prediction = true label | Correct / Total |\n",
    "| **Top-5 Accuracy** | Correct if true label in top 5 predictions | Correct / Total |\n",
    "| **Cross-Entropy Loss** | Measures prediction confidence | -log(P(true class)) |\n",
    "\n",
    "### Multi-Label Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **Precision** | Of predicted labels, how many are correct? |\n",
    "| **Recall** | Of true labels, how many were predicted? |\n",
    "| **F1 Score** | Harmonic mean of precision and recall |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topk_accuracy(predictions_list, true_labels, k=5):\n",
    "    \"\"\"\n",
    "    Calculate top-k accuracy for classification.\n",
    "    \n",
    "    Args:\n",
    "        predictions_list: List of probability arrays (one per image)\n",
    "        true_labels: List of true class indices\n",
    "        k: Top-k parameter\n",
    "    \n",
    "    Returns:\n",
    "        Accuracy value between 0 and 1\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for probs, true_label in zip(predictions_list, true_labels):\n",
    "        top_k_preds = np.argsort(probs)[::-1][:k]\n",
    "        if true_label in top_k_preds:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / len(predictions_list)\n",
    "\n",
    "\n",
    "def calculate_cross_entropy_loss(predictions_list, true_labels):\n",
    "    \"\"\"\n",
    "    Calculate average cross-entropy loss.\n",
    "    \n",
    "    Cross-entropy measures how well the probability distribution\n",
    "    matches the true labels. Lower is better.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Prevent log(0)\n",
    "    total_loss = 0\n",
    "    \n",
    "    for probs, true_label in zip(predictions_list, true_labels):\n",
    "        # Clip probability to prevent log(0)\n",
    "        prob = np.clip(probs[true_label], epsilon, 1 - epsilon)\n",
    "        total_loss += -np.log(prob)\n",
    "    \n",
    "    return total_loss / len(predictions_list)\n",
    "\n",
    "\n",
    "# Simulate a realistic evaluation scenario\n",
    "np.random.seed(42)\n",
    "num_test_samples = 500\n",
    "\n",
    "# Generate test labels\n",
    "test_labels = np.random.randint(0, len(class_names), num_test_samples)\n",
    "\n",
    "# Generate realistic predictions (model is mostly correct but not perfect)\n",
    "test_predictions = []\n",
    "for true_label in test_labels:\n",
    "    # 80% of the time, model is confident and correct\n",
    "    if np.random.random() < 0.80:\n",
    "        probs = np.random.dirichlet(np.ones(len(class_names)) * 0.5)\n",
    "        probs[true_label] += 0.5  # Boost true class\n",
    "        probs = probs / probs.sum()  # Renormalize\n",
    "    else:\n",
    "        # Model makes a mistake\n",
    "        probs = np.random.dirichlet(np.ones(len(class_names)))\n",
    "    test_predictions.append(probs)\n",
    "\n",
    "# Calculate metrics\n",
    "top1_acc = calculate_topk_accuracy(test_predictions, test_labels, k=1)\n",
    "top2_acc = calculate_topk_accuracy(test_predictions, test_labels, k=2)\n",
    "top3_acc = calculate_topk_accuracy(test_predictions, test_labels, k=3)\n",
    "top5_acc = calculate_topk_accuracy(test_predictions, test_labels, k=5)\n",
    "ce_loss = calculate_cross_entropy_loss(test_predictions, test_labels)\n",
    "\n",
    "print(\"Evaluation Metrics (Simulated 500 test samples):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Top-1 Accuracy: {top1_acc:.4f} ({top1_acc*100:.1f}%)\")\n",
    "print(f\"  Top-2 Accuracy: {top2_acc:.4f} ({top2_acc*100:.1f}%)\")\n",
    "print(f\"  Top-3 Accuracy: {top3_acc:.4f} ({top3_acc*100:.1f}%)\")\n",
    "print(f\"  Top-5 Accuracy: {top5_acc:.4f} ({top5_acc*100:.1f}%)\")\n",
    "print(f\"  Cross-Entropy Loss: {ce_loss:.4f}\")\n",
    "print(f\"\\nNote: With 5 classes, random guessing would give ~20% top-1 accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix shows how predictions match (or don't match) true labels. It's essential for understanding:\n",
    "- Which classes the model confuses\n",
    "- Per-class accuracy\n",
    "- Systematic biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(predictions_list, true_labels, num_classes):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix for classification.\n",
    "    \n",
    "    Args:\n",
    "        predictions_list: List of probability arrays\n",
    "        true_labels: List of true class indices\n",
    "        num_classes: Number of classes\n",
    "    \n",
    "    Returns:\n",
    "        Confusion matrix (num_classes x num_classes)\n",
    "        Row = true label, Column = predicted label\n",
    "    \"\"\"\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    \n",
    "    for probs, true_label in zip(predictions_list, true_labels):\n",
    "        predicted_label = np.argmax(probs)\n",
    "        cm[true_label, predicted_label] += 1\n",
    "    \n",
    "    return cm\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, normalize=False, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        cm: Confusion matrix array\n",
    "        class_names: List of class names\n",
    "        normalize: Whether to show percentages\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm_display = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "    else:\n",
    "        cm_display = cm\n",
    "        fmt = 'd'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax.imshow(cm_display, interpolation='nearest', cmap='Blues')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Labels\n",
    "    ax.set(xticks=np.arange(len(class_names)),\n",
    "           yticks=np.arange(len(class_names)),\n",
    "           xticklabels=class_names,\n",
    "           yticklabels=class_names,\n",
    "           xlabel='Predicted Label',\n",
    "           ylabel='True Label',\n",
    "           title=title)\n",
    "    \n",
    "    # Rotate x labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm_display.max() / 2\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            value = cm_display[i, j]\n",
    "            if normalize:\n",
    "                text = f'{value:.2f}'\n",
    "            else:\n",
    "                text = f'{value}'\n",
    "            ax.text(j, i, text,\n",
    "                   ha='center', va='center',\n",
    "                   color='white' if cm_display[i, j] > thresh else 'black',\n",
    "                   fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "cm = compute_confusion_matrix(test_predictions, test_labels, len(class_names))\n",
    "\n",
    "print(\"Confusion Matrix (counts):\")\n",
    "plot_confusion_matrix(cm, class_names, normalize=False, title=\"Confusion Matrix (Counts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix (percentages)\n",
    "print(\"Normalized Confusion Matrix (per-class recall):\")\n",
    "plot_confusion_matrix(cm, class_names, normalize=True, \n",
    "                     title=\"Confusion Matrix (Normalized by True Label)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_report(cm, class_names):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 for each class.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with per-class and average metrics\n",
    "    \"\"\"\n",
    "    num_classes = len(class_names)\n",
    "    report = {}\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    supports = []\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i, i]\n",
    "        fp = cm[:, i].sum() - tp  # Column sum minus diagonal\n",
    "        fn = cm[i, :].sum() - tp  # Row sum minus diagonal\n",
    "        support = cm[i, :].sum()  # Row sum = true positives for this class\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        report[class_names[i]] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1-score': f1,\n",
    "            'support': support\n",
    "        }\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        supports.append(support)\n",
    "    \n",
    "    # Macro average (unweighted mean)\n",
    "    report['macro avg'] = {\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls),\n",
    "        'f1-score': np.mean(f1s),\n",
    "        'support': sum(supports)\n",
    "    }\n",
    "    \n",
    "    # Weighted average\n",
    "    total = sum(supports)\n",
    "    report['weighted avg'] = {\n",
    "        'precision': sum(p * s for p, s in zip(precisions, supports)) / total,\n",
    "        'recall': sum(r * s for r, s in zip(recalls, supports)) / total,\n",
    "        'f1-score': sum(f * s for f, s in zip(f1s, supports)) / total,\n",
    "        'support': total\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Compute and display classification report\n",
    "report = compute_classification_report(cm, class_names)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Class':>15s} {'Precision':>12s} {'Recall':>12s} {'F1-Score':>12s} {'Support':>10s}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for class_name in class_names:\n",
    "    metrics = report[class_name]\n",
    "    print(f\"{class_name:>15s} {metrics['precision']:>12.4f} {metrics['recall']:>12.4f} \"\n",
    "          f\"{metrics['f1-score']:>12.4f} {metrics['support']:>10d}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "for avg_type in ['macro avg', 'weighted avg']:\n",
    "    metrics = report[avg_type]\n",
    "    print(f\"{avg_type:>15s} {metrics['precision']:>12.4f} {metrics['recall']:>12.4f} \"\n",
    "          f\"{metrics['f1-score']:>12.4f} {metrics['support']:>10d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics_names = ['precision', 'recall', 'f1-score']\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "\n",
    "for ax, metric_name in zip(axes, metrics_names):\n",
    "    values = [report[cn][metric_name] for cn in class_names]\n",
    "    bars = ax.bar(class_names, values, color=colors)\n",
    "    \n",
    "    ax.set_ylabel(metric_name.capitalize())\n",
    "    ax.set_title(f'Per-Class {metric_name.capitalize()}')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axhline(y=report['macro avg'][metric_name], color='red', linestyle='--', \n",
    "               label=f'Macro Avg: {report[\"macro avg\"][metric_name]:.3f}')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Rotate x labels\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "               f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Multi-Label Classification\n",
    "\n",
    "In multi-label classification, each image can belong to **multiple classes simultaneously**.\n",
    "\n",
    "**Example Use Cases:**\n",
    "- Image tagging (sunset, beach, people)\n",
    "- Medical imaging (multiple conditions)\n",
    "- Content moderation (violence, nudity, hate speech)\n",
    "\n",
    "**Key Differences from Single-Label:**\n",
    "\n",
    "| Aspect | Single-Label | Multi-Label |\n",
    "|--------|-------------|-------------|\n",
    "| Output activation | Softmax (sums to 1) | Sigmoid (independent) |\n",
    "| Loss function | Cross-entropy | Binary cross-entropy |\n",
    "| Decision | argmax | Threshold (e.g., > 0.5) |\n",
    "| Metrics | Accuracy | Precision, Recall, F1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_multilabel_output(probabilities, class_names, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Parse multi-label classification output.\n",
    "    \n",
    "    In multi-label, we use a threshold to decide which classes are present.\n",
    "    \n",
    "    Args:\n",
    "        probabilities: Array of independent class probabilities (0-1 each)\n",
    "        class_names: List of class names\n",
    "        threshold: Probability threshold for positive prediction\n",
    "    \n",
    "    Returns:\n",
    "        List of predicted classes and all probabilities\n",
    "    \"\"\"\n",
    "    predicted_classes = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i, (prob, name) in enumerate(zip(probabilities, class_names)):\n",
    "        all_predictions.append({\n",
    "            'class_name': name,\n",
    "            'class_id': i,\n",
    "            'probability': prob,\n",
    "            'predicted': prob >= threshold\n",
    "        })\n",
    "        \n",
    "        if prob >= threshold:\n",
    "            predicted_classes.append(name)\n",
    "    \n",
    "    return predicted_classes, all_predictions\n",
    "\n",
    "\n",
    "# Example: Multi-label scenario (image has both 'bird' and 'airplane')\n",
    "multilabel_probs = np.array([0.82, 0.15, 0.78, 0.08, 0.12])  # airplane and bird both high\n",
    "\n",
    "predicted, all_preds = parse_multilabel_output(multilabel_probs, class_names, threshold=0.5)\n",
    "\n",
    "print(\"Multi-Label Classification Example:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nProbabilities: {multilabel_probs}\")\n",
    "print(f\"Note: Probabilities don't need to sum to 1\")\n",
    "print(f\"Sum: {multilabel_probs.sum():.2f}\")\n",
    "print(f\"\\nUsing threshold = 0.5:\")\n",
    "print(f\"Predicted classes: {predicted}\")\n",
    "print(f\"\\nAll predictions:\")\n",
    "for pred in all_preds:\n",
    "    status = \"✓ POSITIVE\" if pred['predicted'] else \"✗ negative\"\n",
    "    print(f\"  {pred['class_name']:12s}: {pred['probability']:.4f} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_multilabel_metrics(predictions_list, true_labels_list, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, F1 for multi-label classification.\n",
    "    \n",
    "    Args:\n",
    "        predictions_list: List of probability arrays\n",
    "        true_labels_list: List of binary label arrays (1 if class present)\n",
    "        threshold: Probability threshold for positive prediction\n",
    "    \"\"\"\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    \n",
    "    for probs, true_labels in zip(predictions_list, true_labels_list):\n",
    "        predicted = (probs >= threshold).astype(int)\n",
    "        \n",
    "        tp = np.sum((predicted == 1) & (true_labels == 1))\n",
    "        fp = np.sum((predicted == 1) & (true_labels == 0))\n",
    "        fn = np.sum((predicted == 0) & (true_labels == 1))\n",
    "        \n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "    \n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "# Simulate multi-label evaluation\n",
    "np.random.seed(42)\n",
    "num_samples = 200\n",
    "\n",
    "# Generate random multi-label data\n",
    "ml_true_labels = []\n",
    "ml_predictions = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    # Random true labels (each class has 30% chance of being present)\n",
    "    true = (np.random.random(len(class_names)) < 0.3).astype(int)\n",
    "    \n",
    "    # Simulate predictions (correlated with true labels + noise)\n",
    "    pred = true * 0.6 + np.random.random(len(class_names)) * 0.4\n",
    "    \n",
    "    ml_true_labels.append(true)\n",
    "    ml_predictions.append(pred)\n",
    "\n",
    "ml_metrics = calculate_multilabel_metrics(ml_predictions, ml_true_labels, threshold=0.5)\n",
    "\n",
    "print(\"Multi-Label Evaluation Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"  Precision: {ml_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {ml_metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {ml_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: CloudWatch Training Metrics\n",
    "\n",
    "During training, SageMaker Image Classification emits these metrics to CloudWatch:\n",
    "\n",
    "| Metric | Description | Good Values |\n",
    "|--------|-------------|-------------|\n",
    "| `train:accuracy` | Training set accuracy | Higher is better |\n",
    "| `validation:accuracy` | Validation set accuracy | Higher is better |\n",
    "| `train:cross_entropy` | Training loss | Lower is better |\n",
    "| `validation:cross_entropy` | Validation loss | Lower is better |\n",
    "| `train:top_k_accuracy_5` | Training top-5 accuracy | Higher is better |\n",
    "| `validation:top_k_accuracy_5` | Validation top-5 accuracy | Higher is better |\n",
    "\n",
    "### What to Watch For\n",
    "\n",
    "**Healthy Training:**\n",
    "- Both training and validation accuracy increasing\n",
    "- Loss decreasing over epochs\n",
    "- Small gap between training and validation metrics\n",
    "\n",
    "**Overfitting Signs:**\n",
    "- Training accuracy keeps improving but validation accuracy plateaus/decreases\n",
    "- Large gap between training and validation metrics\n",
    "- Validation loss starts increasing\n",
    "\n",
    "**Underfitting Signs:**\n",
    "- Both training and validation metrics are poor\n",
    "- Metrics improve very slowly\n",
    "\n",
    "**Learning Rate Issues:**\n",
    "- Loss oscillates wildly → Learning rate too high\n",
    "- Metrics change very slowly → Learning rate too low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training metrics over epochs\n",
    "np.random.seed(42)\n",
    "epochs = 30\n",
    "\n",
    "# Simulate healthy training curves with transfer learning\n",
    "# Transfer learning typically starts with higher accuracy and converges faster\n",
    "\n",
    "# Accuracy curves\n",
    "base_train_acc = 0.5\n",
    "train_accuracy = [min(0.98, base_train_acc + 0.015 * e + np.random.normal(0, 0.01)) for e in range(epochs)]\n",
    "val_accuracy = [min(0.92, base_train_acc - 0.03 + 0.014 * e + np.random.normal(0, 0.015)) for e in range(epochs)]\n",
    "\n",
    "# Loss curves\n",
    "base_loss = 2.0\n",
    "train_loss = [max(0.05, base_loss * np.exp(-0.12 * e) + np.random.normal(0, 0.03)) for e in range(epochs)]\n",
    "val_loss = [max(0.15, base_loss * np.exp(-0.10 * e) + 0.1 + np.random.normal(0, 0.04)) for e in range(epochs)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(range(1, epochs + 1), train_accuracy, 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[0].plot(range(1, epochs + 1), val_accuracy, 'r--', label='Validation Accuracy', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Training Progress: Accuracy')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mark LR reduction points\n",
    "for lr_step in [10, 20]:\n",
    "    axes[0].axvline(x=lr_step, color='green', linestyle=':', alpha=0.7)\n",
    "axes[0].text(10, 0.55, 'LR÷10', fontsize=9, color='green')\n",
    "axes[0].text(20, 0.55, 'LR÷10', fontsize=9, color='green')\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(range(1, epochs + 1), train_loss, 'b-', label='Training Loss', linewidth=2)\n",
    "axes[1].plot(range(1, epochs + 1), val_loss, 'r--', label='Validation Loss', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Cross-Entropy Loss')\n",
    "axes[1].set_title('Training Progress: Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Mark LR reduction points\n",
    "for lr_step in [10, 20]:\n",
    "    axes[1].axvline(x=lr_step, color='green', linestyle=':', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy[-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "print(f\"Final Training Loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_loss[-1]:.4f}\")\n",
    "print(f\"\\nGap (train-val accuracy): {train_accuracy[-1] - val_accuracy[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "### 1. Data Formats\n",
    "- **RecordIO**: Binary format, most efficient for large datasets\n",
    "- **Image + LST**: Separate images and list file mapping labels\n",
    "- **Augmented Manifest**: JSON Lines with S3 references\n",
    "\n",
    "### 2. Key Hyperparameters\n",
    "\n",
    "| Category | Parameters |\n",
    "|----------|------------|\n",
    "| Architecture | `num_layers`, `use_pretrained_model`, `image_shape` |\n",
    "| Training | `epochs`, `mini_batch_size`, `learning_rate` |\n",
    "| Optimizer | `optimizer`, `momentum`, `weight_decay` |\n",
    "| Augmentation | `augmentation_type`, `resize` |\n",
    "| Classification | `top_k`, `multi_label` |\n",
    "\n",
    "### 3. Model Output\n",
    "- **Single-label**: Probability distribution (softmax, sums to 1)\n",
    "- **Multi-label**: Independent probabilities (sigmoid, 0-1 each)\n",
    "\n",
    "### 4. Evaluation Metrics\n",
    "\n",
    "| Task | Metrics |\n",
    "|------|--------|\n",
    "| Single-label | Top-1/5 Accuracy, Confusion Matrix, Per-class Precision/Recall |\n",
    "| Multi-label | Precision, Recall, F1 Score |\n",
    "\n",
    "### 5. Transfer Learning\n",
    "- Use `use_pretrained_model=1` for smaller datasets\n",
    "- Use LOWER learning rate (0.001 instead of 0.1) to preserve pretrained features\n",
    "- Often converges in fewer epochs than training from scratch\n",
    "\n",
    "### Instance Requirements\n",
    "\n",
    "| Task | Instance Types | Notes |\n",
    "|------|----------------|-------|\n",
    "| Training | ml.g4dn.xlarge, ml.p3.2xlarge | **GPU required** |\n",
    "| Inference | ml.m5.large (CPU), ml.c5.large (CPU) | GPU optional for real-time |\n",
    "\n",
    "### Cost Considerations\n",
    "- Training costs: $2-15 depending on dataset size and epochs\n",
    "- Use Spot Instances for up to 70% savings\n",
    "- Start with ml.g4dn.xlarge (~$0.74/hour) for cost efficiency\n",
    "- Use ResNet-50 instead of ResNet-152 for faster iteration\n",
    "\n",
    "### Next Steps\n",
    "1. Obtain real labeled image data (CIFAR-10, ImageNet subset, or custom)\n",
    "2. Use SageMaker Ground Truth for custom dataset labeling\n",
    "3. Experiment with different `num_layers` and `augmentation_type` settings\n",
    "4. Monitor CloudWatch metrics during training\n",
    "5. Try multi-label classification for images with multiple attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [SageMaker Image Classification Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html)\n",
    "- [Image Classification Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html)\n",
    "- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html) - Good for testing\n",
    "- [ImageNet](http://www.image-net.org/) - Large-scale image dataset\n",
    "- [SageMaker Ground Truth](https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html) - For custom labeling\n",
    "- [AWS Pricing Calculator](https://calculator.aws/) - Estimate training costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
