{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker Neural Topic Model (NTM) Exercise\n",
    "\n",
    "This notebook demonstrates Amazon SageMaker's **Neural Topic Model (NTM)** algorithm for topic modeling.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare text data for topic modeling\n",
    "2. How to train an NTM model\n",
    "3. How to interpret discovered topics\n",
    "\n",
    "## What is Neural Topic Model?\n",
    "\n",
    "NTM is an **unsupervised** algorithm that discovers abstract topics in a collection of documents. It uses a neural network approach to learn topic distributions.\n",
    "\n",
    "**Key Concept:**\n",
    "- Documents are mixtures of topics\n",
    "- Topics are distributions over words\n",
    "- NTM learns both simultaneously\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Application | Description |\n",
    "|-------------|-------------|\n",
    "| Document Organization | Categorize large document collections |\n",
    "| Content Recommendation | Find similar content by topic |\n",
    "| Trend Analysis | Track topic evolution over time |\n",
    "| Search Enhancement | Topic-based document retrieval |\n",
    "| Customer Feedback | Analyze reviews/surveys by themes |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"neural-topic-model\"\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_DOCUMENTS = 1000\n",
    "NUM_TOPICS = 5\n",
    "VOCAB_SIZE = 500\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Generate Synthetic Document Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic_documents(num_docs=1000, num_topics=5, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic documents with known topic structure.\n",
    "    \n",
    "    Each topic has a vocabulary of associated words.\n",
    "    Documents are mixtures of topics.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Define topic vocabularies\n",
    "    topic_words = {\n",
    "        0: ['technology', 'software', 'computer', 'data', 'internet', 'digital', \n",
    "            'programming', 'algorithm', 'system', 'application', 'code', 'developer',\n",
    "            'network', 'cloud', 'security', 'database', 'server', 'api', 'machine', 'learning'],\n",
    "        1: ['sports', 'team', 'game', 'player', 'championship', 'score', \n",
    "            'win', 'match', 'coach', 'season', 'league', 'tournament',\n",
    "            'athlete', 'training', 'competition', 'stadium', 'fans', 'victory', 'goal', 'record'],\n",
    "        2: ['health', 'medical', 'patient', 'doctor', 'treatment', 'disease', \n",
    "            'hospital', 'medicine', 'research', 'clinical', 'therapy', 'diagnosis',\n",
    "            'symptoms', 'care', 'wellness', 'prevention', 'vaccine', 'nutrition', 'exercise', 'mental'],\n",
    "        3: ['business', 'market', 'company', 'financial', 'investment', 'economy', \n",
    "            'growth', 'revenue', 'profit', 'stock', 'industry', 'management',\n",
    "            'strategy', 'customer', 'sales', 'startup', 'enterprise', 'capital', 'trade', 'commerce'],\n",
    "        4: ['science', 'research', 'study', 'experiment', 'discovery', 'theory', \n",
    "            'physics', 'chemistry', 'biology', 'laboratory', 'scientist', 'analysis',\n",
    "            'hypothesis', 'evidence', 'findings', 'nature', 'universe', 'energy', 'molecule', 'observation']\n",
    "    }\n",
    "    \n",
    "    topic_names = ['Technology', 'Sports', 'Health', 'Business', 'Science']\n",
    "    \n",
    "    # Build vocabulary\n",
    "    all_words = []\n",
    "    for words in topic_words.values():\n",
    "        all_words.extend(words)\n",
    "    vocab = sorted(set(all_words))\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    documents = []\n",
    "    doc_topics = []  # Ground truth dominant topic\n",
    "    \n",
    "    for _ in range(num_docs):\n",
    "        # Sample dominant topic\n",
    "        dominant_topic = np.random.randint(0, num_topics)\n",
    "        doc_topics.append(dominant_topic)\n",
    "        \n",
    "        # Generate document length\n",
    "        doc_length = np.random.randint(50, 150)\n",
    "        \n",
    "        # Generate words (80% from dominant topic, 20% from others)\n",
    "        doc_words = []\n",
    "        for _ in range(doc_length):\n",
    "            if np.random.random() < 0.8:\n",
    "                word = np.random.choice(topic_words[dominant_topic])\n",
    "            else:\n",
    "                other_topic = np.random.choice([t for t in range(num_topics) if t != dominant_topic])\n",
    "                word = np.random.choice(topic_words[other_topic])\n",
    "            doc_words.append(word)\n",
    "        \n",
    "        documents.append(' '.join(doc_words))\n",
    "    \n",
    "    return documents, doc_topics, vocab, word_to_idx, topic_names\n",
    "\n",
    "# Generate documents\n",
    "documents, doc_topics, vocab, word_to_idx, topic_names = generate_topic_documents(\n",
    "    NUM_DOCUMENTS, NUM_TOPICS, RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(documents)} documents\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Topics: {topic_names}\")\n",
    "print(f\"\\nTopic distribution: {Counter(doc_topics)}\")\n",
    "\n",
    "print(f\"\\nSample document (Topic: {topic_names[doc_topics[0]]}):\")\n",
    "print(f\"  {documents[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for NTM\n",
    "\n",
    "NTM expects **bag-of-words** representation:\n",
    "- Each document is a vector of word counts\n",
    "- Vector length = vocabulary size\n",
    "- Format: CSV or RecordIO-protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents_to_bow(documents, word_to_idx):\n",
    "    \"\"\"\n",
    "    Convert documents to bag-of-words matrix.\n",
    "    \n",
    "    Returns:\n",
    "        bow_matrix: Shape (num_docs, vocab_size)\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_to_idx)\n",
    "    bow_matrix = np.zeros((len(documents), vocab_size), dtype=np.float32)\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        words = doc.lower().split()\n",
    "        for word in words:\n",
    "            if word in word_to_idx:\n",
    "                bow_matrix[doc_idx, word_to_idx[word]] += 1\n",
    "    \n",
    "    return bow_matrix\n",
    "\n",
    "# Convert to bag-of-words\n",
    "bow_matrix = documents_to_bow(documents, word_to_idx)\n",
    "\n",
    "print(f\"Bag-of-words matrix shape: {bow_matrix.shape}\")\n",
    "print(f\"Non-zero entries: {np.count_nonzero(bow_matrix)}\")\n",
    "print(f\"Sparsity: {1 - np.count_nonzero(bow_matrix) / bow_matrix.size:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "np.random.seed(RANDOM_STATE)\n",
    "indices = np.random.permutation(len(bow_matrix))\n",
    "\n",
    "val_size = int(0.1 * len(bow_matrix))\n",
    "train_idx = indices[val_size:]\n",
    "val_idx = indices[:val_size]\n",
    "\n",
    "train_data = bow_matrix[train_idx]\n",
    "val_data = bow_matrix[val_idx]\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "os.makedirs('data/ntm', exist_ok=True)\n",
    "\n",
    "np.savetxt('data/ntm/train.csv', train_data, delimiter=',')\n",
    "np.savetxt('data/ntm/validation.csv', val_data, delimiter=',')\n",
    "\n",
    "# Save vocabulary for later interpretation\n",
    "with open('data/ntm/vocab.json', 'w') as f:\n",
    "    json.dump({'vocab': vocab, 'word_to_idx': word_to_idx}, f)\n",
    "\n",
    "print(\"Data files created:\")\n",
    "for f in os.listdir('data/ntm'):\n",
    "    size = os.path.getsize(f'data/ntm/{f}') / 1024\n",
    "    print(f\"  data/ntm/{f} ({size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "for split in ['train', 'validation']:\n",
    "    s3_key = f\"{PREFIX}/{split}/{split}.csv\"\n",
    "    s3_client.upload_file(f'data/ntm/{split}.csv', BUCKET_NAME, s3_key)\n",
    "    print(f\"Uploaded: s3://{BUCKET_NAME}/{s3_key}\")\n",
    "\n",
    "train_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/train\"\n",
    "val_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 4: Train NTM Model\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `num_topics` | Number of topics to discover | Required |\n",
    "| `feature_dim` | Vocabulary size | Required |\n",
    "| `mini_batch_size` | Batch size | 256 |\n",
    "| `epochs` | Training epochs | 50 |\n",
    "| `learning_rate` | Learning rate | 0.001 |\n",
    "| `encoder_layers` | Hidden layer sizes | [100] |\n",
    "| `encoder_layers_activation` | Activation function | relu |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NTM container image\n",
    "ntm_image = retrieve(\n",
    "    framework='ntm',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"NTM Image URI: {ntm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NTM estimator\n",
    "ntm_estimator = Estimator(\n",
    "    image_uri=ntm_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='neural-topic-model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "hyperparameters = {\n",
    "    \"num_topics\": NUM_TOPICS,\n",
    "    \"feature_dim\": len(vocab),\n",
    "    \"mini_batch_size\": 128,\n",
    "    \"epochs\": 50,\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"num_patience_epochs\": 5,\n",
    "    \"tolerance\": 0.001,\n",
    "}\n",
    "\n",
    "ntm_estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "print(\"NTM hyperparameters:\")\n",
    "for k, v in hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting NTM training job...\")\n",
    "print(\"This will take approximately 5-10 minutes.\\n\")\n",
    "\n",
    "ntm_estimator.fit(\n",
    "    {\n",
    "        'train': train_uri,\n",
    "        'validation': val_uri\n",
    "    },\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "job_name = ntm_estimator.latest_training_job.name\n",
    "print(f\"Training job completed: {job_name}\")\n",
    "print(f\"Model artifacts: {ntm_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 5: Deploy and Get Topic Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "print(\"Deploying NTM model...\")\n",
    "print(\"This will take approximately 5-7 minutes.\\n\")\n",
    "\n",
    "ntm_predictor = ntm_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=f'ntm-{datetime.now().strftime(\"%Y%m%d%H%M\")}'\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint deployed: {ntm_predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Configure predictor\n",
    "ntm_predictor.serializer = CSVSerializer()\n",
    "ntm_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "def get_topic_distributions(data, predictor, batch_size=100):\n",
    "    \"\"\"\n",
    "    Get topic distributions for documents.\n",
    "    \"\"\"\n",
    "    all_distributions = []\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        response = predictor.predict(batch)\n",
    "        \n",
    "        for pred in response['predictions']:\n",
    "            all_distributions.append(pred['topic_weights'])\n",
    "    \n",
    "    return np.array(all_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distributions for all documents\n",
    "print(\"Getting topic distributions...\")\n",
    "topic_distributions = get_topic_distributions(bow_matrix, ntm_predictor)\n",
    "\n",
    "print(f\"Topic distributions shape: {topic_distributions.shape}\")\n",
    "print(f\"\\nSample distribution (first document):\")\n",
    "for i, weight in enumerate(topic_distributions[0]):\n",
    "    print(f\"  Topic {i}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Discovered Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign documents to their dominant topic\n",
    "predicted_topics = np.argmax(topic_distributions, axis=1)\n",
    "\n",
    "print(\"Predicted Topic Distribution:\")\n",
    "print(Counter(predicted_topics))\n",
    "\n",
    "print(\"\\nTrue Topic Distribution:\")\n",
    "print(Counter(doc_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Average topic weights\n",
    "avg_weights = topic_distributions.mean(axis=0)\n",
    "axes[0].bar(range(NUM_TOPICS), avg_weights)\n",
    "axes[0].set_xlabel('Topic')\n",
    "axes[0].set_ylabel('Average Weight')\n",
    "axes[0].set_title('Average Topic Weights Across All Documents')\n",
    "axes[0].set_xticks(range(NUM_TOPICS))\n",
    "\n",
    "# Topic distribution heatmap for sample documents\n",
    "sample_docs = np.random.choice(len(topic_distributions), 20, replace=False)\n",
    "im = axes[1].imshow(topic_distributions[sample_docs], aspect='auto', cmap='YlOrRd')\n",
    "axes[1].set_xlabel('Topic')\n",
    "axes[1].set_ylabel('Document')\n",
    "axes[1].set_title('Topic Weights for Sample Documents')\n",
    "axes[1].set_xticks(range(NUM_TOPICS))\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predicted vs true topics (matching by most common co-occurrence)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(doc_topics, predicted_topics)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=[f'Pred {i}' for i in range(NUM_TOPICS)],\n",
    "            yticklabels=topic_names)\n",
    "ax.set_xlabel('Predicted Topic')\n",
    "ax.set_ylabel('True Topic')\n",
    "ax.set_title('True vs Predicted Topic Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Step 7: Interpret Topics by Top Words\n",
    "\n",
    "To interpret topics, we examine which words have highest probability in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each discovered topic, find documents most representative\n",
    "# and examine their most frequent words\n",
    "\n",
    "print(\"Top Words per Discovered Topic (based on document analysis):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for topic_id in range(NUM_TOPICS):\n",
    "    # Get documents where this topic is dominant\n",
    "    topic_docs_mask = predicted_topics == topic_id\n",
    "    topic_docs = bow_matrix[topic_docs_mask]\n",
    "    \n",
    "    # Sum word counts across documents\n",
    "    word_counts = topic_docs.sum(axis=0)\n",
    "    \n",
    "    # Get top words\n",
    "    top_word_indices = np.argsort(word_counts)[::-1][:10]\n",
    "    top_words = [vocab[idx] for idx in top_word_indices]\n",
    "    \n",
    "    print(f\"\\nTopic {topic_id} ({topic_docs_mask.sum()} documents):\")\n",
    "    print(f\"  Top words: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Step 8: Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "print(f\"Deleting endpoint: {ntm_predictor.endpoint_name}\")\n",
    "ntm_predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **Data Format**: Bag-of-words vectors (CSV or RecordIO)\n",
    "\n",
    "2. **Key Hyperparameters**:\n",
    "   - `num_topics`: Number of topics to discover\n",
    "   - `feature_dim`: Vocabulary size\n",
    "   - `epochs`, `learning_rate`: Training parameters\n",
    "\n",
    "3. **Output**: Topic distribution vector per document\n",
    "\n",
    "4. **Topic Interpretation**:\n",
    "   - Examine top words per topic\n",
    "   - Review representative documents\n",
    "   - Use domain knowledge to label topics\n",
    "\n",
    "### NTM vs LDA\n",
    "\n",
    "| Aspect | NTM | LDA |\n",
    "|--------|-----|-----|\n",
    "| Architecture | Neural network | Probabilistic |\n",
    "| Training | GPU supported | CPU only |\n",
    "| Scalability | Multi-GPU, distributed | Single CPU |\n",
    "| Coherence | Good | Better (theory) |\n",
    "| Speed | Faster | Slower |\n",
    "\n",
    "### Instance Recommendations\n",
    "\n",
    "| Task | Instance Types |\n",
    "|------|----------------|\n",
    "| Training | ml.c5.xlarge, ml.p2.xlarge, ml.p3.2xlarge |\n",
    "| Inference | ml.m5.large, ml.c5.large |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Apply to real document collections\n",
    "- Experiment with different `num_topics` values\n",
    "- Use topic distributions as features for downstream tasks\n",
    "- Combine with document embeddings for richer representations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
