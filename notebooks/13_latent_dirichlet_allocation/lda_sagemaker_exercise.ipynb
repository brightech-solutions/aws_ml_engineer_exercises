{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker Latent Dirichlet Allocation (LDA) Exercise\n",
    "\n",
    "This notebook demonstrates Amazon SageMaker's **Latent Dirichlet Allocation (LDA)** algorithm for topic modeling.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare document data for LDA\n",
    "2. How to train an LDA topic model\n",
    "3. How to interpret topic-word distributions\n",
    "\n",
    "## What is LDA?\n",
    "\n",
    "LDA is an **unsupervised** probabilistic model that discovers hidden topics in document collections. It assumes:\n",
    "- Each document is a mixture of topics\n",
    "- Each topic is a distribution over words\n",
    "\n",
    "**SageMaker's Implementation:**\n",
    "- Uses tensor spectral decomposition (not Gibbs sampling)\n",
    "- Provides theoretical guarantees on results\n",
    "- Highly parallelizable\n",
    "\n",
    "## LDA vs NTM\n",
    "\n",
    "| Aspect | LDA | NTM |\n",
    "|--------|-----|-----|\n",
    "| Method | Probabilistic | Neural network |\n",
    "| Instance | CPU only | CPU and GPU |\n",
    "| Parallelization | Single CPU | Multi-GPU, distributed |\n",
    "| Topic coherence | Often better | Good |\n",
    "| Perplexity | Better | Good |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"lda\"\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_DOCUMENTS = 1000\n",
    "NUM_TOPICS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Generate Synthetic Document Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic_documents(num_docs=1000, num_topics=5, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic documents with known topic structure.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Define topic vocabularies\n",
    "    topic_words = {\n",
    "        0: ['movie', 'film', 'actor', 'director', 'scene', 'character', \n",
    "            'cinema', 'award', 'performance', 'screen', 'drama', 'comedy',\n",
    "            'hollywood', 'studio', 'premiere', 'cast', 'sequel', 'script', 'review', 'rating'],\n",
    "        1: ['music', 'song', 'album', 'artist', 'concert', 'band', \n",
    "            'guitar', 'drums', 'piano', 'melody', 'lyrics', 'rhythm',\n",
    "            'singer', 'tour', 'release', 'track', 'genre', 'festival', 'hit', 'record'],\n",
    "        2: ['food', 'recipe', 'cooking', 'chef', 'restaurant', 'ingredient', \n",
    "            'kitchen', 'meal', 'taste', 'flavor', 'dish', 'cuisine',\n",
    "            'baking', 'dinner', 'lunch', 'breakfast', 'vegetable', 'spice', 'delicious', 'homemade'],\n",
    "        3: ['travel', 'destination', 'hotel', 'flight', 'vacation', 'tourism', \n",
    "            'beach', 'mountain', 'city', 'country', 'adventure', 'explore',\n",
    "            'passport', 'trip', 'journey', 'sightseeing', 'culture', 'resort', 'booking', 'itinerary'],\n",
    "        4: ['fashion', 'style', 'clothing', 'designer', 'trend', 'outfit', \n",
    "            'collection', 'runway', 'model', 'brand', 'accessories', 'shoes',\n",
    "            'dress', 'fabric', 'boutique', 'wardrobe', 'elegant', 'casual', 'luxury', 'season']\n",
    "    }\n",
    "    \n",
    "    topic_names = ['Movies', 'Music', 'Food', 'Travel', 'Fashion']\n",
    "    \n",
    "    # Build vocabulary\n",
    "    all_words = []\n",
    "    for words in topic_words.values():\n",
    "        all_words.extend(words)\n",
    "    vocab = sorted(set(all_words))\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    documents = []\n",
    "    doc_topics = []\n",
    "    \n",
    "    for _ in range(num_docs):\n",
    "        dominant_topic = np.random.randint(0, num_topics)\n",
    "        doc_topics.append(dominant_topic)\n",
    "        \n",
    "        doc_length = np.random.randint(50, 150)\n",
    "        doc_words = []\n",
    "        \n",
    "        for _ in range(doc_length):\n",
    "            if np.random.random() < 0.8:\n",
    "                word = np.random.choice(topic_words[dominant_topic])\n",
    "            else:\n",
    "                other_topic = np.random.choice([t for t in range(num_topics) if t != dominant_topic])\n",
    "                word = np.random.choice(topic_words[other_topic])\n",
    "            doc_words.append(word)\n",
    "        \n",
    "        documents.append(' '.join(doc_words))\n",
    "    \n",
    "    return documents, doc_topics, vocab, word_to_idx, topic_names\n",
    "\n",
    "# Generate documents\n",
    "documents, doc_topics, vocab, word_to_idx, topic_names = generate_topic_documents(\n",
    "    NUM_DOCUMENTS, NUM_TOPICS, RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(documents)} documents\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Topics: {topic_names}\")\n",
    "print(f\"\\nTopic distribution: {Counter(doc_topics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for LDA\n",
    "\n",
    "LDA expects **bag-of-words** representation in CSV or RecordIO-protobuf format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents_to_bow(documents, word_to_idx):\n",
    "    \"\"\"\n",
    "    Convert documents to bag-of-words matrix.\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_to_idx)\n",
    "    bow_matrix = np.zeros((len(documents), vocab_size), dtype=np.float32)\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        words = doc.lower().split()\n",
    "        for word in words:\n",
    "            if word in word_to_idx:\n",
    "                bow_matrix[doc_idx, word_to_idx[word]] += 1\n",
    "    \n",
    "    return bow_matrix\n",
    "\n",
    "# Convert to bag-of-words\n",
    "bow_matrix = documents_to_bow(documents, word_to_idx)\n",
    "\n",
    "print(f\"Bag-of-words matrix shape: {bow_matrix.shape}\")\n",
    "print(f\"Non-zero entries: {np.count_nonzero(bow_matrix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "os.makedirs('data/lda', exist_ok=True)\n",
    "\n",
    "np.savetxt('data/lda/train.csv', bow_matrix, delimiter=',')\n",
    "\n",
    "# Save vocabulary\n",
    "with open('data/lda/vocab.json', 'w') as f:\n",
    "    json.dump({'vocab': vocab, 'word_to_idx': word_to_idx}, f)\n",
    "\n",
    "print(f\"Saved: data/lda/train.csv ({os.path.getsize('data/lda/train.csv') / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "train_s3_key = f\"{PREFIX}/train/train.csv\"\n",
    "s3_client.upload_file('data/lda/train.csv', BUCKET_NAME, train_s3_key)\n",
    "\n",
    "train_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/train\"\n",
    "print(f\"Data uploaded to: {train_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Train LDA Model\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `num_topics` | Number of topics | Required |\n",
    "| `feature_dim` | Vocabulary size | Required |\n",
    "| `mini_batch_size` | Batch size | 256 |\n",
    "| `alpha0` | Initial guess for topic concentration | 1.0 |\n",
    "\n",
    "**Note:** SageMaker LDA only supports single-instance CPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LDA container image\n",
    "lda_image = retrieve(\n",
    "    framework='lda',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"LDA Image URI: {lda_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LDA estimator\n",
    "lda_estimator = Estimator(\n",
    "    image_uri=lda_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',  # CPU only\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='lda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "hyperparameters = {\n",
    "    \"num_topics\": NUM_TOPICS,\n",
    "    \"feature_dim\": len(vocab),\n",
    "    \"mini_batch_size\": 128,\n",
    "    \"alpha0\": 1.0,\n",
    "}\n",
    "\n",
    "lda_estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "print(\"LDA hyperparameters:\")\n",
    "for k, v in hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting LDA training job...\")\n",
    "print(\"This will take approximately 3-5 minutes.\\n\")\n",
    "\n",
    "lda_estimator.fit(\n",
    "    {'train': train_uri},\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "job_name = lda_estimator.latest_training_job.name\n",
    "print(f\"Training job completed: {job_name}\")\n",
    "print(f\"Model artifacts: {lda_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 5: Deploy and Infer Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "print(\"Deploying LDA model...\")\n",
    "print(\"This will take approximately 5-7 minutes.\\n\")\n",
    "\n",
    "lda_predictor = lda_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=f'lda-{datetime.now().strftime(\"%Y%m%d%H%M\")}'\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint deployed: {lda_predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Configure predictor\n",
    "lda_predictor.serializer = CSVSerializer()\n",
    "lda_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "def get_topic_mixtures(data, predictor, batch_size=100):\n",
    "    \"\"\"\n",
    "    Get topic mixtures for documents.\n",
    "    \"\"\"\n",
    "    all_mixtures = []\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        response = predictor.predict(batch)\n",
    "        \n",
    "        for pred in response['predictions']:\n",
    "            all_mixtures.append(pred['topic_mixture'])\n",
    "    \n",
    "    return np.array(all_mixtures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic mixtures\n",
    "print(\"Getting topic mixtures...\")\n",
    "topic_mixtures = get_topic_mixtures(bow_matrix, lda_predictor)\n",
    "\n",
    "print(f\"Topic mixtures shape: {topic_mixtures.shape}\")\n",
    "print(f\"\\nSample mixture (first document):\")\n",
    "for i, weight in enumerate(topic_mixtures[0]):\n",
    "    print(f\"  Topic {i}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign dominant topics\n",
    "predicted_topics = np.argmax(topic_mixtures, axis=1)\n",
    "\n",
    "print(\"Predicted Topic Distribution:\")\n",
    "print(Counter(predicted_topics))\n",
    "\n",
    "print(\"\\nTrue Topic Distribution:\")\n",
    "print(Counter(doc_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic mixtures\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Average topic weights\n",
    "avg_weights = topic_mixtures.mean(axis=0)\n",
    "axes[0].bar(range(NUM_TOPICS), avg_weights)\n",
    "axes[0].set_xlabel('Topic')\n",
    "axes[0].set_ylabel('Average Weight')\n",
    "axes[0].set_title('Average Topic Weights')\n",
    "\n",
    "# Sample document mixtures\n",
    "sample_docs = np.random.choice(len(topic_mixtures), 20, replace=False)\n",
    "im = axes[1].imshow(topic_mixtures[sample_docs], aspect='auto', cmap='YlOrRd')\n",
    "axes[1].set_xlabel('Topic')\n",
    "axes[1].set_ylabel('Document')\n",
    "axes[1].set_title('Topic Mixtures for Sample Documents')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top words per topic\n",
    "print(\"Top Words per Discovered Topic:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for topic_id in range(NUM_TOPICS):\n",
    "    topic_docs_mask = predicted_topics == topic_id\n",
    "    topic_docs = bow_matrix[topic_docs_mask]\n",
    "    \n",
    "    word_counts = topic_docs.sum(axis=0)\n",
    "    top_word_indices = np.argsort(word_counts)[::-1][:10]\n",
    "    top_words = [vocab[idx] for idx in top_word_indices]\n",
    "    \n",
    "    print(f\"\\nTopic {topic_id} ({topic_docs_mask.sum()} documents):\")\n",
    "    print(f\"  Top words: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Step 7: Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "print(f\"Deleting endpoint: {lda_predictor.endpoint_name}\")\n",
    "lda_predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **Data Format**: Bag-of-words (CSV or RecordIO)\n",
    "\n",
    "2. **Key Hyperparameters**:\n",
    "   - `num_topics`: Number of topics\n",
    "   - `feature_dim`: Vocabulary size\n",
    "   - `alpha0`: Topic concentration\n",
    "\n",
    "3. **Output**: Topic mixture (probability distribution over topics per document)\n",
    "\n",
    "4. **Limitations**:\n",
    "   - Single-instance CPU only\n",
    "   - Less flexible than NTM\n",
    "\n",
    "### When to Use LDA vs NTM\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Small dataset, need coherent topics | LDA |\n",
    "| Large dataset, need speed | NTM |\n",
    "| Need distributed training | NTM |\n",
    "| Academic/research (interpretability) | LDA |\n",
    "\n",
    "### Instance Recommendations\n",
    "\n",
    "| Task | Instance Types |\n",
    "|------|----------------|\n",
    "| Training | ml.c5.xlarge, ml.m5.large (CPU only) |\n",
    "| Inference | ml.m5.large, ml.c5.large |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Compare results with NTM on same data\n",
    "- Experiment with different `num_topics` values\n",
    "- Use topic mixtures for document similarity\n",
    "- Apply to real document collections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
