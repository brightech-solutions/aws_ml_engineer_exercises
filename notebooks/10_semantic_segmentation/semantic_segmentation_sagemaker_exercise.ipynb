{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker Semantic Segmentation Exercise\n",
    "\n",
    "This notebook demonstrates Amazon SageMaker's **Semantic Segmentation** algorithm for pixel-level image classification.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare pixel-level annotations\n",
    "2. How to train a segmentation model\n",
    "3. How to interpret segmentation masks\n",
    "\n",
    "## What is Semantic Segmentation?\n",
    "\n",
    "Semantic Segmentation classifies **every pixel** in an image, assigning each pixel to a class. Unlike object detection which provides bounding boxes, segmentation provides exact object shapes.\n",
    "\n",
    "**Key Difference from Other CV Tasks:**\n",
    "\n",
    "| Task | Output | Granularity |\n",
    "|------|--------|-------------|\n",
    "| Classification | Single label | Image-level |\n",
    "| Object Detection | Bounding boxes + labels | Object-level |\n",
    "| Semantic Segmentation | Pixel mask | Pixel-level |\n",
    "| Instance Segmentation | Pixel mask per object | Pixel + Instance level |\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Industry | Application |\n",
    "|----------|-------------|\n",
    "| Autonomous Driving | Road/lane detection, obstacle segmentation |\n",
    "| Medical Imaging | Tumor segmentation, organ identification |\n",
    "| Satellite Imagery | Land use classification, building detection |\n",
    "| Robotics | Scene understanding, navigation |\n",
    "| Fashion | Clothing segmentation, virtual try-on |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"semantic-segmentation\"\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Understand Data Format\n",
    "\n",
    "SageMaker Semantic Segmentation requires:\n",
    "\n",
    "### Directory Structure\n",
    "```\n",
    "train/\n",
    "  image001.jpg\n",
    "  image002.jpg\n",
    "train_annotation/\n",
    "  image001.png  # Grayscale mask, pixel values = class IDs\n",
    "  image002.png\n",
    "validation/\n",
    "validation_annotation/\n",
    "```\n",
    "\n",
    "### Annotation Format\n",
    "- **PNG images** (uncompressed)\n",
    "- **Grayscale** where each pixel value is the class ID\n",
    "- Same dimensions as input image\n",
    "- Class IDs from 0 to (num_classes - 1)\n",
    "- Background is typically class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_segmentation_mask(height=256, width=256, num_classes=5, seed=None):\n",
    "    \"\"\"\n",
    "    Generate a synthetic segmentation mask with random shapes.\n",
    "    \n",
    "    Returns:\n",
    "        mask: 2D numpy array with class IDs\n",
    "        class_names: List of class names\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    class_names = ['background', 'road', 'building', 'vegetation', 'sky']\n",
    "    \n",
    "    # Start with background\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Add sky (top portion)\n",
    "    sky_height = np.random.randint(height // 4, height // 2)\n",
    "    mask[:sky_height, :] = 4  # sky class\n",
    "    \n",
    "    # Add road (bottom center)\n",
    "    road_width = width // 3\n",
    "    road_start = (width - road_width) // 2\n",
    "    mask[height * 2 // 3:, road_start:road_start + road_width] = 1  # road class\n",
    "    \n",
    "    # Add random buildings\n",
    "    num_buildings = np.random.randint(2, 5)\n",
    "    for _ in range(num_buildings):\n",
    "        bw = np.random.randint(30, 80)\n",
    "        bh = np.random.randint(50, 120)\n",
    "        bx = np.random.randint(0, width - bw)\n",
    "        by = np.random.randint(sky_height, height - bh - 20)\n",
    "        mask[by:by + bh, bx:bx + bw] = 2  # building class\n",
    "    \n",
    "    # Add vegetation patches\n",
    "    num_veg = np.random.randint(3, 7)\n",
    "    for _ in range(num_veg):\n",
    "        vw = np.random.randint(20, 60)\n",
    "        vh = np.random.randint(20, 50)\n",
    "        vx = np.random.randint(0, width - vw)\n",
    "        vy = np.random.randint(sky_height, height - vh)\n",
    "        mask[vy:vy + vh, vx:vx + vw] = 3  # vegetation class\n",
    "    \n",
    "    return mask, class_names\n",
    "\n",
    "# Generate sample mask\n",
    "sample_mask, class_names = generate_synthetic_segmentation_mask(seed=42)\n",
    "\n",
    "print(f\"Mask shape: {sample_mask.shape}\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Unique values in mask: {np.unique(sample_mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentation(mask, class_names, title=\"Segmentation Mask\"):\n",
    "    \"\"\"\n",
    "    Visualize segmentation mask with colored classes.\n",
    "    \"\"\"\n",
    "    num_classes = len(class_names)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, num_classes))\n",
    "    \n",
    "    # Create colored mask\n",
    "    colored_mask = np.zeros((*mask.shape, 3))\n",
    "    for class_id in range(num_classes):\n",
    "        colored_mask[mask == class_id] = colors[class_id][:3]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Grayscale mask (actual annotation format)\n",
    "    axes[0].imshow(mask, cmap='gray', vmin=0, vmax=num_classes-1)\n",
    "    axes[0].set_title(\"Annotation (Grayscale - pixel values = class IDs)\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Colored visualization\n",
    "    axes[1].imshow(colored_mask)\n",
    "    axes[1].set_title(title)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Legend\n",
    "    patches = [mpatches.Patch(color=colors[i], label=class_names[i]) \n",
    "               for i in range(num_classes)]\n",
    "    axes[1].legend(handles=patches, loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_segmentation(sample_mask, class_names, \"Synthetic Scene Segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Training Configuration\n",
    "\n",
    "### Architecture Options\n",
    "\n",
    "| Algorithm | Description | Best For |\n",
    "|-----------|-------------|----------|\n",
    "| FCN | Fully Convolutional Network | Fast inference |\n",
    "| PSP | Pyramid Scene Parsing | Multi-scale features |\n",
    "| DeepLabV3 | Atrous convolutions | High accuracy |\n",
    "\n",
    "### Backbone Networks\n",
    "\n",
    "| Backbone | Depth | Speed | Accuracy |\n",
    "|----------|-------|-------|----------|\n",
    "| ResNet-50 | 50 layers | Faster | Good |\n",
    "| ResNet-101 | 101 layers | Slower | Better |\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `num_classes` | Number of segmentation classes | Required |\n",
    "| `algorithm` | fcn, psp, deeplab | fcn |\n",
    "| `backbone` | resnet-50, resnet-101 | resnet-50 |\n",
    "| `use_pretrained_model` | Use pretrained backbone | True |\n",
    "| `epochs` | Training epochs | 30 |\n",
    "| `learning_rate` | Initial learning rate | 0.001 |\n",
    "| `mini_batch_size` | Batch size | 16 |\n",
    "| `crop_size` | Training crop size | 240 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Semantic Segmentation container image\n",
    "semantic_segmentation_image = retrieve(\n",
    "    framework='semantic-segmentation',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"Semantic Segmentation Image URI: {semantic_segmentation_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example estimator configuration (for reference)\n",
    "print(\"\"\"\n",
    "Semantic Segmentation Estimator Configuration:\n",
    "===============================================\n",
    "\n",
    "semantic_segmentation_estimator = Estimator(\n",
    "    image_uri=semantic_segmentation_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',  # GPU required\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='semantic-segmentation'\n",
    ")\n",
    "\n",
    "hyperparameters = {\n",
    "    \"num_classes\": 5,\n",
    "    \"algorithm\": \"deeplab\",         # fcn, psp, or deeplab\n",
    "    \"backbone\": \"resnet-50\",\n",
    "    \"use_pretrained_model\": \"True\",\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"lr_scheduler\": \"poly\",\n",
    "    \"mini_batch_size\": 16,\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"crop_size\": 480,\n",
    "    \"num_training_samples\": 1000,\n",
    "}\n",
    "\n",
    "Data channels:\n",
    "- train: Training images (JPG)\n",
    "- train_annotation: Training masks (PNG, grayscale)\n",
    "- validation: Validation images\n",
    "- validation_annotation: Validation masks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Understanding Model Output\n",
    "\n",
    "The model outputs a grayscale PNG image where each pixel value is the predicted class ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_prediction_comparison(ground_truth, class_names):\n",
    "    \"\"\"\n",
    "    Simulate model prediction and compare to ground truth.\n",
    "    \"\"\"\n",
    "    # Simulate prediction with some noise\n",
    "    prediction = ground_truth.copy()\n",
    "    \n",
    "    # Add some random misclassifications (simulate model errors)\n",
    "    noise_mask = np.random.random(ground_truth.shape) < 0.05  # 5% noise\n",
    "    random_classes = np.random.randint(0, len(class_names), ground_truth.shape)\n",
    "    prediction[noise_mask] = random_classes[noise_mask]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Generate prediction\n",
    "prediction = simulate_prediction_comparison(sample_mask, class_names)\n",
    "\n",
    "# Visualize comparison\n",
    "num_classes = len(class_names)\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, num_classes))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Ground truth\n",
    "colored_gt = np.zeros((*sample_mask.shape, 3))\n",
    "for class_id in range(num_classes):\n",
    "    colored_gt[sample_mask == class_id] = colors[class_id][:3]\n",
    "axes[0].imshow(colored_gt)\n",
    "axes[0].set_title(\"Ground Truth\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Prediction\n",
    "colored_pred = np.zeros((*prediction.shape, 3))\n",
    "for class_id in range(num_classes):\n",
    "    colored_pred[prediction == class_id] = colors[class_id][:3]\n",
    "axes[1].imshow(colored_pred)\n",
    "axes[1].set_title(\"Prediction\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Error map\n",
    "error_map = (sample_mask != prediction).astype(float)\n",
    "axes[2].imshow(error_map, cmap='Reds')\n",
    "axes[2].set_title(f\"Errors (red pixels: {error_map.sum():.0f})\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Legend\n",
    "patches = [mpatches.Patch(color=colors[i], label=class_names[i]) \n",
    "           for i in range(num_classes)]\n",
    "fig.legend(handles=patches, loc='lower center', ncol=num_classes, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation Metrics\n",
    "\n",
    "### Mean Intersection over Union (mIoU)\n",
    "Primary metric for semantic segmentation:\n",
    "```\n",
    "IoU(class) = True Positives / (True Positives + False Positives + False Negatives)\n",
    "mIoU = Average IoU across all classes\n",
    "```\n",
    "\n",
    "### Pixel Accuracy\n",
    "```\n",
    "Pixel Accuracy = Correctly classified pixels / Total pixels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_segmentation_metrics(ground_truth, prediction, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate segmentation metrics.\n",
    "    \n",
    "    Returns:\n",
    "        dict with pixel_accuracy, mean_iou, and per-class IoU\n",
    "    \"\"\"\n",
    "    # Pixel accuracy\n",
    "    correct = (ground_truth == prediction).sum()\n",
    "    total = ground_truth.size\n",
    "    pixel_accuracy = correct / total\n",
    "    \n",
    "    # Per-class IoU\n",
    "    iou_per_class = []\n",
    "    for class_id in range(num_classes):\n",
    "        gt_mask = ground_truth == class_id\n",
    "        pred_mask = prediction == class_id\n",
    "        \n",
    "        intersection = (gt_mask & pred_mask).sum()\n",
    "        union = (gt_mask | pred_mask).sum()\n",
    "        \n",
    "        if union > 0:\n",
    "            iou = intersection / union\n",
    "        else:\n",
    "            iou = 0.0\n",
    "        \n",
    "        iou_per_class.append(iou)\n",
    "    \n",
    "    mean_iou = np.mean(iou_per_class)\n",
    "    \n",
    "    return {\n",
    "        'pixel_accuracy': pixel_accuracy,\n",
    "        'mean_iou': mean_iou,\n",
    "        'iou_per_class': iou_per_class\n",
    "    }\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_segmentation_metrics(sample_mask, prediction, len(class_names))\n",
    "\n",
    "print(\"Segmentation Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Pixel Accuracy: {metrics['pixel_accuracy']:.4f}\")\n",
    "print(f\"Mean IoU (mIoU): {metrics['mean_iou']:.4f}\")\n",
    "print(f\"\\nPer-class IoU:\")\n",
    "for i, (name, iou) in enumerate(zip(class_names, metrics['iou_per_class'])):\n",
    "    print(f\"  {name}: {iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class IoU\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "colors_bar = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "bars = ax.barh(class_names, metrics['iou_per_class'], color=colors_bar)\n",
    "\n",
    "ax.set_xlabel('IoU Score')\n",
    "ax.set_title(f'Per-Class IoU (mIoU = {metrics[\"mean_iou\"]:.4f})')\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, iou in zip(bars, metrics['iou_per_class']):\n",
    "    ax.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "           f'{iou:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **Data Format**:\n",
    "   - Images: JPG/PNG in train/, validation/ folders\n",
    "   - Annotations: Grayscale PNG (pixel value = class ID)\n",
    "   - Same filename for image and annotation\n",
    "\n",
    "2. **Architecture Options**:\n",
    "   - FCN: Fast, basic segmentation\n",
    "   - PSP: Pyramid pooling for multi-scale\n",
    "   - DeepLabV3: Atrous convolutions for accuracy\n",
    "\n",
    "3. **Key Hyperparameters**:\n",
    "   - `algorithm`: fcn, psp, deeplab\n",
    "   - `backbone`: resnet-50, resnet-101\n",
    "   - `crop_size`: Training patch size\n",
    "\n",
    "4. **Output Format**:\n",
    "   - Grayscale PNG mask\n",
    "   - Same dimensions as input\n",
    "   - Pixel values are class IDs\n",
    "\n",
    "5. **Evaluation Metrics**:\n",
    "   - Mean IoU (mIoU): Primary metric\n",
    "   - Pixel Accuracy: Overall accuracy\n",
    "   - Per-class IoU: Class-specific performance\n",
    "\n",
    "### Instance Requirements\n",
    "\n",
    "| Task | Instance Types |\n",
    "|------|----------------|\n",
    "| Training | ml.p2.xlarge, ml.p3.2xlarge, ml.g4dn.xlarge |\n",
    "| Inference | ml.c5.xlarge (CPU) or GPU instances |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Prepare real image data with pixel annotations\n",
    "- Use SageMaker Ground Truth for semantic segmentation labeling\n",
    "- Experiment with different algorithms (FCN vs PSP vs DeepLab)\n",
    "- Try different crop sizes for your image resolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
