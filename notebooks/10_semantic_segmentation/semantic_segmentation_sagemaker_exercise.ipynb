{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Semantic Segmentation Exercise\n",
    "\n",
    "This notebook demonstrates Amazon SageMaker's **Semantic Segmentation** algorithm for pixel-level image classification.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare pixel-level annotations\n",
    "2. How to configure and understand semantic segmentation hyperparameters\n",
    "3. How to train a segmentation model\n",
    "4. How to interpret and evaluate segmentation masks\n",
    "\n",
    "## What is Semantic Segmentation?\n",
    "\n",
    "Semantic Segmentation classifies **every pixel** in an image, assigning each pixel to a class. Unlike object detection which provides bounding boxes, segmentation provides exact object shapes and boundaries.\n",
    "\n",
    "**Key Difference from Other CV Tasks:**\n",
    "\n",
    "| Task | Output | Granularity | Example Output |\n",
    "|------|--------|-------------|----------------|\n",
    "| Classification | Single label | Image-level | \"cat\" |\n",
    "| Object Detection | Bounding boxes + labels | Object-level | Box around cat |\n",
    "| **Semantic Segmentation** | Pixel mask | Pixel-level | Every cat pixel labeled |\n",
    "| Instance Segmentation | Pixel mask per instance | Pixel + Instance | Cat1 pixels, Cat2 pixels |\n",
    "\n",
    "**Note on Instance vs Semantic:**\n",
    "- **Semantic**: All cats labeled as \"cat\" (same color)\n",
    "- **Instance**: Each cat has unique ID (different colors per cat)\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Industry | Application |\n",
    "|----------|-------------|\n",
    "| Autonomous Driving | Road/lane detection, pedestrian segmentation, obstacle identification |\n",
    "| Medical Imaging | Tumor segmentation, organ identification, cell counting |\n",
    "| Satellite/Aerial Imagery | Land use classification, building detection, flood mapping |\n",
    "| Robotics | Scene understanding, navigation, manipulation |\n",
    "| Fashion/Retail | Clothing segmentation, virtual try-on, background removal |\n",
    "| Agriculture | Crop health analysis, weed detection, yield estimation |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ Important: Training Cost Warning\n",
    "\n",
    "<div style=\"background-color: #090907ff; border: 1px solid #ffc107; border-radius: 5px; padding: 15px; margin: 10px 0;\">\n",
    "\n",
    "### GPU Requirements and Costs\n",
    "\n",
    "**Semantic Segmentation training requires GPU instances.** This algorithm processes entire images at pixel-level resolution, making it computationally intensive.\n",
    "\n",
    "| Instance Type | GPU | Memory | On-Demand Price* |\n",
    "|---------------|-----|--------|------------------|\n",
    "| ml.p2.xlarge | 1x K80 | 12 GB | ~$1.26/hour |\n",
    "| ml.p3.2xlarge | 1x V100 | 16 GB | ~$3.83/hour |\n",
    "| ml.p3.8xlarge | 4x V100 | 64 GB | ~$14.69/hour |\n",
    "| ml.g4dn.xlarge | 1x T4 | 16 GB | ~$0.74/hour |\n",
    "| ml.g4dn.2xlarge | 1x T4 | 32 GB | ~$1.05/hour |\n",
    "| ml.g5.xlarge | 1x A10G | 24 GB | ~$1.41/hour |\n",
    "\n",
    "*Prices are approximate for us-west-2 and subject to change. Check [AWS SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/) for current rates.\n",
    "\n",
    "### Cost Estimation Example\n",
    "\n",
    "Training a typical semantic segmentation model:\n",
    "- **30 epochs** with **5,000 images** (512x512): ~3-5 hours on ml.p3.2xlarge\n",
    "- **Estimated cost**: $11.49 - $19.15 for training\n",
    "- Semantic segmentation is **more expensive** than image classification due to pixel-level processing\n",
    "\n",
    "### Cost-Saving Recommendations\n",
    "\n",
    "1. **Use Spot Instances**: Can save up to 70% - add `use_spot_instances=True` to Estimator\n",
    "2. **Start with ml.g4dn.xlarge**: Most cost-effective GPU option (~$0.74/hour)\n",
    "3. **Use transfer learning**: Set `use_pretrained_model=True` - critical for this task\n",
    "4. **Reduce crop_size**: Start with `crop_size=240` instead of 480 for faster iteration\n",
    "5. **Use FCN over DeepLab**: FCN is faster, use DeepLab only when you need maximum accuracy\n",
    "6. **Start with fewer epochs**: Use 10 epochs to validate setup before full training\n",
    "7. **Use ResNet-50 backbone**: Faster than ResNet-101, often sufficient accuracy\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"semantic-segmentation\"\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understand Data Format\n",
    "\n",
    "SageMaker Semantic Segmentation requires a specific data format with paired images and annotation masks.\n",
    "\n",
    "### Directory Structure\n",
    "```\n",
    "train/\n",
    "  image001.jpg\n",
    "  image002.jpg\n",
    "train_annotation/\n",
    "  image001.png  # Grayscale mask, pixel values = class IDs\n",
    "  image002.png\n",
    "validation/\n",
    "  image001.jpg\n",
    "validation_annotation/\n",
    "  image001.png\n",
    "```\n",
    "\n",
    "### Critical Requirements\n",
    "\n",
    "1. **Matching filenames**: `train/image001.jpg` must have `train_annotation/image001.png`\n",
    "2. **Same dimensions**: Annotation mask must be EXACT same size as input image\n",
    "3. **PNG format for annotations**: Must be PNG (lossless compression preserves class IDs)\n",
    "4. **Grayscale annotations**: Single channel where pixel value = class ID\n",
    "5. **Class ID range**: 0 to (num_classes - 1)\n",
    "6. **No compression artifacts**: Use PNG or uncompressed formats for annotations\n",
    "\n",
    "### Annotation Format Details\n",
    "\n",
    "| Aspect | Requirement |\n",
    "|--------|-------------|\n",
    "| Format | PNG (8-bit grayscale) |\n",
    "| Channels | 1 (grayscale) |\n",
    "| Pixel values | 0 to num_classes-1 |\n",
    "| Background | Typically class 0 |\n",
    "| Dimensions | Must match input image exactly |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation Example\n",
    "\n",
    "For a scene with 5 classes:\n",
    "\n",
    "```\n",
    "Class 0: Background (pixel value = 0)\n",
    "Class 1: Road (pixel value = 1)\n",
    "Class 2: Building (pixel value = 2)\n",
    "Class 3: Vegetation (pixel value = 3)\n",
    "Class 4: Sky (pixel value = 4)\n",
    "```\n",
    "\n",
    "The annotation PNG looks grayscale to humans (very dark) because values 0-4 are nearly black. When visualized with a colormap, each class gets a distinct color.\n",
    "\n",
    "**Common Mistake**: Using RGB colors in annotations. The algorithm expects grayscale class IDs, not RGB colors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Synthetic Data - Limitations and Purpose\n",
    "\n",
    "<div style=\"background-color: #030405ff; border: 1px solid #0c5460; border-radius: 5px; padding: 15px; margin: 10px 0;\">\n",
    "\n",
    "### ⚠️ Important: Why We Can't Truly Simulate Semantic Segmentation\n",
    "\n",
    "Like other deep learning vision tasks, Semantic Segmentation requires **real images** with actual visual features.\n",
    "\n",
    "**Why synthetic data doesn't work for training:**\n",
    "1. **Pixel-level features matter**: The model learns to recognize boundaries, textures, and context at every pixel\n",
    "2. **Random shapes don't generalize**: A model trained on synthetic rectangles won't segment real buildings\n",
    "3. **Context is crucial**: Real scenes have natural spatial relationships (sky above buildings, road at bottom)\n",
    "\n",
    "**What we CAN demonstrate:**\n",
    "- ✅ Annotation format (grayscale PNG with class IDs)\n",
    "- ✅ Data structure and pairing\n",
    "- ✅ Evaluation metrics (IoU, Dice, pixel accuracy)\n",
    "- ✅ Output visualization and interpretation\n",
    "- ✅ Hyperparameter configuration\n",
    "\n",
    "**For actual training, you need:**\n",
    "- Real images with pixel-level annotations\n",
    "- Public datasets: Cityscapes, PASCAL VOC, ADE20K, COCO-Stuff\n",
    "- SageMaker Ground Truth for custom labeling (supports semantic segmentation)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_segmentation_mask(height=256, width=256, num_classes=5, seed=None):\n",
    "    \"\"\"\n",
    "    Generate a synthetic segmentation mask with random shapes.\n",
    "    \n",
    "    NOTE: This is for FORMAT DEMONSTRATION ONLY.\n",
    "    Real training requires actual images with corresponding annotations.\n",
    "    \n",
    "    Args:\n",
    "        height: Image height in pixels\n",
    "        width: Image width in pixels\n",
    "        num_classes: Number of segmentation classes\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        mask: 2D numpy array with class IDs (dtype=uint8)\n",
    "        class_names: List of class names\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    class_names = ['background', 'road', 'building', 'vegetation', 'sky']\n",
    "    \n",
    "    # Start with background\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Add sky (top portion)\n",
    "    sky_height = np.random.randint(height // 4, height // 2)\n",
    "    mask[:sky_height, :] = 4  # sky class\n",
    "    \n",
    "    # Add road (bottom center)\n",
    "    road_width = width // 3\n",
    "    road_start = (width - road_width) // 2\n",
    "    mask[height * 2 // 3:, road_start:road_start + road_width] = 1  # road class\n",
    "    \n",
    "    # Add random buildings\n",
    "    num_buildings = np.random.randint(2, 5)\n",
    "    for _ in range(num_buildings):\n",
    "        bw = np.random.randint(30, 80)\n",
    "        bh = np.random.randint(50, 120)\n",
    "        bx = np.random.randint(0, width - bw)\n",
    "        by = np.random.randint(sky_height, height - bh - 20)\n",
    "        mask[by:by + bh, bx:bx + bw] = 2  # building class\n",
    "    \n",
    "    # Add vegetation patches\n",
    "    num_veg = np.random.randint(3, 7)\n",
    "    for _ in range(num_veg):\n",
    "        vw = np.random.randint(20, 60)\n",
    "        vh = np.random.randint(20, 50)\n",
    "        vx = np.random.randint(0, width - vw)\n",
    "        vy = np.random.randint(sky_height, height - vh)\n",
    "        mask[vy:vy + vh, vx:vx + vw] = 3  # vegetation class\n",
    "    \n",
    "    return mask, class_names\n",
    "\n",
    "# Generate sample mask\n",
    "sample_mask, class_names = generate_synthetic_segmentation_mask(seed=42)\n",
    "\n",
    "print(f\"Mask shape: {sample_mask.shape}\")\n",
    "print(f\"Mask dtype: {sample_mask.dtype} (must be uint8 for PNG)\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Unique values in mask: {np.unique(sample_mask)}\")\n",
    "print(f\"Value range: {sample_mask.min()} to {sample_mask.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentation(mask, class_names, title=\"Segmentation Mask\"):\n",
    "    \"\"\"\n",
    "    Visualize segmentation mask with colored classes.\n",
    "    \n",
    "    Shows both the raw annotation format and a human-readable colored version.\n",
    "    \"\"\"\n",
    "    num_classes = len(class_names)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, num_classes))\n",
    "    \n",
    "    # Create colored mask for visualization\n",
    "    colored_mask = np.zeros((*mask.shape, 3))\n",
    "    for class_id in range(num_classes):\n",
    "        colored_mask[mask == class_id] = colors[class_id][:3]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Raw annotation (grayscale - what's actually saved)\n",
    "    im1 = axes[0].imshow(mask, cmap='gray', vmin=0, vmax=num_classes-1)\n",
    "    axes[0].set_title(\"Raw Annotation (Grayscale PNG)\\nPixel values = Class IDs\")\n",
    "    axes[0].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[0], label='Class ID', ticks=range(num_classes))\n",
    "    \n",
    "    # Colored visualization (for human understanding)\n",
    "    axes[1].imshow(colored_mask)\n",
    "    axes[1].set_title(f\"{title}\\n(Colored for visualization)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Legend\n",
    "    patches = [mpatches.Patch(color=colors[i], label=f\"{i}: {class_names[i]}\") \n",
    "               for i in range(num_classes)]\n",
    "    axes[1].legend(handles=patches, loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_segmentation(sample_mask, class_names, \"Synthetic Urban Scene Segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_class_distribution(mask, class_names):\n",
    "    \"\"\"\n",
    "    Analyze pixel distribution across classes.\n",
    "    \n",
    "    Important for understanding class imbalance - common issue in segmentation.\n",
    "    \"\"\"\n",
    "    total_pixels = mask.size\n",
    "    class_pixels = {}\n",
    "    \n",
    "    for i, name in enumerate(class_names):\n",
    "        count = (mask == i).sum()\n",
    "        class_pixels[name] = {\n",
    "            'count': count,\n",
    "            'percentage': count / total_pixels * 100\n",
    "        }\n",
    "    \n",
    "    return class_pixels\n",
    "\n",
    "# Analyze distribution\n",
    "distribution = analyze_class_distribution(sample_mask, class_names)\n",
    "\n",
    "print(\"Class Distribution (Pixel Count):\")\n",
    "print(\"=\" * 50)\n",
    "for name, data in distribution.items():\n",
    "    bar = '#' * int(data['percentage'] / 2)\n",
    "    print(f\"{name:12s}: {data['count']:6d} pixels ({data['percentage']:5.1f}%) {bar}\")\n",
    "\n",
    "# Check for class imbalance\n",
    "percentages = [d['percentage'] for d in distribution.values()]\n",
    "imbalance_ratio = max(percentages) / max(min(percentages), 0.1)\n",
    "print(f\"\\nClass imbalance ratio: {imbalance_ratio:.1f}x\")\n",
    "if imbalance_ratio > 10:\n",
    "    print(\"⚠️  Significant class imbalance! Consider using weighted loss or class balancing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "\n",
    "# Bar chart\n",
    "percentages = [distribution[name]['percentage'] for name in class_names]\n",
    "bars = axes[0].bar(class_names, percentages, color=colors)\n",
    "axes[0].set_ylabel('Percentage of Image')\n",
    "axes[0].set_title('Class Distribution (Pixel Percentage)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, pct in zip(bars, percentages):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{pct:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(percentages, labels=class_names, colors=colors, autopct='%1.1f%%',\n",
    "           startangle=90)\n",
    "axes[1].set_title('Class Distribution (Pie Chart)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Training Configuration and Hyperparameters\n",
    "\n",
    "### Understanding Semantic Segmentation Hyperparameters\n",
    "\n",
    "SageMaker's Semantic Segmentation algorithm has specific hyperparameters for architecture selection and training configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Required Parameters\n",
    "\n",
    "**num_classes** (Required)\n",
    "- Total number of segmentation classes INCLUDING background\n",
    "- Must match the maximum class ID + 1 in your annotations\n",
    "- Example: Classes 0-4 → `num_classes=5`\n",
    "\n",
    "**num_training_samples** (Required)\n",
    "- Total number of training images\n",
    "- Used for learning rate scheduling\n",
    "- Must match your actual dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Parameters\n",
    "\n",
    "**algorithm**\n",
    "- The segmentation architecture to use\n",
    "- Options: `fcn`, `psp`, `deeplab`\n",
    "\n",
    "| Algorithm | Full Name | Description | Best For |\n",
    "|-----------|-----------|-------------|----------|\n",
    "| `fcn` | Fully Convolutional Network | Simple encoder-decoder | Fast inference, basic tasks |\n",
    "| `psp` | Pyramid Scene Parsing | Multi-scale pooling | Scene parsing, global context |\n",
    "| `deeplab` | DeepLabV3 | Atrous/dilated convolutions | **Maximum accuracy**, fine boundaries |\n",
    "\n",
    "- Default: `fcn`\n",
    "- Recommendation: Use `deeplab` for best accuracy, `fcn` for speed\n",
    "\n",
    "**backbone**\n",
    "- The encoder network that extracts features\n",
    "- Options: `resnet-50`, `resnet-101`\n",
    "\n",
    "| Backbone | Layers | Speed | Accuracy | Memory |\n",
    "|----------|--------|-------|----------|--------|\n",
    "| `resnet-50` | 50 | Faster | Good | ~4GB |\n",
    "| `resnet-101` | 101 | Slower | Better | ~6GB |\n",
    "\n",
    "- Default: `resnet-50`\n",
    "- Recommendation: Start with `resnet-50`, upgrade if needed\n",
    "\n",
    "**use_pretrained_model**\n",
    "- Whether to initialize backbone with ImageNet pretrained weights\n",
    "- `True`: **Highly recommended** - critical for segmentation\n",
    "- `False`: Train from scratch (needs much more data)\n",
    "- Default: `True`\n",
    "- Note: Pretrained weights help the model understand basic visual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters\n",
    "\n",
    "**epochs**\n",
    "- Number of complete passes through training data\n",
    "- Segmentation often needs more epochs than classification\n",
    "- Typical range: 30-100 depending on dataset size\n",
    "- Default: `30`\n",
    "\n",
    "**mini_batch_size**\n",
    "- Number of images per batch\n",
    "- **Memory intensive**: Segmentation processes full images\n",
    "- Reduce if you get OOM (Out of Memory) errors\n",
    "- Typical range: 4-16 depending on crop_size and GPU\n",
    "- Default: `16`\n",
    "- Rule of thumb: crop_size=480 → batch_size 4-8; crop_size=240 → batch_size 8-16\n",
    "\n",
    "**learning_rate**\n",
    "- Initial learning rate\n",
    "- Lower for fine-tuning pretrained models\n",
    "- Typical range: 0.001 - 0.01\n",
    "- Default: `0.001`\n",
    "\n",
    "**lr_scheduler**\n",
    "- Learning rate decay strategy\n",
    "- Options: `poly`, `step`, `cosine`\n",
    "\n",
    "| Scheduler | Description | Formula |\n",
    "|-----------|-------------|--------|\n",
    "| `poly` | Polynomial decay | LR × (1 - iter/max_iter)^power |\n",
    "| `step` | Step decay at specific epochs | LR × factor at each step |\n",
    "| `cosine` | Cosine annealing | Smooth decay following cosine |\n",
    "\n",
    "- Default: `poly` (recommended for segmentation)\n",
    "\n",
    "**lr_scheduler_step**\n",
    "- For `step` scheduler: epochs at which to reduce LR\n",
    "- Format: comma-separated (e.g., `\"10,20,30\"`)\n",
    "\n",
    "**crop_size**\n",
    "- Size of random crops during training\n",
    "- Images are randomly cropped to this size for training\n",
    "- Larger = more context, but slower and more memory\n",
    "- Must be smaller than smallest image dimension\n",
    "- Typical values: 240, 320, 480, 512\n",
    "- Default: `240`\n",
    "- **Important**: Larger crop_size requires reducing batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Parameters\n",
    "\n",
    "**optimizer**\n",
    "- Optimization algorithm\n",
    "- Options: `sgd`, `adam`, `rmsprop`\n",
    "- Default: `sgd` (recommended with momentum)\n",
    "\n",
    "**momentum**\n",
    "- Momentum for SGD optimizer\n",
    "- Typical value: 0.9\n",
    "- Default: `0.9`\n",
    "\n",
    "**weight_decay**\n",
    "- L2 regularization\n",
    "- Helps prevent overfitting\n",
    "- Typical range: 0.0001 - 0.001\n",
    "- Default: `0.0001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Parameters\n",
    "\n",
    "**sync_bn**\n",
    "- Synchronized Batch Normalization across GPUs\n",
    "- Useful for multi-GPU training with small batch sizes\n",
    "- Default: `False`\n",
    "\n",
    "**validation_crop_size**\n",
    "- Crop size for validation\n",
    "- Often same as crop_size or larger\n",
    "- Default: Same as crop_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Semantic Segmentation container image\n",
    "semantic_segmentation_image = retrieve(\n",
    "    framework='semantic-segmentation',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"Semantic Segmentation Image URI: {semantic_segmentation_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete hyperparameter configuration with explanations\n",
    "hyperparameters = {\n",
    "    # === REQUIRED PARAMETERS ===\n",
    "    \"num_classes\": 5,                    # Number of segmentation classes (including background)\n",
    "    \"num_training_samples\": 1000,        # Total training images\n",
    "    \n",
    "    # === ARCHITECTURE ===\n",
    "    \"algorithm\": \"deeplab\",              # fcn, psp, or deeplab (deeplab for best accuracy)\n",
    "    \"backbone\": \"resnet-50\",             # Feature extractor backbone\n",
    "    \"use_pretrained_model\": \"True\",      # Transfer learning (critical for segmentation)\n",
    "    \n",
    "    # === TRAINING PARAMETERS ===\n",
    "    \"epochs\": 30,                        # Training epochs\n",
    "    \"mini_batch_size\": 8,                # Batch size (reduce if OOM)\n",
    "    \"learning_rate\": 0.001,              # Initial learning rate\n",
    "    \"lr_scheduler\": \"poly\",              # Learning rate scheduler\n",
    "    \n",
    "    # === OPTIMIZER ===\n",
    "    \"optimizer\": \"sgd\",                  # Optimizer algorithm\n",
    "    \"momentum\": 0.9,                     # SGD momentum\n",
    "    \"weight_decay\": 0.0001,              # L2 regularization\n",
    "    \n",
    "    # === CROP SIZE ===\n",
    "    \"crop_size\": 480,                    # Training crop size (pixels)\n",
    "}\n",
    "\n",
    "print(\"Semantic Segmentation Hyperparameters:\")\n",
    "print(\"=\" * 55)\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Estimator Configuration\n",
    "# NOTE: Do NOT run training without actual image data!\n",
    "\n",
    "print(\"\"\"\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "                    EXAMPLE ESTIMATOR CONFIGURATION\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "⚠️  WARNING: Running this training job will incur GPU costs!\n",
    "    Semantic segmentation is compute-intensive.\n",
    "    Estimated cost: $10-30 depending on dataset and epochs.\n",
    "\n",
    "# Standard training (On-Demand)\n",
    "semantic_segmentation_estimator = Estimator(\n",
    "    image_uri=semantic_segmentation_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',  # GPU required!\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='semantic-segmentation',\n",
    "    max_run=3600 * 6,  # 6 hour max runtime\n",
    ")\n",
    "\n",
    "# Cost-saving alternative with Spot Instances (up to 70% savings)\n",
    "semantic_segmentation_estimator_spot = Estimator(\n",
    "    image_uri=semantic_segmentation_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',  # Most cost-effective GPU\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='semantic-segmentation-spot',\n",
    "    use_spot_instances=True,         # Enable Spot pricing\n",
    "    max_wait=3600 * 8,               # Max time to wait for spot capacity\n",
    "    max_run=3600 * 6,                # Max training time\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "semantic_segmentation_estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "# Data channels configuration\n",
    "# train: s3://bucket/prefix/train/  (JPG images)\n",
    "# train_annotation: s3://bucket/prefix/train_annotation/  (PNG masks)\n",
    "# validation: s3://bucket/prefix/validation/  (JPG images)\n",
    "# validation_annotation: s3://bucket/prefix/validation_annotation/  (PNG masks)\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Understanding Model Output\n",
    "\n",
    "The model outputs a **grayscale PNG image** where each pixel value is the predicted class ID.\n",
    "\n",
    "**Output Format:**\n",
    "- Same dimensions as input image\n",
    "- 8-bit grayscale PNG\n",
    "- Pixel values range from 0 to (num_classes - 1)\n",
    "- Each pixel independently classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_prediction(ground_truth, class_names, noise_level=0.05):\n",
    "    \"\"\"\n",
    "    Simulate model prediction with controlled noise.\n",
    "    \n",
    "    In reality, errors tend to occur at boundaries between classes.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth: Ground truth mask\n",
    "        class_names: List of class names\n",
    "        noise_level: Fraction of pixels to randomly misclassify\n",
    "    \n",
    "    Returns:\n",
    "        Simulated prediction mask\n",
    "    \"\"\"\n",
    "    prediction = ground_truth.copy()\n",
    "    \n",
    "    # Add random misclassifications\n",
    "    noise_mask = np.random.random(ground_truth.shape) < noise_level\n",
    "    random_classes = np.random.randint(0, len(class_names), ground_truth.shape)\n",
    "    prediction[noise_mask] = random_classes[noise_mask]\n",
    "    \n",
    "    # Add boundary noise (more realistic - errors at edges)\n",
    "    from scipy import ndimage\n",
    "    edges = ndimage.sobel(ground_truth.astype(float)) != 0\n",
    "    edge_noise = np.random.random(ground_truth.shape) < 0.15  # Higher error at edges\n",
    "    boundary_noise = edges & edge_noise\n",
    "    prediction[boundary_noise] = random_classes[boundary_noise]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Generate prediction\n",
    "try:\n",
    "    prediction = simulate_prediction(sample_mask, class_names)\n",
    "except ImportError:\n",
    "    # Fallback if scipy not available\n",
    "    prediction = sample_mask.copy()\n",
    "    noise_mask = np.random.random(sample_mask.shape) < 0.05\n",
    "    random_classes = np.random.randint(0, len(class_names), sample_mask.shape)\n",
    "    prediction[noise_mask] = random_classes[noise_mask]\n",
    "\n",
    "print(f\"Ground truth shape: {sample_mask.shape}\")\n",
    "print(f\"Prediction shape: {prediction.shape}\")\n",
    "print(f\"Matching pixels: {(sample_mask == prediction).sum()} / {sample_mask.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction_comparison(ground_truth, prediction, class_names):\n",
    "    \"\"\"\n",
    "    Compare ground truth and prediction with error visualization.\n",
    "    \"\"\"\n",
    "    num_classes = len(class_names)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, num_classes))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    # Ground truth (colored)\n",
    "    colored_gt = np.zeros((*ground_truth.shape, 3))\n",
    "    for class_id in range(num_classes):\n",
    "        colored_gt[ground_truth == class_id] = colors[class_id][:3]\n",
    "    axes[0].imshow(colored_gt)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Prediction (colored)\n",
    "    colored_pred = np.zeros((*prediction.shape, 3))\n",
    "    for class_id in range(num_classes):\n",
    "        colored_pred[prediction == class_id] = colors[class_id][:3]\n",
    "    axes[1].imshow(colored_pred)\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Error map (binary)\n",
    "    error_map = (ground_truth != prediction).astype(float)\n",
    "    axes[2].imshow(error_map, cmap='Reds')\n",
    "    error_count = error_map.sum()\n",
    "    error_pct = error_count / error_map.size * 100\n",
    "    axes[2].set_title(f\"Errors (Red)\\n{error_count:.0f} pixels ({error_pct:.1f}%)\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Correct pixels (green) vs errors (red)\n",
    "    comparison = np.zeros((*ground_truth.shape, 3))\n",
    "    comparison[ground_truth == prediction] = [0, 0.7, 0]  # Green for correct\n",
    "    comparison[ground_truth != prediction] = [0.9, 0, 0]  # Red for errors\n",
    "    axes[3].imshow(comparison)\n",
    "    axes[3].set_title(f\"Accuracy Map\\nGreen=Correct, Red=Error\")\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    # Legend\n",
    "    patches = [mpatches.Patch(color=colors[i], label=class_names[i]) \n",
    "               for i in range(num_classes)]\n",
    "    fig.legend(handles=patches, loc='lower center', ncol=num_classes, fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.show()\n",
    "\n",
    "visualize_prediction_comparison(sample_mask, prediction, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Evaluation Metrics Deep Dive\n",
    "\n",
    "Semantic segmentation uses pixel-level metrics. The primary metric is **Mean Intersection over Union (mIoU)**.\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "| Metric | Description | Formula | Range |\n",
    "|--------|-------------|---------|-------|\n",
    "| **Pixel Accuracy** | % pixels correctly classified | Correct / Total | 0-1 |\n",
    "| **Mean Accuracy** | Average per-class accuracy | Mean(class accuracies) | 0-1 |\n",
    "| **IoU (per class)** | Intersection / Union | TP / (TP+FP+FN) | 0-1 |\n",
    "| **mIoU** | Mean IoU across classes | Mean(class IoUs) | 0-1 |\n",
    "| **Dice Score** | Similar to IoU, used in medical | 2×TP / (2×TP+FP+FN) | 0-1 |\n",
    "| **Frequency Weighted IoU** | IoU weighted by class frequency | Weighted mean | 0-1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union (IoU) Explained\n",
    "\n",
    "IoU measures the overlap between prediction and ground truth for a single class:\n",
    "\n",
    "```\n",
    "IoU = Area of Overlap / Area of Union\n",
    "    = True Positives / (True Positives + False Positives + False Negatives)\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- IoU = 1.0: Perfect segmentation\n",
    "- IoU = 0.5: Acceptable for many applications\n",
    "- IoU = 0.0: Complete miss (no overlap)\n",
    "\n",
    "**Why IoU over Pixel Accuracy?**\n",
    "- Pixel accuracy can be misleading with class imbalance\n",
    "- Example: 90% background → predicting all background gives 90% accuracy but 0 IoU for other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(ground_truth, prediction, class_id):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union for a single class.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth: Ground truth mask\n",
    "        prediction: Predicted mask\n",
    "        class_id: Class ID to calculate IoU for\n",
    "    \n",
    "    Returns:\n",
    "        IoU score (0-1)\n",
    "    \"\"\"\n",
    "    gt_mask = ground_truth == class_id\n",
    "    pred_mask = prediction == class_id\n",
    "    \n",
    "    intersection = (gt_mask & pred_mask).sum()\n",
    "    union = (gt_mask | pred_mask).sum()\n",
    "    \n",
    "    if union == 0:\n",
    "        return float('nan')  # Class not present in either\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def calculate_dice(ground_truth, prediction, class_id):\n",
    "    \"\"\"\n",
    "    Calculate Dice coefficient (F1 score) for a single class.\n",
    "    \n",
    "    Dice = 2 * |A ∩ B| / (|A| + |B|)\n",
    "    \n",
    "    Commonly used in medical image segmentation.\n",
    "    \"\"\"\n",
    "    gt_mask = ground_truth == class_id\n",
    "    pred_mask = prediction == class_id\n",
    "    \n",
    "    intersection = (gt_mask & pred_mask).sum()\n",
    "    total = gt_mask.sum() + pred_mask.sum()\n",
    "    \n",
    "    if total == 0:\n",
    "        return float('nan')\n",
    "    \n",
    "    return 2 * intersection / total\n",
    "\n",
    "\n",
    "def calculate_segmentation_metrics(ground_truth, prediction, num_classes, class_names):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive segmentation metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all metrics\n",
    "    \"\"\"\n",
    "    # Pixel accuracy\n",
    "    correct = (ground_truth == prediction).sum()\n",
    "    total = ground_truth.size\n",
    "    pixel_accuracy = correct / total\n",
    "    \n",
    "    # Per-class metrics\n",
    "    iou_per_class = []\n",
    "    dice_per_class = []\n",
    "    accuracy_per_class = []\n",
    "    class_pixels = []\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        iou = calculate_iou(ground_truth, prediction, class_id)\n",
    "        dice = calculate_dice(ground_truth, prediction, class_id)\n",
    "        \n",
    "        # Per-class accuracy (recall)\n",
    "        gt_mask = ground_truth == class_id\n",
    "        if gt_mask.sum() > 0:\n",
    "            class_acc = ((prediction == class_id) & gt_mask).sum() / gt_mask.sum()\n",
    "        else:\n",
    "            class_acc = float('nan')\n",
    "        \n",
    "        iou_per_class.append(iou)\n",
    "        dice_per_class.append(dice)\n",
    "        accuracy_per_class.append(class_acc)\n",
    "        class_pixels.append(gt_mask.sum())\n",
    "    \n",
    "    # Mean metrics (excluding NaN classes)\n",
    "    valid_ious = [x for x in iou_per_class if not np.isnan(x)]\n",
    "    valid_dices = [x for x in dice_per_class if not np.isnan(x)]\n",
    "    valid_accs = [x for x in accuracy_per_class if not np.isnan(x)]\n",
    "    \n",
    "    mean_iou = np.mean(valid_ious) if valid_ious else 0\n",
    "    mean_dice = np.mean(valid_dices) if valid_dices else 0\n",
    "    mean_accuracy = np.mean(valid_accs) if valid_accs else 0\n",
    "    \n",
    "    # Frequency weighted IoU\n",
    "    total_pixels = sum(class_pixels)\n",
    "    freq_weighted_iou = sum(\n",
    "        (pixels / total_pixels) * iou \n",
    "        for pixels, iou in zip(class_pixels, iou_per_class) \n",
    "        if not np.isnan(iou) and pixels > 0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'pixel_accuracy': pixel_accuracy,\n",
    "        'mean_accuracy': mean_accuracy,\n",
    "        'mean_iou': mean_iou,\n",
    "        'mean_dice': mean_dice,\n",
    "        'freq_weighted_iou': freq_weighted_iou,\n",
    "        'iou_per_class': dict(zip(class_names, iou_per_class)),\n",
    "        'dice_per_class': dict(zip(class_names, dice_per_class)),\n",
    "        'accuracy_per_class': dict(zip(class_names, accuracy_per_class)),\n",
    "    }\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_segmentation_metrics(sample_mask, prediction, len(class_names), class_names)\n",
    "\n",
    "print(\"Segmentation Evaluation Metrics:\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\\n  Overall Metrics:\")\n",
    "print(f\"    Pixel Accuracy:      {metrics['pixel_accuracy']:.4f} ({metrics['pixel_accuracy']*100:.1f}%)\")\n",
    "print(f\"    Mean Accuracy:       {metrics['mean_accuracy']:.4f}\")\n",
    "print(f\"    Mean IoU (mIoU):     {metrics['mean_iou']:.4f}\")\n",
    "print(f\"    Mean Dice:           {metrics['mean_dice']:.4f}\")\n",
    "print(f\"    Freq. Weighted IoU:  {metrics['freq_weighted_iou']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display per-class metrics in a formatted table\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Class':>15s} {'IoU':>10s} {'Dice':>10s} {'Accuracy':>10s}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for name in class_names:\n",
    "    iou = metrics['iou_per_class'][name]\n",
    "    dice = metrics['dice_per_class'][name]\n",
    "    acc = metrics['accuracy_per_class'][name]\n",
    "    \n",
    "    iou_str = f\"{iou:.4f}\" if not np.isnan(iou) else \"N/A\"\n",
    "    dice_str = f\"{dice:.4f}\" if not np.isnan(dice) else \"N/A\"\n",
    "    acc_str = f\"{acc:.4f}\" if not np.isnan(acc) else \"N/A\"\n",
    "    \n",
    "    print(f\"{name:>15s} {iou_str:>10s} {dice_str:>10s} {acc_str:>10s}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Mean':>15s} {metrics['mean_iou']:>10.4f} {metrics['mean_dice']:>10.4f} {metrics['mean_accuracy']:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class IoU\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "\n",
    "# IoU bar chart\n",
    "ious = [metrics['iou_per_class'][name] for name in class_names]\n",
    "ious_clean = [x if not np.isnan(x) else 0 for x in ious]\n",
    "bars = axes[0].barh(class_names, ious_clean, color=colors)\n",
    "axes[0].set_xlabel('IoU Score')\n",
    "axes[0].set_title(f'Per-Class IoU (mIoU = {metrics[\"mean_iou\"]:.4f})')\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].axvline(x=metrics['mean_iou'], color='red', linestyle='--', linewidth=2, label=f'mIoU')\n",
    "axes[0].legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, iou in zip(bars, ious):\n",
    "    label = f'{iou:.3f}' if not np.isnan(iou) else 'N/A'\n",
    "    axes[0].text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "                label, va='center')\n",
    "\n",
    "# Dice bar chart\n",
    "dices = [metrics['dice_per_class'][name] for name in class_names]\n",
    "dices_clean = [x if not np.isnan(x) else 0 for x in dices]\n",
    "bars2 = axes[1].barh(class_names, dices_clean, color=colors)\n",
    "axes[1].set_xlabel('Dice Score')\n",
    "axes[1].set_title(f'Per-Class Dice (Mean = {metrics[\"mean_dice\"]:.4f})')\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].axvline(x=metrics['mean_dice'], color='red', linestyle='--', linewidth=2, label='Mean Dice')\n",
    "axes[1].legend()\n",
    "\n",
    "for bar, dice in zip(bars2, dices):\n",
    "    label = f'{dice:.3f}' if not np.isnan(dice) else 'N/A'\n",
    "    axes[1].text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "                label, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for Segmentation\n",
    "\n",
    "A pixel-level confusion matrix shows which classes get confused with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(ground_truth, prediction, num_classes):\n",
    "    \"\"\"\n",
    "    Compute pixel-level confusion matrix.\n",
    "    \n",
    "    Row = Ground Truth class\n",
    "    Column = Predicted class\n",
    "    Values = Number of pixels\n",
    "    \"\"\"\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "    \n",
    "    for gt_class in range(num_classes):\n",
    "        for pred_class in range(num_classes):\n",
    "            cm[gt_class, pred_class] = ((ground_truth == gt_class) & \n",
    "                                        (prediction == pred_class)).sum()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, normalize=False, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as heatmap.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        # Normalize by row (ground truth)\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        cm_display = np.divide(cm, row_sums, where=row_sums!=0)\n",
    "        fmt = '.2f'\n",
    "    else:\n",
    "        cm_display = cm\n",
    "        fmt = 'd'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    im = ax.imshow(cm_display, interpolation='nearest', cmap='Blues')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    ax.set(xticks=np.arange(len(class_names)),\n",
    "           yticks=np.arange(len(class_names)),\n",
    "           xticklabels=class_names,\n",
    "           yticklabels=class_names,\n",
    "           xlabel='Predicted Class',\n",
    "           ylabel='Ground Truth Class',\n",
    "           title=title)\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm_display.max() / 2\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            value = cm_display[i, j]\n",
    "            if normalize:\n",
    "                text = f'{value:.2f}'\n",
    "            else:\n",
    "                if value > 1000:\n",
    "                    text = f'{value/1000:.1f}K'\n",
    "                else:\n",
    "                    text = f'{value}'\n",
    "            ax.text(j, i, text,\n",
    "                   ha='center', va='center',\n",
    "                   color='white' if cm_display[i, j] > thresh else 'black',\n",
    "                   fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "cm = compute_confusion_matrix(sample_mask, prediction, len(class_names))\n",
    "\n",
    "print(\"Pixel-Level Confusion Matrix:\")\n",
    "plot_confusion_matrix(cm, class_names, normalize=False, \n",
    "                     title=\"Confusion Matrix (Pixel Counts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix (shows per-class recall)\n",
    "print(\"Normalized Confusion Matrix (Per-class Recall):\")\n",
    "plot_confusion_matrix(cm, class_names, normalize=True,\n",
    "                     title=\"Confusion Matrix (Normalized by Ground Truth)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: CloudWatch Training Metrics\n",
    "\n",
    "During training, SageMaker Semantic Segmentation emits these metrics to CloudWatch:\n",
    "\n",
    "| Metric | Description | Good Values |\n",
    "|--------|-------------|-------------|\n",
    "| `train:loss` | Training loss | Decreasing |\n",
    "| `validation:loss` | Validation loss | Decreasing |\n",
    "| `train:mIoU` | Training mean IoU | Increasing |\n",
    "| `validation:mIoU` | Validation mean IoU | **Primary metric** - Increasing |\n",
    "| `train:pixacc` | Training pixel accuracy | Increasing |\n",
    "| `validation:pixacc` | Validation pixel accuracy | Increasing |\n",
    "\n",
    "### What to Watch For\n",
    "\n",
    "**Healthy Training:**\n",
    "- Loss decreasing steadily\n",
    "- mIoU increasing on both training and validation\n",
    "- Small gap between training and validation metrics\n",
    "\n",
    "**Overfitting Signs:**\n",
    "- Training mIoU keeps improving, validation mIoU plateaus/decreases\n",
    "- Large gap between training and validation metrics\n",
    "\n",
    "**Underfitting Signs:**\n",
    "- Both metrics are poor and improve very slowly\n",
    "- Consider: larger model, more data, longer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training metrics over epochs\n",
    "np.random.seed(42)\n",
    "epochs = 30\n",
    "\n",
    "# Simulate healthy training curves for segmentation\n",
    "# Segmentation typically has slower convergence than classification\n",
    "\n",
    "# Loss curves\n",
    "base_loss = 1.5\n",
    "train_loss = [max(0.1, base_loss * np.exp(-0.08 * e) + np.random.normal(0, 0.02)) for e in range(epochs)]\n",
    "val_loss = [max(0.15, base_loss * np.exp(-0.06 * e) + 0.05 + np.random.normal(0, 0.03)) for e in range(epochs)]\n",
    "\n",
    "# mIoU curves\n",
    "base_miou = 0.2\n",
    "train_miou = [min(0.85, base_miou + 0.02 * e + np.random.normal(0, 0.01)) for e in range(epochs)]\n",
    "val_miou = [min(0.75, base_miou - 0.02 + 0.018 * e + np.random.normal(0, 0.015)) for e in range(epochs)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(range(1, epochs + 1), train_loss, 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0].plot(range(1, epochs + 1), val_loss, 'r--', label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Progress: Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# mIoU plot\n",
    "axes[1].plot(range(1, epochs + 1), train_miou, 'b-', label='Training mIoU', linewidth=2)\n",
    "axes[1].plot(range(1, epochs + 1), val_miou, 'r--', label='Validation mIoU', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Mean IoU')\n",
    "axes[1].set_title('Training Progress: Mean IoU')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training mIoU: {train_miou[-1]:.4f}\")\n",
    "print(f\"Final Validation mIoU: {val_miou[-1]:.4f}\")\n",
    "print(f\"Final Training Loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_loss[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "### 1. Data Format\n",
    "- **Images**: JPG/PNG in `train/`, `validation/` folders\n",
    "- **Annotations**: Grayscale PNG where pixel value = class ID\n",
    "- **Matching**: Same filename for image and annotation\n",
    "- **Dimensions**: Annotation must match image size exactly\n",
    "\n",
    "### 2. Architecture Options\n",
    "\n",
    "| Algorithm | Description | Speed | Accuracy |\n",
    "|-----------|-------------|-------|----------|\n",
    "| FCN | Fully Convolutional | Fast | Good |\n",
    "| PSP | Pyramid Scene Parsing | Medium | Better |\n",
    "| DeepLab | Atrous Convolutions | Slower | Best |\n",
    "\n",
    "### 3. Key Hyperparameters\n",
    "\n",
    "| Category | Parameters |\n",
    "|----------|------------|\n",
    "| Architecture | `algorithm`, `backbone`, `use_pretrained_model` |\n",
    "| Training | `epochs`, `mini_batch_size`, `learning_rate`, `crop_size` |\n",
    "| Optimizer | `optimizer`, `momentum`, `weight_decay` |\n",
    "| Scheduler | `lr_scheduler` |\n",
    "\n",
    "### 4. Output Format\n",
    "- Grayscale PNG mask\n",
    "- Same dimensions as input image\n",
    "- Pixel values = predicted class IDs (0 to num_classes-1)\n",
    "\n",
    "### 5. Evaluation Metrics\n",
    "\n",
    "| Metric | Description | Primary? |\n",
    "|--------|-------------|----------|\n",
    "| mIoU | Mean Intersection over Union | **Yes** |\n",
    "| Pixel Accuracy | % pixels correct | No (misleading with imbalance) |\n",
    "| Dice Score | 2×TP/(2×TP+FP+FN) | Medical imaging |\n",
    "| Per-class IoU | IoU for each class | Detailed analysis |\n",
    "\n",
    "### Instance Requirements\n",
    "\n",
    "| Task | Instance Types | Notes |\n",
    "|------|----------------|-------|\n",
    "| Training | ml.g4dn.xlarge, ml.p3.2xlarge | **GPU required**, memory intensive |\n",
    "| Inference | ml.c5.xlarge (CPU), ml.g4dn.xlarge (GPU) | GPU for real-time |\n",
    "\n",
    "### Cost Considerations\n",
    "- Training costs: $10-30+ depending on dataset and settings\n",
    "- **More expensive than classification** due to pixel-level processing\n",
    "- Use Spot Instances for up to 70% savings\n",
    "- Reduce `crop_size` and use `fcn` algorithm for faster iteration\n",
    "- Use ResNet-50 backbone unless you need maximum accuracy\n",
    "\n",
    "### Next Steps\n",
    "1. Obtain real pixel-annotated data (Cityscapes, PASCAL VOC, ADE20K)\n",
    "2. Use SageMaker Ground Truth for custom segmentation labeling\n",
    "3. Experiment with different algorithms (FCN vs PSP vs DeepLab)\n",
    "4. Monitor mIoU during training - it's the key metric\n",
    "5. Address class imbalance if needed (weighted loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [SageMaker Semantic Segmentation Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html)\n",
    "- [Semantic Segmentation Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/segmentation-hyperparameters.html)\n",
    "- [Cityscapes Dataset](https://www.cityscapes-dataset.com/) - Urban scene segmentation\n",
    "- [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) - Multi-class segmentation\n",
    "- [ADE20K](https://groups.csail.mit.edu/vision/datasets/ADE20K/) - Scene parsing dataset\n",
    "- [SageMaker Ground Truth](https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html) - Semantic segmentation labeling\n",
    "- [AWS Pricing Calculator](https://calculator.aws/) - Estimate training costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
