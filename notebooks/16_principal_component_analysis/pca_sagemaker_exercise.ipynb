{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker Principal Component Analysis (PCA) Exercise\n",
    "\n",
    "This notebook demonstrates Amazon SageMaker's **Principal Component Analysis (PCA)** algorithm for dimensionality reduction.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare data for PCA\n",
    "2. How to train a PCA model\n",
    "3. How to interpret and use principal components\n",
    "\n",
    "## What is PCA?\n",
    "\n",
    "PCA is an **unsupervised** algorithm that:\n",
    "- Reduces high-dimensional data to fewer dimensions\n",
    "- Finds directions of maximum variance (principal components)\n",
    "- Components are orthogonal and ordered by explained variance\n",
    "\n",
    "**SageMaker's Implementation:**\n",
    "- Two modes: `regular` and `randomized`\n",
    "- GPU acceleration\n",
    "- Single-pass streaming\n",
    "- Scalable to large datasets\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Application | Description |\n",
    "|-------------|-------------|\n",
    "| Feature reduction | Reduce features before ML model |\n",
    "| Visualization | Project high-dim data to 2D/3D |\n",
    "| Noise reduction | Keep only high-variance components |\n",
    "| Anomaly detection | Outliers in reduced space |\n",
    "| Data compression | Store/transmit fewer features |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"pca\"\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_SAMPLES = 5000\n",
    "NUM_FEATURES = 50\n",
    "NUM_COMPONENTS = 10\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Generate Synthetic High-Dimensional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with many features but only some informative\n",
    "X, y = make_classification(\n",
    "    n_samples=NUM_SAMPLES,\n",
    "    n_features=NUM_FEATURES,\n",
    "    n_informative=10,       # Only 10 features are informative\n",
    "    n_redundant=20,          # 20 features are linear combinations\n",
    "    n_clusters_per_class=3,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Original features: {NUM_FEATURES}\")\n",
    "print(f\"Target components: {NUM_COMPONENTS}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(f\"  Mean range: [{X.mean(axis=0).min():.2f}, {X.mean(axis=0).max():.2f}]\")\n",
    "print(f\"  Std range: [{X.std(axis=0).min():.2f}, {X.std(axis=0).max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation matrix\n",
    "correlation = np.corrcoef(X.T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(correlation, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax.set_title('Feature Correlation Matrix')\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Feature')\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "print(\"Many features are correlated - ideal for PCA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for PCA\n",
    "\n",
    "PCA expects features only in CSV or RecordIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "os.makedirs('data/pca', exist_ok=True)\n",
    "\n",
    "np.savetxt('data/pca/train.csv', X, delimiter=',')\n",
    "\n",
    "print(f\"Saved: data/pca/train.csv ({os.path.getsize('data/pca/train.csv') / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "train_s3_key = f\"{PREFIX}/train/train.csv\"\n",
    "s3_client.upload_file('data/pca/train.csv', BUCKET_NAME, train_s3_key)\n",
    "\n",
    "train_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/train\"\n",
    "print(f\"Data uploaded to: {train_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Train PCA Model\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `num_components` | Number of components to compute | Required |\n",
    "| `feature_dim` | Number of input features | Required |\n",
    "| `mini_batch_size` | Batch size | 500 |\n",
    "| `algorithm_mode` | `regular` or `randomized` | regular |\n",
    "| `subtract_mean` | Center data | True |\n",
    "\n",
    "### Mode Selection\n",
    "\n",
    "| Mode | Best For |\n",
    "|------|----------|\n",
    "| `regular` | Moderate features, moderate samples |\n",
    "| `randomized` | Very large features OR very large samples |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PCA container image\n",
    "pca_image = retrieve(\n",
    "    framework='pca',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"PCA Image URI: {pca_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA estimator\n",
    "pca_estimator = Estimator(\n",
    "    image_uri=pca_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='pca'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "hyperparameters = {\n",
    "    \"num_components\": NUM_COMPONENTS,\n",
    "    \"feature_dim\": NUM_FEATURES,\n",
    "    \"mini_batch_size\": 500,\n",
    "    \"algorithm_mode\": \"regular\",\n",
    "    \"subtract_mean\": \"True\",\n",
    "}\n",
    "\n",
    "pca_estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "print(\"PCA hyperparameters:\")\n",
    "for k, v in hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting PCA training job...\")\n",
    "print(\"This will take approximately 3-5 minutes.\\n\")\n",
    "\n",
    "pca_estimator.fit(\n",
    "    {'train': train_uri},\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "job_name = pca_estimator.latest_training_job.name\n",
    "print(f\"Training job completed: {job_name}\")\n",
    "print(f\"Model artifacts: {pca_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 5: Deploy and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "print(\"Deploying PCA model...\")\n",
    "print(\"This will take approximately 5-7 minutes.\\n\")\n",
    "\n",
    "pca_predictor = pca_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=f'pca-{datetime.now().strftime(\"%Y%m%d%H%M\")}'\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint deployed: {pca_predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Configure predictor\n",
    "pca_predictor.serializer = CSVSerializer()\n",
    "pca_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "def transform_data(data, predictor, batch_size=500):\n",
    "    \"\"\"\n",
    "    Transform data to principal components.\n",
    "    \"\"\"\n",
    "    projections = []\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        response = predictor.predict(batch)\n",
    "        \n",
    "        for pred in response['predictions']:\n",
    "            projections.append(pred['projections'])\n",
    "    \n",
    "    return np.array(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "print(\"Transforming data to principal components...\")\n",
    "X_transformed = transform_data(X, pca_predictor)\n",
    "\n",
    "print(f\"\\nOriginal shape: {X.shape}\")\n",
    "print(f\"Transformed shape: {X_transformed.shape}\")\n",
    "print(f\"Dimensionality reduction: {NUM_FEATURES} -> {NUM_COMPONENTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 2 principal components\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "scatter = ax.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y, cmap='tab10', alpha=0.6)\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_title('Data Projected onto First 2 Principal Components')\n",
    "plt.colorbar(scatter, ax=ax, label='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance in each component\n",
    "component_variance = np.var(X_transformed, axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Variance per component\n",
    "axes[0].bar(range(NUM_COMPONENTS), component_variance)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Variance')\n",
    "axes[0].set_title('Variance per Principal Component')\n",
    "axes[0].set_xticks(range(NUM_COMPONENTS))\n",
    "\n",
    "# Cumulative variance\n",
    "cumulative_variance = np.cumsum(component_variance) / np.sum(component_variance)\n",
    "axes[1].plot(range(NUM_COMPONENTS), cumulative_variance, 'bo-')\n",
    "axes[1].axhline(y=0.9, color='r', linestyle='--', label='90% variance')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Variance Explained')\n",
    "axes[1].set_title('Cumulative Variance Explained')\n",
    "axes[1].set_xticks(range(NUM_COMPONENTS))\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVariance explained by {NUM_COMPONENTS} components: {cumulative_variance[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(X_transformed[:, 0], X_transformed[:, 1], X_transformed[:, 2], \n",
    "                    c=y, cmap='tab10', alpha=0.6)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('First 3 Principal Components')\n",
    "plt.colorbar(scatter, ax=ax, label='Class', shrink=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Step 7: Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "print(f\"Deleting endpoint: {pca_predictor.endpoint_name}\")\n",
    "pca_predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **Data Format**: CSV with features only\n",
    "\n",
    "2. **Key Hyperparameters**:\n",
    "   - `num_components`: Target dimensions\n",
    "   - `feature_dim`: Input dimensions\n",
    "   - `algorithm_mode`: regular or randomized\n",
    "   - `subtract_mean`: Center data\n",
    "\n",
    "3. **Output**: Projections onto principal components\n",
    "\n",
    "4. **Analysis**:\n",
    "   - Variance per component\n",
    "   - Cumulative variance explained\n",
    "   - Visualization in reduced space\n",
    "\n",
    "### Choosing Number of Components\n",
    "\n",
    "| Method | Approach |\n",
    "|--------|----------|\n",
    "| Variance threshold | Keep components for 90-95% variance |\n",
    "| Elbow method | Plot variance, find bend |\n",
    "| Domain knowledge | Based on downstream task needs |\n",
    "\n",
    "### Instance Recommendations\n",
    "\n",
    "| Task | Instance Types |\n",
    "|------|----------------|\n",
    "| Training | ml.m5.large, ml.c5.xlarge, ml.p2.xlarge |\n",
    "| Inference | ml.m5.large, ml.c5.large |\n",
    "\n",
    "### SageMaker PCA Advantages\n",
    "\n",
    "- GPU acceleration\n",
    "- Single-pass algorithm\n",
    "- Near-linear scalability\n",
    "- Streaming for incremental updates\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Use PCA output as input to other ML models\n",
    "- Combine with K-Means for clustering\n",
    "- Apply for anomaly detection in reduced space\n",
    "- Visualize high-dimensional data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
