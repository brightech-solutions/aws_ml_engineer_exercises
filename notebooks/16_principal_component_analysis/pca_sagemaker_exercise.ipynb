{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker Principal Component Analysis (PCA) Exercise\n",
    "\n",
    "This notebook demonstrates Amazon SageMaker's **Principal Component Analysis (PCA)** algorithm for dimensionality reduction.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare data for PCA\n",
    "2. How to configure and understand PCA hyperparameters\n",
    "3. How to train a PCA model\n",
    "4. How to interpret and use principal components\n",
    "\n",
    "## What is PCA?\n",
    "\n",
    "PCA is an **unsupervised** algorithm that:\n",
    "- Reduces high-dimensional data to fewer dimensions\n",
    "- Finds directions of maximum variance (principal components)\n",
    "- Components are orthogonal and ordered by explained variance\n",
    "- Preserves as much information as possible in fewer dimensions\n",
    "\n",
    "**SageMaker's Implementation:**\n",
    "- Two modes: `regular` and `randomized`\n",
    "- GPU acceleration for large datasets\n",
    "- Single-pass streaming algorithm\n",
    "- Near-linear scalability\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Application | Description |\n",
    "|-------------|-------------|\n",
    "| Feature reduction | Reduce features before ML model |\n",
    "| Visualization | Project high-dim data to 2D/3D |\n",
    "| Noise reduction | Keep only high-variance components |\n",
    "| Anomaly detection | Outliers in reduced space |\n",
    "| Data compression | Store/transmit fewer features |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Training Cost Information\n",
    "\n",
    "<div style=\"background-color: #020703ff; border: 1px solid #28a745; border-radius: 5px; padding: 15px; margin: 10px 0;\">\n",
    "\n",
    "### PCA Supports CPU and GPU\n",
    "\n",
    "PCA training is computationally efficient. GPU helps with very large matrices.\n",
    "\n",
    "| Instance Type | Type | On-Demand Price* | Best For |\n",
    "|---------------|------|------------------|----------|\n",
    "| ml.m5.large | CPU | ~$0.13/hour | Small-medium data |\n",
    "| ml.c5.xlarge | CPU | ~$0.24/hour | Medium data |\n",
    "| ml.p2.xlarge | GPU | ~$1.26/hour | Large data, many features |\n",
    "| ml.p3.2xlarge | GPU | ~$3.83/hour | Very large datasets |\n",
    "\n",
    "*Prices are approximate for us-west-2.\n",
    "\n",
    "### Cost Estimation\n",
    "- **Training**: Very fast! Usually 2-5 minutes (~$0.01-0.03)\n",
    "- **Inference endpoint**: ~$0.13/hour for ml.m5.large\n",
    "- PCA is one of the most cost-effective SageMaker algorithms\n",
    "\n",
    "### Mode Selection\n",
    "- Use `regular` mode for moderate features (<5000)\n",
    "- Use `randomized` mode for many features (>5000) or samples\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"pca\"\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_SAMPLES = 5000\n",
    "NUM_FEATURES = 50\n",
    "NUM_COMPONENTS = 10\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Generate Synthetic High-Dimensional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with many features but only some informative\n",
    "X, y = make_classification(\n",
    "    n_samples=NUM_SAMPLES,\n",
    "    n_features=NUM_FEATURES,\n",
    "    n_informative=10,       # Only 10 features are informative\n",
    "    n_redundant=20,          # 20 features are linear combinations\n",
    "    n_clusters_per_class=3,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Original features: {NUM_FEATURES}\")\n",
    "print(f\"Target components: {NUM_COMPONENTS}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(f\"  Mean range: [{X.mean(axis=0).min():.2f}, {X.mean(axis=0).max():.2f}]\")\n",
    "print(f\"  Std range: [{X.std(axis=0).min():.2f}, {X.std(axis=0).max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation matrix\n",
    "correlation = np.corrcoef(X.T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(correlation, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax.set_title('Feature Correlation Matrix')\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Feature')\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "print(\"Many features are correlated - ideal for PCA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for PCA\n",
    "\n",
    "PCA expects features only in CSV or RecordIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "os.makedirs('data/pca', exist_ok=True)\n",
    "\n",
    "np.savetxt('data/pca/train.csv', X, delimiter=',')\n",
    "\n",
    "print(f\"Saved: data/pca/train.csv ({os.path.getsize('data/pca/train.csv') / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "train_s3_key = f\"{PREFIX}/train/train.csv\"\n",
    "s3_client.upload_file('data/pca/train.csv', BUCKET_NAME, train_s3_key)\n",
    "\n",
    "train_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/train\"\n",
    "print(f\"Data uploaded to: {train_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Train PCA Model\n",
    "\n",
    "### Understanding PCA Hyperparameters\n",
    "\n",
    "| Parameter | Description | Default | Recommendation |\n",
    "|-----------|-------------|---------|----------------|\n",
    "| `num_components` | Number of components to compute | Required | Based on variance needs |\n",
    "| `feature_dim` | Number of input features | Required | Must match data |\n",
    "| `mini_batch_size` | Batch size | 500 | 200-1000 |\n",
    "| `algorithm_mode` | `regular` or `randomized` | regular | See below |\n",
    "| `subtract_mean` | Center data before PCA | True | Almost always True |\n",
    "| `extra_components` | Extra components (randomized mode) | -1 | 0-10 for stability |\n",
    "\n",
    "### Mode Selection\n",
    "\n",
    "| Mode | Complexity | Best For |\n",
    "|------|------------|----------|\n",
    "| `regular` | O(min(n²d, nd²)) | Moderate dimensions (<5000 features) |\n",
    "| `randomized` | O(nd × num_components) | Very large dimensions (>5000 features) |\n",
    "\n",
    "**Regular mode**: Computes exact PCA using SVD decomposition\n",
    "**Randomized mode**: Approximate PCA, faster for large matrices\n",
    "\n",
    "### Key Hyperparameter Details\n",
    "\n",
    "**num_components**\n",
    "- How many principal components to keep\n",
    "- Lower = more compression, potential info loss\n",
    "- Higher = less compression, more info retained\n",
    "- Typically choose based on cumulative variance threshold (e.g., 95%)\n",
    "- For visualization: use 2 or 3\n",
    "\n",
    "**subtract_mean**\n",
    "- Centers data by subtracting feature means\n",
    "- **Almost always True** - standard PCA requires centered data\n",
    "- Only set False if data is already centered\n",
    "\n",
    "**mini_batch_size**\n",
    "- Controls memory usage during training\n",
    "- Larger = faster, more memory\n",
    "- Smaller = slower, less memory\n",
    "- 500 is good default\n",
    "\n",
    "### Choosing Number of Components\n",
    "\n",
    "| Method | Description | Threshold |\n",
    "|--------|-------------|-----------|\n",
    "| **Variance threshold** | Keep components explaining X% variance | 90-99% |\n",
    "| **Elbow method** | Plot variance, find bend | Visual |\n",
    "| **Kaiser criterion** | Keep components with eigenvalue > 1 | For standardized data |\n",
    "| **Task-specific** | Based on downstream model needs | Varies |\n",
    "\n",
    "### CloudWatch Training Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| (None reported) | PCA doesn't emit CloudWatch metrics during training |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PCA container image\n",
    "pca_image = retrieve(\n",
    "    framework='pca',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"PCA Image URI: {pca_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA estimator\n",
    "pca_estimator = Estimator(\n",
    "    image_uri=pca_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='pca'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "hyperparameters = {\n",
    "    \"num_components\": NUM_COMPONENTS,\n",
    "    \"feature_dim\": NUM_FEATURES,\n",
    "    \"mini_batch_size\": 500,\n",
    "    \"algorithm_mode\": \"regular\",\n",
    "    \"subtract_mean\": \"True\",\n",
    "}\n",
    "\n",
    "pca_estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "print(\"PCA hyperparameters:\")\n",
    "for k, v in hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting PCA training job...\")\n",
    "print(\"This will take approximately 3-5 minutes.\\n\")\n",
    "\n",
    "pca_estimator.fit(\n",
    "    {'train': train_uri},\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "job_name = pca_estimator.latest_training_job.name\n",
    "print(f\"Training job completed: {job_name}\")\n",
    "print(f\"Model artifacts: {pca_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 5: Deploy and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "print(\"Deploying PCA model...\")\n",
    "print(\"This will take approximately 5-7 minutes.\\n\")\n",
    "\n",
    "pca_predictor = pca_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=f'pca-{datetime.now().strftime(\"%Y%m%d%H%M\")}'\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint deployed: {pca_predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Configure predictor\n",
    "pca_predictor.serializer = CSVSerializer()\n",
    "pca_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "def transform_data(data, predictor, batch_size=500):\n",
    "    \"\"\"\n",
    "    Transform data to principal components.\n",
    "    \"\"\"\n",
    "    projections = []\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        response = predictor.predict(batch)\n",
    "        \n",
    "        for pred in response['predictions']:\n",
    "            projections.append(pred['projections'])\n",
    "    \n",
    "    return np.array(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "print(\"Transforming data to principal components...\")\n",
    "X_transformed = transform_data(X, pca_predictor)\n",
    "\n",
    "print(f\"\\nOriginal shape: {X.shape}\")\n",
    "print(f\"Transformed shape: {X_transformed.shape}\")\n",
    "print(f\"Dimensionality reduction: {NUM_FEATURES} -> {NUM_COMPONENTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 2 principal components\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "scatter = ax.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y, cmap='tab10', alpha=0.6)\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_title('Data Projected onto First 2 Principal Components')\n",
    "plt.colorbar(scatter, ax=ax, label='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance in each component\n",
    "component_variance = np.var(X_transformed, axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Variance per component\n",
    "axes[0].bar(range(NUM_COMPONENTS), component_variance)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Variance')\n",
    "axes[0].set_title('Variance per Principal Component')\n",
    "axes[0].set_xticks(range(NUM_COMPONENTS))\n",
    "\n",
    "# Cumulative variance\n",
    "cumulative_variance = np.cumsum(component_variance) / np.sum(component_variance)\n",
    "axes[1].plot(range(NUM_COMPONENTS), cumulative_variance, 'bo-')\n",
    "axes[1].axhline(y=0.9, color='r', linestyle='--', label='90% variance')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Variance Explained')\n",
    "axes[1].set_title('Cumulative Variance Explained')\n",
    "axes[1].set_xticks(range(NUM_COMPONENTS))\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVariance explained by {NUM_COMPONENTS} components: {cumulative_variance[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(X_transformed[:, 0], X_transformed[:, 1], X_transformed[:, 2], \n",
    "                    c=y, cmap='tab10', alpha=0.6)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('First 3 Principal Components')\n",
    "plt.colorbar(scatter, ax=ax, label='Class', shrink=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Step 7: Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "print(f\"Deleting endpoint: {pca_predictor.endpoint_name}\")\n",
    "pca_predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **Data Format**: CSV with features only (no labels)\n",
    "\n",
    "2. **Key Hyperparameters**:\n",
    "   - `num_components`: Target dimensions (most critical)\n",
    "   - `feature_dim`: Input dimensions\n",
    "   - `algorithm_mode`: regular or randomized\n",
    "   - `subtract_mean`: Center data (almost always True)\n",
    "\n",
    "3. **Output**: Projections onto principal components\n",
    "\n",
    "4. **Analysis**:\n",
    "   - Variance per component\n",
    "   - Cumulative variance explained\n",
    "   - Visualization in reduced space (2D/3D)\n",
    "\n",
    "### Instance Recommendations\n",
    "\n",
    "| Task | Instance Types | Notes |\n",
    "|------|----------------|-------|\n",
    "| Training (small) | ml.m5.large | CPU, <5000 features |\n",
    "| Training (large) | ml.p2.xlarge, ml.p3.2xlarge | GPU, >5000 features |\n",
    "| Inference | ml.m5.large, ml.c5.large | CPU sufficient |\n",
    "\n",
    "### Choosing Number of Components\n",
    "\n",
    "| Target Variance | Typical Use Case |\n",
    "|-----------------|------------------|\n",
    "| 70-80% | Fast, lossy compression |\n",
    "| 90-95% | Good balance (recommended) |\n",
    "| 99%+ | Near-lossless, noise reduction |\n",
    "| 2-3 | Visualization only |\n",
    "\n",
    "### SageMaker PCA Advantages\n",
    "\n",
    "- GPU acceleration for large matrices\n",
    "- Single-pass streaming algorithm\n",
    "- Near-linear scalability\n",
    "- Randomized mode for huge datasets\n",
    "- Supports incremental updates\n",
    "\n",
    "### PCA vs Other Dimensionality Reduction\n",
    "\n",
    "| Method | Type | Best For |\n",
    "|--------|------|----------|\n",
    "| **PCA** | Linear | General purpose, speed |\n",
    "| t-SNE | Non-linear | Visualization, clusters |\n",
    "| UMAP | Non-linear | Visualization, preserves global structure |\n",
    "| Autoencoders | Non-linear | Complex patterns, deep learning |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Standardize features**: If features have different scales, standardize first!\n",
    "2. **Check variance explained**: Ensure you retain enough information\n",
    "3. **Use regular mode** unless features > 5000 or samples are huge\n",
    "4. **Visualize components**: Plot first 2-3 to understand structure\n",
    "5. **Watch for outliers**: PCA is sensitive to outliers\n",
    "\n",
    "### Applications of PCA Output\n",
    "\n",
    "| Application | How to Use |\n",
    "|-------------|------------|\n",
    "| Feature reduction | Use projections as input to ML models |\n",
    "| Visualization | Plot first 2-3 components |\n",
    "| Noise reduction | Reconstruct using only top components |\n",
    "| Anomaly detection | High reconstruction error = anomaly |\n",
    "| Data compression | Store reduced representation |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Use PCA output as input to other ML models\n",
    "- Combine with K-Means for clustering in reduced space\n",
    "- Apply for anomaly detection using reconstruction error\n",
    "- Visualize high-dimensional data in 2D/3D\n",
    "- Compare with other dimensionality reduction (t-SNE, UMAP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
