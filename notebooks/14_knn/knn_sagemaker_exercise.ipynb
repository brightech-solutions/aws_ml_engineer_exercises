{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker K-Nearest Neighbors (k-NN) Exercise\n",
    "\n",
    "This notebook demonstrates Amazon SageMaker's **k-Nearest Neighbors (k-NN)** algorithm for classification and regression.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare data for k-NN\n",
    "2. How to train a k-NN model with indexing\n",
    "3. How to use k-NN for classification and regression\n",
    "\n",
    "## What is k-NN?\n",
    "\n",
    "k-NN is a **non-parametric** algorithm that:\n",
    "- **Classification**: Predicts class based on majority vote of k nearest neighbors\n",
    "- **Regression**: Predicts value based on average of k nearest neighbors\n",
    "\n",
    "**SageMaker's Implementation:**\n",
    "- Uses efficient index structures for fast querying\n",
    "- Supports dimension reduction for large feature spaces\n",
    "- Scales to large datasets\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Application | Type |\n",
    "|-------------|------|\n",
    "| Product recommendation | Classification/Similarity |\n",
    "| Anomaly detection | Classification |\n",
    "| Image similarity search | Nearest neighbor search |\n",
    "| Price prediction | Regression |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"knn\"\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_SAMPLES = 5000\n",
    "NUM_FEATURES = 20\n",
    "NUM_CLASSES = 3\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Generate Synthetic Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=NUM_SAMPLES,\n",
    "    n_features=NUM_FEATURES,\n",
    "    n_informative=15,\n",
    "    n_redundant=3,\n",
    "    n_classes=NUM_CLASSES,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Convert to float32\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {NUM_FEATURES}\")\n",
    "print(f\"Classes: {NUM_CLASSES}\")\n",
    "print(f\"\\nClass distribution (train):\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for c, n in zip(unique, counts):\n",
    "    print(f\"  Class {int(c)}: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first two dimensions\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for c in range(NUM_CLASSES):\n",
    "    mask = y_train == c\n",
    "    ax.scatter(X_train[mask, 0], X_train[mask, 1], \n",
    "              label=f'Class {c}', alpha=0.6)\n",
    "\n",
    "ax.set_xlabel('Feature 0')\n",
    "ax.set_ylabel('Feature 1')\n",
    "ax.set_title('Training Data (First 2 Features)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for k-NN\n",
    "\n",
    "k-NN expects CSV format with label in the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV data with label first\n",
    "os.makedirs('data/knn', exist_ok=True)\n",
    "\n",
    "# Training data: label, features\n",
    "train_data = np.column_stack([y_train, X_train])\n",
    "np.savetxt('data/knn/train.csv', train_data, delimiter=',')\n",
    "\n",
    "# Test data: label, features  \n",
    "test_data = np.column_stack([y_test, X_test])\n",
    "np.savetxt('data/knn/test.csv', test_data, delimiter=',')\n",
    "\n",
    "print(\"Data files created:\")\n",
    "for f in os.listdir('data/knn'):\n",
    "    size = os.path.getsize(f'data/knn/{f}') / 1024\n",
    "    print(f\"  data/knn/{f} ({size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    s3_key = f\"{PREFIX}/{split}/{split}.csv\"\n",
    "    s3_client.upload_file(f'data/knn/{split}.csv', BUCKET_NAME, s3_key)\n",
    "    print(f\"Uploaded: s3://{BUCKET_NAME}/{s3_key}\")\n",
    "\n",
    "train_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/train\"\n",
    "test_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Train k-NN Model\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `k` | Number of neighbors | Required |\n",
    "| `predictor_type` | `classifier` or `regressor` | Required |\n",
    "| `sample_size` | Number of samples for index | Total dataset |\n",
    "| `feature_dim` | Number of features | Required |\n",
    "| `index_type` | `faiss.Flat`, `faiss.IVFFlat`, `faiss.IVFPQ` | faiss.Flat |\n",
    "| `dimension_reduction_type` | `sign`, `fjlt` (dimension reduction) | None |\n",
    "| `dimension_reduction_target` | Target dimension | feature_dim |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get k-NN container image\n",
    "knn_image = retrieve(\n",
    "    framework='knn',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"k-NN Image URI: {knn_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create k-NN estimator\n",
    "knn_estimator = Estimator(\n",
    "    image_uri=knn_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='knn'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "hyperparameters = {\n",
    "    \"k\": 5,                          # Number of neighbors\n",
    "    \"predictor_type\": \"classifier\",  # or \"regressor\"\n",
    "    \"feature_dim\": NUM_FEATURES,\n",
    "    \"sample_size\": len(X_train),\n",
    "    \"index_type\": \"faiss.Flat\",      # Exact search\n",
    "}\n",
    "\n",
    "knn_estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "print(\"k-NN hyperparameters:\")\n",
    "for k, v in hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting k-NN training job...\")\n",
    "print(\"This will take approximately 3-5 minutes.\\n\")\n",
    "\n",
    "knn_estimator.fit(\n",
    "    {\n",
    "        'train': train_uri,\n",
    "        'test': test_uri\n",
    "    },\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "job_name = knn_estimator.latest_training_job.name\n",
    "print(f\"Training job completed: {job_name}\")\n",
    "print(f\"Model artifacts: {knn_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 5: Deploy and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "print(\"Deploying k-NN model...\")\n",
    "print(\"This will take approximately 5-7 minutes.\\n\")\n",
    "\n",
    "knn_predictor = knn_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=f'knn-{datetime.now().strftime(\"%Y%m%d%H%M\")}'\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint deployed: {knn_predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Configure predictor\n",
    "knn_predictor.serializer = CSVSerializer()\n",
    "knn_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "def predict(data, predictor, batch_size=100):\n",
    "    \"\"\"\n",
    "    Get predictions for data.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        response = predictor.predict(batch)\n",
    "        \n",
    "        for pred in response['predictions']:\n",
    "            predictions.append(pred['predicted_label'])\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "print(\"Getting predictions...\")\n",
    "y_pred = predict(X_test, knn_predictor)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 6: Effect of k Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: In production, you would train separate models with different k values\n",
    "# Here we demonstrate the concept\n",
    "\n",
    "print(\"\"\"\n",
    "Effect of k Value:\n",
    "==================\n",
    "\n",
    "| k Value | Characteristics |\n",
    "|---------|----------------|\n",
    "| Small k (1-3) | More sensitive to noise, sharper boundaries |\n",
    "| Medium k (5-10) | Balanced, good for most cases |\n",
    "| Large k (20+) | Smoother boundaries, may miss local patterns |\n",
    "\n",
    "Choosing k:\n",
    "- Use cross-validation to find optimal k\n",
    "- Rule of thumb: k = sqrt(n) for classification\n",
    "- Odd k avoids ties in binary classification\n",
    "- Larger datasets can use larger k\n",
    "\n",
    "For this dataset:\n",
    "- Training samples: {}\n",
    "- Suggested k range: 5-20\n",
    "- sqrt(n) â‰ˆ {:.0f}\n",
    "\"\"\".format(len(X_train), np.sqrt(len(X_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Step 7: Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "print(f\"Deleting endpoint: {knn_predictor.endpoint_name}\")\n",
    "knn_predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **Data Format**: CSV with label in first column\n",
    "\n",
    "2. **Key Hyperparameters**:\n",
    "   - `k`: Number of neighbors\n",
    "   - `predictor_type`: classifier or regressor\n",
    "   - `index_type`: Index structure for search\n",
    "\n",
    "3. **Index Types**:\n",
    "   - `faiss.Flat`: Exact search (small datasets)\n",
    "   - `faiss.IVFFlat`: Approximate (medium datasets)\n",
    "   - `faiss.IVFPQ`: Approximate + compression (large datasets)\n",
    "\n",
    "4. **Output**:\n",
    "   - Classification: Predicted label\n",
    "   - Regression: Predicted value\n",
    "\n",
    "### Instance Recommendations\n",
    "\n",
    "| Task | Instance Types |\n",
    "|------|----------------|\n",
    "| Training | ml.m5.large, ml.c5.xlarge (CPU), ml.p2.xlarge (GPU) |\n",
    "| Inference | ml.m5.large, ml.c5.large |\n",
    "\n",
    "### When to Use k-NN\n",
    "\n",
    "| Good for | Not ideal for |\n",
    "|----------|---------------|\n",
    "| Small-medium datasets | Very large datasets |\n",
    "| Non-linear boundaries | High-dimensional sparse data |\n",
    "| When interpretability matters | When training speed is critical |\n",
    "| Multi-class classification | Streaming data |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different k values with cross-validation\n",
    "- Use dimension reduction for high-dimensional data\n",
    "- Experiment with different index types\n",
    "- Apply to regression problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
