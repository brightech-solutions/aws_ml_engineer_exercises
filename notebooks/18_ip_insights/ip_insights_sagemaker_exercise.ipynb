{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker IP Insights Exercise\n",
    "\n",
    "This notebook demonstrates Amazon SageMaker's **IP Insights** algorithm for detecting anomalous IP address usage patterns.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare entity-IP pair data\n",
    "2. How to configure and understand IP Insights hyperparameters\n",
    "3. How to train an IP Insights model\n",
    "4. How to detect anomalous IP associations and evaluate performance\n",
    "\n",
    "## What is IP Insights?\n",
    "\n",
    "IP Insights is an **unsupervised** algorithm that:\n",
    "- Learns associations between entities (users, accounts) and IP addresses\n",
    "- Creates embeddings for entities and IPs in a shared vector space\n",
    "- Detects when an entity uses an unusual/unexpected IP\n",
    "\n",
    "**Key Concept:**\n",
    "- Learn latent vectors for entities and IPs\n",
    "- Distance between vectors indicates association likelihood\n",
    "- Anomalies are unexpected entity-IP pairs (large distance)\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Application | Description |\n",
    "|-------------|-------------|\n",
    "| Account takeover detection | User logging from unusual IP |\n",
    "| Fraud prevention | Account created from suspicious IP |\n",
    "| Bot detection | Automated access patterns |\n",
    "| Compromised credentials | Login from attacker's IP |\n",
    "| Insider threat detection | Access from unexpected location |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Training Cost Information\n",
    "\n",
    "<div style=\"background-color: #050906ff; border: 1px solid #28a745; border-radius: 5px; padding: 15px; margin: 10px 0;\">\n",
    "\n",
    "### IP Insights Supports CPU and GPU\n",
    "\n",
    "IP Insights can use CPU or GPU. GPU significantly speeds up training for large datasets.\n",
    "\n",
    "| Instance Type | Type | On-Demand Price* | Best For |\n",
    "|---------------|------|------------------|----------|\n",
    "| ml.m5.large | CPU | ~$0.13/hour | Small datasets (<1M events) |\n",
    "| ml.c5.xlarge | CPU | ~$0.24/hour | Medium datasets |\n",
    "| ml.p2.xlarge | GPU | ~$1.26/hour | Large datasets (>1M events) |\n",
    "| ml.p3.2xlarge | GPU | ~$3.83/hour | Very large datasets |\n",
    "\n",
    "*Prices are approximate for us-west-2.\n",
    "\n",
    "### Cost Estimation\n",
    "- **CPU Training**: ~5-10 minutes for small datasets (~$0.02-0.05)\n",
    "- **GPU Training**: Faster for large datasets\n",
    "- **Inference endpoint**: ~$0.13/hour for ml.m5.large (CPU recommended)\n",
    "\n",
    "### Important\n",
    "IP Insights is designed for **security use cases**. Consider the cost-benefit of detecting fraud vs training/inference costs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"ip-insights\"\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_USERS = 1000\n",
    "NUM_EVENTS = 50000\n",
    "ANOMALY_RATE = 0.02\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Generate Synthetic Login Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ip():\n",
    "    \"\"\"Generate random IP address.\"\"\"\n",
    "    return f\"{np.random.randint(1, 255)}.{np.random.randint(0, 255)}.{np.random.randint(0, 255)}.{np.random.randint(1, 255)}\"\n",
    "\n",
    "def generate_login_data(num_users=1000, num_events=50000, anomaly_rate=0.02, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic user login data.\n",
    "    \n",
    "    Each user typically logs in from a small set of IPs.\n",
    "    Anomalies are logins from completely new IPs.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate user IDs\n",
    "    user_ids = [f\"user_{i:05d}\" for i in range(num_users)]\n",
    "    \n",
    "    # Assign typical IPs to each user (1-3 IPs per user)\n",
    "    user_ips = {}\n",
    "    for user_id in user_ids:\n",
    "        num_ips = np.random.randint(1, 4)\n",
    "        user_ips[user_id] = [generate_ip() for _ in range(num_ips)]\n",
    "    \n",
    "    # Generate login events\n",
    "    events = []\n",
    "    anomaly_labels = []\n",
    "    \n",
    "    for _ in range(num_events):\n",
    "        user_id = np.random.choice(user_ids)\n",
    "        \n",
    "        # Determine if this is an anomaly\n",
    "        is_anomaly = np.random.random() < anomaly_rate\n",
    "        \n",
    "        if is_anomaly:\n",
    "            # Use a completely random IP (anomaly)\n",
    "            ip = generate_ip()\n",
    "            # Make sure it's not accidentally a normal IP\n",
    "            while ip in user_ips[user_id]:\n",
    "                ip = generate_ip()\n",
    "        else:\n",
    "            # Use one of the user's typical IPs\n",
    "            ip = np.random.choice(user_ips[user_id])\n",
    "        \n",
    "        events.append({'entity': user_id, 'ip': ip})\n",
    "        anomaly_labels.append(is_anomaly)\n",
    "    \n",
    "    df = pd.DataFrame(events)\n",
    "    df['is_anomaly'] = anomaly_labels\n",
    "    \n",
    "    return df, user_ips\n",
    "\n",
    "# Generate data\n",
    "df, user_normal_ips = generate_login_data(NUM_USERS, NUM_EVENTS, ANOMALY_RATE, RANDOM_STATE)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Anomaly count: {df['is_anomaly'].sum()} ({100*df['is_anomaly'].mean():.1f}%)\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example user's normal IPs\n",
    "sample_user = 'user_00000'\n",
    "print(f\"\\nSample user '{sample_user}' normal IPs:\")\n",
    "print(f\"  {user_normal_ips[sample_user]}\")\n",
    "\n",
    "print(f\"\\nSample user's events:\")\n",
    "user_events = df[df['entity'] == sample_user].head(10)\n",
    "print(user_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for IP Insights\n",
    "\n",
    "IP Insights expects CSV with two columns:\n",
    "- Column 1: Entity identifier (string)\n",
    "- Column 2: IPv4 address (decimal-dot notation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (we'll train on normal data, test on all data)\n",
    "# In production, you wouldn't know which are anomalies during training\n",
    "\n",
    "# For training: use all data (IP Insights learns from patterns)\n",
    "train_df = df[['entity', 'ip']].copy()\n",
    "\n",
    "# For validation: hold out some data\n",
    "np.random.seed(RANDOM_STATE)\n",
    "val_mask = np.random.random(len(df)) < 0.1\n",
    "val_df = df[val_mask][['entity', 'ip']].copy()\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV (no header)\n",
    "os.makedirs('data/ip_insights', exist_ok=True)\n",
    "\n",
    "train_df.to_csv('data/ip_insights/train.csv', index=False, header=False)\n",
    "val_df.to_csv('data/ip_insights/validation.csv', index=False, header=False)\n",
    "\n",
    "# Also save labels for later evaluation\n",
    "df[val_mask][['entity', 'ip', 'is_anomaly']].to_csv('data/ip_insights/val_labels.csv', index=False)\n",
    "\n",
    "print(\"Data files created:\")\n",
    "for f in os.listdir('data/ip_insights'):\n",
    "    size = os.path.getsize(f'data/ip_insights/{f}') / 1024\n",
    "    print(f\"  data/ip_insights/{f} ({size:.1f} KB)\")\n",
    "\n",
    "print(\"\\nFile preview (train.csv):\")\n",
    "with open('data/ip_insights/train.csv', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        print(f\"  {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "for split in ['train', 'validation']:\n",
    "    s3_key = f\"{PREFIX}/{split}/{split}.csv\"\n",
    "    s3_client.upload_file(f'data/ip_insights/{split}.csv', BUCKET_NAME, s3_key)\n",
    "    print(f\"Uploaded: s3://{BUCKET_NAME}/{s3_key}\")\n",
    "\n",
    "train_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/train\"\n",
    "val_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 4: Train IP Insights Model\n",
    "\n",
    "### Understanding IP Insights Hyperparameters\n",
    "\n",
    "| Parameter | Description | Default | Recommendation |\n",
    "|-----------|-------------|---------|----------------|\n",
    "| `num_entity_vectors` | Hash buckets for entities | Required | 2× unique entities |\n",
    "| `vector_dim` | Embedding dimension | 128 | 64-256 |\n",
    "| `epochs` | Training epochs | 10 | 5-50 |\n",
    "| `learning_rate` | Learning rate | 0.001 | 0.0001-0.01 |\n",
    "| `mini_batch_size` | Batch size | 10000 | 1000-50000 |\n",
    "| `num_ip_encoder_layers` | IP encoder depth | 1 | 1-3 |\n",
    "| `random_negative_sampling_rate` | Negative samples per positive | 1 | 1-5 |\n",
    "| `shuffled_negative_sampling_rate` | Shuffled negative samples | 1 | 0-5 |\n",
    "| `weight_decay` | L2 regularization | 0 | 0-0.01 |\n",
    "\n",
    "### Key Hyperparameter Details\n",
    "\n",
    "**num_entity_vectors**\n",
    "- Hash space for entity embeddings\n",
    "- Should be **larger** than number of unique entities\n",
    "- Recommendation: 2× to 3× unique entities\n",
    "- Too small → hash collisions, accuracy loss\n",
    "- Too large → memory waste, potential overfitting\n",
    "\n",
    "**vector_dim**\n",
    "- Dimension of entity and IP embeddings\n",
    "- Higher = more expressive, captures complex patterns\n",
    "- Lower = faster, less memory, may miss patterns\n",
    "- 128 is good default, try 64 for small datasets\n",
    "\n",
    "**random_negative_sampling_rate**\n",
    "- Number of random entity-IP pairs (negatives) per positive pair\n",
    "- Helps model learn what is NOT normal\n",
    "- Higher = stronger negative signal, slower training\n",
    "- 1-2 is usually sufficient\n",
    "\n",
    "**shuffled_negative_sampling_rate**\n",
    "- Creates negatives by shuffling entities with different IPs\n",
    "- More realistic negatives than pure random\n",
    "- Combine with random negative sampling\n",
    "\n",
    "**num_ip_encoder_layers**\n",
    "- Depth of neural network that encodes IP addresses\n",
    "- 1 layer is usually sufficient\n",
    "- 2-3 for more complex IP patterns\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "| Metric | Description | Good Values |\n",
    "|--------|-------------|-------------|\n",
    "| AUC-ROC | Overall detection performance | >0.9 for production |\n",
    "| Precision | True positives / Predicted positives | Depends on cost of FP |\n",
    "| Recall | True positives / Actual anomalies | Depends on cost of FN |\n",
    "\n",
    "### CloudWatch Training Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `train:loss` | Training loss |\n",
    "| `validation:loss` | Validation loss |\n",
    "\n",
    "### Understanding the Output\n",
    "\n",
    "**dot_product score:**\n",
    "- **Higher** = entity-IP pair is more normal/expected\n",
    "- **Lower** = entity-IP pair is more anomalous\n",
    "- Negate for anomaly score (higher = more anomalous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get IP Insights container image\n",
    "ip_insights_image = retrieve(\n",
    "    framework='ipinsights',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"IP Insights Image URI: {ip_insights_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IP Insights estimator\n",
    "ip_insights_estimator = Estimator(\n",
    "    image_uri=ip_insights_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='ip-insights'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "hyperparameters = {\n",
    "    \"num_entity_vectors\": NUM_USERS * 2,  # Hash space larger than users\n",
    "    \"vector_dim\": 128,\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"mini_batch_size\": 1000,\n",
    "    \"random_negative_sampling_rate\": 1,\n",
    "}\n",
    "\n",
    "ip_insights_estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "print(\"IP Insights hyperparameters:\")\n",
    "for k, v in hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting IP Insights training job...\")\n",
    "print(\"This will take approximately 5-10 minutes.\\n\")\n",
    "\n",
    "ip_insights_estimator.fit(\n",
    "    {\n",
    "        'train': train_uri,\n",
    "        'validation': val_uri\n",
    "    },\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "job_name = ip_insights_estimator.latest_training_job.name\n",
    "print(f\"Training job completed: {job_name}\")\n",
    "print(f\"Model artifacts: {ip_insights_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 5: Deploy and Score Associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "print(\"Deploying IP Insights model...\")\n",
    "print(\"This will take approximately 5-7 minutes.\\n\")\n",
    "\n",
    "ip_predictor = ip_insights_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=f'ip-insights-{datetime.now().strftime(\"%Y%m%d%H%M\")}'\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint deployed: {ip_predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Configure predictor\n",
    "ip_predictor.serializer = CSVSerializer()\n",
    "ip_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "def get_anomaly_scores(entities, ips, predictor, batch_size=500):\n",
    "    \"\"\"\n",
    "    Get anomaly scores for entity-IP pairs.\n",
    "    \n",
    "    Higher scores = more anomalous (less likely association)\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    # Prepare data\n",
    "    data = [[e, ip] for e, ip in zip(entities, ips)]\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        # Convert to CSV format\n",
    "        csv_batch = '\\n'.join([f\"{row[0]},{row[1]}\" for row in batch])\n",
    "        \n",
    "        response = predictor.predict(csv_batch)\n",
    "        \n",
    "        for pred in response['predictions']:\n",
    "            # dot_product: lower = more anomalous\n",
    "            # We negate to make higher = more anomalous\n",
    "            scores.append(-pred['dot_product'])\n",
    "    \n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation labels\n",
    "val_labels_df = pd.read_csv('data/ip_insights/val_labels.csv')\n",
    "\n",
    "# Get scores\n",
    "print(\"Getting anomaly scores...\")\n",
    "scores = get_anomaly_scores(\n",
    "    val_labels_df['entity'].tolist(),\n",
    "    val_labels_df['ip'].tolist(),\n",
    "    ip_predictor\n",
    ")\n",
    "\n",
    "val_labels_df['anomaly_score'] = scores\n",
    "\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(f\"  Normal events - Mean: {scores[~val_labels_df['is_anomaly']].mean():.4f}\")\n",
    "print(f\"  Anomaly events - Mean: {scores[val_labels_df['is_anomaly']].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve\n",
    "\n",
    "# Calculate AUC\n",
    "true_labels = val_labels_df['is_anomaly'].astype(int)\n",
    "auc = roc_auc_score(true_labels, scores)\n",
    "\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "# Plot score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Score distribution\n",
    "axes[0].hist(scores[~val_labels_df['is_anomaly']], bins=50, alpha=0.5, label='Normal', color='blue')\n",
    "axes[0].hist(scores[val_labels_df['is_anomaly']], bins=50, alpha=0.5, label='Anomaly', color='red')\n",
    "axes[0].set_xlabel('Anomaly Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Score Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(true_labels, scores)\n",
    "axes[1].plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top anomalies\n",
    "val_labels_df_sorted = val_labels_df.sort_values('anomaly_score', ascending=False)\n",
    "\n",
    "print(\"Top 15 Highest Anomaly Scores:\")\n",
    "print(\"=\" * 70)\n",
    "print(val_labels_df_sorted[['entity', 'ip', 'anomaly_score', 'is_anomaly']].head(15).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Step 7: Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "print(f\"Deleting endpoint: {ip_predictor.endpoint_name}\")\n",
    "ip_predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **Data Format**: CSV with entity (string) and IP (decimal-dot notation)\n",
    "\n",
    "2. **Key Hyperparameters**:\n",
    "   - `num_entity_vectors`: Hash space for entities (2-3× unique entities)\n",
    "   - `vector_dim`: Embedding dimension (64-256)\n",
    "   - `random_negative_sampling_rate`: Negative samples for training\n",
    "   - `epochs`, `learning_rate`: Training parameters\n",
    "\n",
    "3. **Output**: `dot_product` score\n",
    "   - Higher dot_product = more normal\n",
    "   - Lower dot_product = more anomalous\n",
    "\n",
    "4. **Threshold Selection**:\n",
    "   - Based on business requirements (cost of FP vs FN)\n",
    "   - Use validation data with known anomalies\n",
    "   - Consider percentile-based thresholds\n",
    "\n",
    "### Instance Recommendations\n",
    "\n",
    "| Task | Instance Types | Notes |\n",
    "|------|----------------|-------|\n",
    "| Training (small) | ml.m5.large | CPU, <1M events |\n",
    "| Training (large) | ml.p2.xlarge, ml.p3.2xlarge | GPU, >1M events |\n",
    "| Inference | ml.m5.large, ml.c5.large | CPU recommended |\n",
    "\n",
    "### Score Interpretation\n",
    "\n",
    "| Score (negated dot_product) | Meaning |\n",
    "|-----------------------------|---------|\n",
    "| < 50th percentile | Normal activity |\n",
    "| 50th - 90th percentile | Slightly unusual |\n",
    "| 90th - 99th percentile | Suspicious, investigate |\n",
    "| > 99th percentile | Highly anomalous, alert |\n",
    "\n",
    "### Threshold Selection for Security\n",
    "\n",
    "| Priority | Focus | Threshold |\n",
    "|----------|-------|-----------|\n",
    "| Catch all threats | High recall | Lower threshold (more alerts) |\n",
    "| Reduce alert fatigue | High precision | Higher threshold (fewer alerts) |\n",
    "| Balanced | F1 score | Medium threshold |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Hash space sizing**: Use 2-3× unique entities to minimize collisions\n",
    "2. **Monitor score distribution**: Track over time for drift\n",
    "3. **Combine with other signals**: IP reputation, geo-location, time patterns\n",
    "4. **Retrain periodically**: User behavior changes over time\n",
    "5. **Set up alerting**: Integrate with SIEM or security tools\n",
    "\n",
    "### Integration Patterns\n",
    "\n",
    "**Real-time Detection:**\n",
    "```\n",
    "User Login → SageMaker Endpoint → Score → Alert System\n",
    "```\n",
    "\n",
    "**Batch Detection:**\n",
    "```\n",
    "Daily Logs → Batch Transform → Flag Anomalies → Security Review\n",
    "```\n",
    "\n",
    "**Streaming:**\n",
    "```\n",
    "Kinesis Stream → Lambda → SageMaker Endpoint → SNS Alert\n",
    "```\n",
    "\n",
    "### Comparison with Other Anomaly Detection\n",
    "\n",
    "| Method | Best For | IP Insights Advantage |\n",
    "|--------|----------|----------------------|\n",
    "| Rule-based | Known threats | Catches unknown patterns |\n",
    "| IP blacklists | Known bad IPs | Detects first-time attacks |\n",
    "| Geo-location | Location-based | More granular per-entity |\n",
    "| IP Insights | Entity-specific patterns | Learns normal behavior per user |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Apply to real login/access log data\n",
    "- Combine with time-based features (unusual hours)\n",
    "- Set up alerting for high-score events\n",
    "- Integrate with authentication systems (MFA trigger)\n",
    "- Build dashboard for security team review\n",
    "- Monitor and retrain as user behavior evolves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
