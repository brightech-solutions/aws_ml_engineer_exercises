{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete XGBoost Tutorial: From Basics to Advanced\n",
    "\n",
    "This hands-on tutorial will guide you through XGBoost with practical exercises.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Data Generation\n",
    "2. Exploratory Data Analysis\n",
    "3. Basic XGBoost Model\n",
    "4. Understanding Parameters\n",
    "5. Hyperparameter Tuning\n",
    "6. Feature Importance and Interpretation\n",
    "7. Advanced Techniques\n",
    "8. Model Persistence and Deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Installation\n",
    "\n",
    "First, let's install and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install xgboost scikit-learn pandas numpy matplotlib seaborn shap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"XGBoost Version: {xgb.__version__}\")\n",
    "print(\"Setup complete! âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Generate Realistic Dataset\n",
    "\n",
    "We'll create a realistic customer churn prediction dataset with:\n",
    "- Multiple feature types (numerical, categorical)\n",
    "- Missing values\n",
    "- Non-linear relationships\n",
    "- Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_customer_churn_data(n_samples=5000):\n",
    "    \"\"\"\n",
    "    Generate synthetic customer churn dataset\n",
    "    \"\"\"\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    \n",
    "    # Customer demographics\n",
    "    age = np.random.normal(45, 15, n_samples).clip(18, 80)\n",
    "    tenure_months = np.random.exponential(24, n_samples).clip(1, 72)\n",
    "    monthly_charges = np.random.normal(70, 30, n_samples).clip(20, 150)\n",
    "    total_charges = monthly_charges * tenure_months + np.random.normal(0, 100, n_samples)\n",
    "    \n",
    "    # Service usage patterns\n",
    "    num_products = np.random.poisson(2.5, n_samples).clip(1, 6)\n",
    "    support_calls = np.random.poisson(1.5, n_samples)\n",
    "    contract_type = np.random.choice(['Month-to-month', 'One year', 'Two year'], \n",
    "                                     n_samples, p=[0.5, 0.3, 0.2])\n",
    "    payment_method = np.random.choice(['Electronic check', 'Credit card', 'Bank transfer', 'Mailed check'],\n",
    "                                      n_samples, p=[0.35, 0.25, 0.25, 0.15])\n",
    "    \n",
    "    # Binary features\n",
    "    has_tech_support = np.random.binomial(1, 0.4, n_samples)\n",
    "    paperless_billing = np.random.binomial(1, 0.6, n_samples)\n",
    "    auto_payment = np.random.binomial(1, 0.45, n_samples)\n",
    "    \n",
    "    # Create churn based on multiple factors (complex relationships)\n",
    "    churn_probability = (\n",
    "        0.15 +  # Base churn rate\n",
    "        (contract_type == 'Month-to-month').astype(int) * 0.25 +\n",
    "        (payment_method == 'Electronic check').astype(int) * 0.15 +\n",
    "        (1 - has_tech_support) * 0.10 +\n",
    "        (support_calls > 3).astype(int) * 0.20 +\n",
    "        (tenure_months < 12).astype(int) * 0.25 +\n",
    "        (monthly_charges > 100).astype(int) * 0.10 +\n",
    "        (age < 30).astype(int) * 0.08 -\n",
    "        (num_products > 3).astype(int) * 0.15\n",
    "    )\n",
    "    \n",
    "    churn_probability = np.clip(churn_probability, 0.05, 0.85)\n",
    "    churn = np.random.binomial(1, churn_probability)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'customer_id': [f'CUST_{i:05d}' for i in range(n_samples)],\n",
    "        'age': age,\n",
    "        'tenure_months': tenure_months,\n",
    "        'monthly_charges': monthly_charges,\n",
    "        'total_charges': total_charges,\n",
    "        'num_products': num_products,\n",
    "        'support_calls': support_calls,\n",
    "        'contract_type': contract_type,\n",
    "        'payment_method': payment_method,\n",
    "        'has_tech_support': has_tech_support,\n",
    "        'paperless_billing': paperless_billing,\n",
    "        'auto_payment': auto_payment,\n",
    "        'churn': churn\n",
    "    })\n",
    "    \n",
    "    # Introduce some missing values (realistic scenario)\n",
    "    missing_mask = np.random.random(n_samples) < 0.05\n",
    "    df.loc[missing_mask, 'total_charges'] = np.nan\n",
    "    \n",
    "    missing_mask = np.random.random(n_samples) < 0.02\n",
    "    df.loc[missing_mask, 'age'] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_customer_churn_data(5000)\n",
    "\n",
    "print(\"Dataset generated successfully!\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nChurn rate: {df['churn'].mean():.2%}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Exploratory Data Analysis\n",
    "\n",
    "Let's understand our data before building models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_pct = (missing_counts / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the target variable and key features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Churn distribution\n",
    "ax = axes[0, 0]\n",
    "churn_counts = df['churn'].value_counts()\n",
    "ax.bar(['No Churn', 'Churn'], churn_counts.values, color=['lightblue', 'lightcoral'])\n",
    "ax.set_title('Churn Distribution', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Count')\n",
    "for i, v in enumerate(churn_counts.values):\n",
    "    ax.text(i, v + 50, f'{v}\\n({v/len(df):.1%})', ha='center')\n",
    "\n",
    "# Age distribution by churn\n",
    "ax = axes[0, 1]\n",
    "df[df['churn']==0]['age'].hist(bins=30, alpha=0.6, label='No Churn', ax=ax, color='blue')\n",
    "df[df['churn']==1]['age'].hist(bins=30, alpha=0.6, label='Churn', ax=ax, color='red')\n",
    "ax.set_title('Age Distribution by Churn', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "\n",
    "# Tenure vs Churn\n",
    "ax = axes[0, 2]\n",
    "df[df['churn']==0]['tenure_months'].hist(bins=30, alpha=0.6, label='No Churn', ax=ax, color='blue')\n",
    "df[df['churn']==1]['tenure_months'].hist(bins=30, alpha=0.6, label='Churn', ax=ax, color='red')\n",
    "ax.set_title('Tenure Distribution by Churn', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Tenure (months)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "\n",
    "# Contract type vs Churn\n",
    "ax = axes[1, 0]\n",
    "contract_churn = df.groupby('contract_type')['churn'].mean().sort_values()\n",
    "contract_churn.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_title('Churn Rate by Contract Type', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Churn Rate')\n",
    "for i, v in enumerate(contract_churn.values):\n",
    "    ax.text(v + 0.01, i, f'{v:.1%}', va='center')\n",
    "\n",
    "# Monthly charges vs Churn\n",
    "ax = axes[1, 1]\n",
    "df[df['churn']==0]['monthly_charges'].hist(bins=30, alpha=0.6, label='No Churn', ax=ax, color='blue')\n",
    "df[df['churn']==1]['monthly_charges'].hist(bins=30, alpha=0.6, label='Churn', ax=ax, color='red')\n",
    "ax.set_title('Monthly Charges by Churn', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Monthly Charges ($)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "\n",
    "# Support calls vs Churn\n",
    "ax = axes[1, 2]\n",
    "support_churn = df.groupby('support_calls')['churn'].mean()\n",
    "support_churn.plot(kind='bar', ax=ax, color='coral')\n",
    "ax.set_title('Churn Rate by Support Calls', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Number of Support Calls')\n",
    "ax.set_ylabel('Churn Rate')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Key Observations:\")\n",
    "print(f\"1. Churn rate is imbalanced: {df['churn'].mean():.1%}\")\n",
    "print(f\"2. Average tenure for churned customers: {df[df['churn']==1]['tenure_months'].mean():.1f} months\")\n",
    "print(f\"3. Average tenure for retained customers: {df[df['churn']==0]['tenure_months'].mean():.1f} months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Preprocessing\n",
    "\n",
    "### Exercise 1: Prepare the data for XGBoost\n",
    "\n",
    "**Your Task:**\n",
    "1. Separate features and target\n",
    "2. Handle categorical variables\n",
    "3. Split into train and test sets\n",
    "\n",
    "**Note:** XGBoost can handle missing values natively, so we'll keep them for now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "# Drop customer_id as it's not a feature\n",
    "df_model = df.drop('customer_id', axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_model.drop('churn', axis=1)\n",
    "y = df_model['churn']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "# XGBoost requires numerical inputs, so we'll use Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_columns = ['contract_type', 'payment_method']\n",
    "label_encoders = {}\n",
    "\n",
    "X_encoded = X.copy()\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"{col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "print(\"\\nEncoding complete!\")\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTrain churn rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test churn rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Building Your First XGBoost Model\n",
    "\n",
    "### Exercise 2: Train a basic XGBoost classifier\n",
    "\n",
    "Let's start with default parameters to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic XGBoost classifier\n",
    "basic_model = xgb.XGBClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='logloss'  # Suppress warning\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training basic XGBoost model...\")\n",
    "basic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = basic_model.predict(X_test)\n",
    "y_pred_proba = basic_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE MODEL RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "ax = axes[0]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_xticklabels(['No Churn', 'Churn'])\n",
    "ax.set_yticklabels(['No Churn', 'Churn'])\n",
    "\n",
    "# ROC Curve\n",
    "ax = axes[1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "ax.plot(fpr, tpr, linewidth=2, label=f'XGBoost (AUC = {auc_score:.3f})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=11)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=11)\n",
    "ax.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "ax = axes[2]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "ax.plot(recall, precision, linewidth=2, label='XGBoost')\n",
    "ax.set_xlabel('Recall', fontsize=11)\n",
    "ax.set_ylabel('Precision', fontsize=11)\n",
    "ax.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Understanding Model Parameters\n",
    "\n",
    "### Exercise 3: Experiment with key parameters\n",
    "\n",
    "Let's understand how different parameters affect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test different learning rates\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.3]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = xgb.XGBClassifier(\n",
    "        learning_rate=lr,\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    results.append({'learning_rate': lr, 'auc': auc})\n",
    "    print(f\"Learning Rate: {lr:.2f} â†’ AUC: {auc:.4f}\")\n",
    "\n",
    "# Visualize impact\n",
    "results_df = pd.DataFrame(results)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_df['learning_rate'], results_df['auc'], marker='o', linewidth=2, markersize=10)\n",
    "plt.xlabel('Learning Rate', fontsize=12)\n",
    "plt.ylabel('AUC Score', fontsize=12)\n",
    "plt.title('Impact of Learning Rate on Model Performance', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Best learning rate: {results_df.loc[results_df['auc'].idxmax(), 'learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different max_depth values\n",
    "max_depths = [3, 5, 7, 9, 11]\n",
    "depth_results = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=depth,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "    test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train, train_pred_proba)\n",
    "    test_auc = roc_auc_score(y_test, test_pred_proba)\n",
    "    \n",
    "    depth_results.append({\n",
    "        'max_depth': depth,\n",
    "        'train_auc': train_auc,\n",
    "        'test_auc': test_auc,\n",
    "        'overfit_gap': train_auc - test_auc\n",
    "    })\n",
    "    print(f\"Depth: {depth} â†’ Train AUC: {train_auc:.4f}, Test AUC: {test_auc:.4f}, Gap: {train_auc - test_auc:.4f}\")\n",
    "\n",
    "# Visualize overfitting\n",
    "depth_df = pd.DataFrame(depth_results)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(depth_df['max_depth'], depth_df['train_auc'], marker='o', linewidth=2, label='Train AUC', markersize=10)\n",
    "plt.plot(depth_df['max_depth'], depth_df['test_auc'], marker='s', linewidth=2, label='Test AUC', markersize=10)\n",
    "plt.xlabel('Max Depth', fontsize=12)\n",
    "plt.ylabel('AUC Score', fontsize=12)\n",
    "plt.title('Impact of Tree Depth on Overfitting', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâš ï¸ Watch for overfitting: When train AUC >> test AUC, the model is overfitting\")\n",
    "print(f\"âœ“ Recommended max_depth: {depth_df.loc[depth_df['test_auc'].idxmax(), 'max_depth']:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Using XGBoost's Native API\n",
    "\n",
    "### Exercise 4: Train with DMatrix and get more control\n",
    "\n",
    "XGBoost's native API (DMatrix) is more powerful and flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DMatrix objects\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=False)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=False)\n",
    "\n",
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'auc',\n",
    "    'seed': RANDOM_STATE\n",
    "}\n",
    "\n",
    "# Train with early stopping\n",
    "evals = [(dtrain, 'train'), (dtest, 'test')]\n",
    "evals_result = {}\n",
    "\n",
    "print(\"Training with early stopping...\\n\")\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=500,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=20,\n",
    "    evals_result=evals_result,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Best iteration: {bst.best_iteration}\")\n",
    "print(f\"âœ“ Best score: {bst.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Learning curves\n",
    "ax = axes[0]\n",
    "epochs = len(evals_result['train']['auc'])\n",
    "x_axis = range(0, epochs)\n",
    "ax.plot(x_axis, evals_result['train']['auc'], label='Train', linewidth=2)\n",
    "ax.plot(x_axis, evals_result['test']['auc'], label='Test', linewidth=2)\n",
    "ax.axvline(bst.best_iteration, color='red', linestyle='--', label='Best Iteration', linewidth=2)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylabel('AUC Score', fontsize=12)\n",
    "ax.set_xlabel('Boosting Round', fontsize=12)\n",
    "ax.set_title('Training Progress with Early Stopping', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions distribution\n",
    "ax = axes[1]\n",
    "y_pred_proba_native = bst.predict(dtest)\n",
    "ax.hist(y_pred_proba_native[y_test == 0], bins=50, alpha=0.6, label='No Churn', color='blue')\n",
    "ax.hist(y_pred_proba_native[y_test == 1], bins=50, alpha=0.6, label='Churn', color='red')\n",
    "ax.set_xlabel('Predicted Probability', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate final metrics\n",
    "y_pred_native = (y_pred_proba_native > 0.5).astype(int)\n",
    "final_auc = roc_auc_score(y_test, y_pred_proba_native)\n",
    "final_acc = accuracy_score(y_test, y_pred_native)\n",
    "\n",
    "print(f\"\\nFinal Test AUC: {final_auc:.4f}\")\n",
    "print(f\"Final Test Accuracy: {final_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Feature Importance Analysis\n",
    "\n",
    "### Exercise 5: Understand which features drive predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance using different metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "importance_types = ['weight', 'gain', 'cover']\n",
    "titles = ['Feature Importance (Frequency)', 'Feature Importance (Gain)', 'Feature Importance (Cover)']\n",
    "\n",
    "for ax, imp_type, title in zip(axes, importance_types, titles):\n",
    "    xgb.plot_importance(bst, ax=ax, importance_type=imp_type, max_num_features=10)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Feature Importance Metrics Explained:\")\n",
    "print(\"- Weight: How many times a feature is used in splits\")\n",
    "print(\"- Gain: Average improvement in accuracy when the feature is used\")\n",
    "print(\"- Cover: Average number of samples affected by splits using this feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed feature importance DataFrame\n",
    "importance_dict = bst.get_score(importance_type='gain')\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': importance_dict.keys(),\n",
    "    'Importance': importance_dict.values()\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Map back to original feature names\n",
    "feature_names = X_train.columns.tolist()\n",
    "feature_importance['Feature'] = feature_importance['Feature'].apply(lambda x: feature_names[int(x.replace('f', ''))])\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (by Gain):\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize top features\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importance.head(10)['Feature'], feature_importance.head(10)['Importance'], color='steelblue')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.title('Top 10 Features by Importance (Gain)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Hyperparameter Tuning\n",
    "\n",
    "### Exercise 6: Find optimal parameters using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Create model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Grid search with cross-validation\n",
    "print(\"Performing Grid Search (this may take a few minutes)...\\n\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GRID SEARCH RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best Score (AUC): {grid_search.best_score_:.4f}\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "best_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "best_auc = roc_auc_score(y_test, y_pred_proba_best)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OPTIMIZED MODEL RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Test AUC: {best_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "# Compare with baseline\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"IMPROVEMENT OVER BASELINE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Baseline AUC: {auc_score:.4f}\")\n",
    "print(f\"Optimized AUC: {best_auc:.4f}\")\n",
    "print(f\"Improvement: {(best_auc - auc_score):.4f} ({((best_auc - auc_score) / auc_score * 100):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Advanced Feature: SHAP Values\n",
    "\n",
    "### Exercise 7: Explain individual predictions (Optional - requires shap library)\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) helps explain how each feature contributes to a specific prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SHAP if not already installed\n",
    "try:\n",
    "    import shap\n",
    "    shap_available = True\n",
    "except ImportError:\n",
    "    print(\"SHAP not installed. Install with: pip install shap\")\n",
    "    print(\"Skipping SHAP analysis...\")\n",
    "    shap_available = False\n",
    "\n",
    "if shap_available:\n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    \n",
    "    # Calculate SHAP values for test set (sample for speed)\n",
    "    sample_size = min(100, len(X_test))\n",
    "    X_test_sample = X_test.iloc[:sample_size]\n",
    "    shap_values = explainer.shap_values(X_test_sample)\n",
    "    \n",
    "    print(f\"Calculated SHAP values for {sample_size} samples\")\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample, show=False)\n",
    "    plt.title('SHAP Summary Plot - Feature Impact on Predictions', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“Š SHAP Plot Interpretation:\")\n",
    "    print(\"- Each dot represents one prediction\")\n",
    "    print(\"- Color indicates feature value (red=high, blue=low)\")\n",
    "    print(\"- X-axis shows impact on prediction (positive = pushes toward churn)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_available:\n",
    "    # Explain a specific prediction\n",
    "    sample_idx = 0  # Change this to examine different predictions\n",
    "    \n",
    "    print(f\"Explaining prediction for customer at index {sample_idx}\")\n",
    "    print(f\"\\nCustomer features:\")\n",
    "    print(X_test_sample.iloc[sample_idx])\n",
    "    print(f\"\\nActual churn: {y_test.iloc[sample_idx]}\")\n",
    "    print(f\"Predicted churn probability: {best_model.predict_proba(X_test_sample.iloc[[sample_idx]])[:, 1][0]:.2%}\")\n",
    "    \n",
    "    # Force plot for individual prediction\n",
    "    shap.initjs()\n",
    "    shap.force_plot(\n",
    "        explainer.expected_value,\n",
    "        shap_values[sample_idx],\n",
    "        X_test_sample.iloc[sample_idx],\n",
    "        matplotlib=True,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Force Plot for Customer {sample_idx}', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“Š Force Plot Interpretation:\")\n",
    "    print(\"- Red features push prediction toward churn (positive impact)\")\n",
    "    print(\"- Blue features push prediction toward no churn (negative impact)\")\n",
    "    print(\"- Width of each feature shows its contribution magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Model Persistence\n",
    "\n",
    "### Exercise 8: Save and load your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "# Save the model\n",
    "model_filename = 'xgboost_churn_model.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save label encoders\n",
    "encoders_filename = 'label_encoders.pkl'\n",
    "with open(encoders_filename, 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "# Save feature names\n",
    "feature_names_file = 'feature_names.json'\n",
    "with open(feature_names_file, 'w') as f:\n",
    "    json.dump(list(X_train.columns), f)\n",
    "\n",
    "print(f\"âœ“ Model saved to: {model_filename}\")\n",
    "print(f\"âœ“ Encoders saved to: {encoders_filename}\")\n",
    "print(f\"âœ“ Feature names saved to: {feature_names_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and make predictions\n",
    "with open(model_filename, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "with open(encoders_filename, 'rb') as f:\n",
    "    loaded_encoders = pickle.load(f)\n",
    "\n",
    "with open(feature_names_file, 'r') as f:\n",
    "    loaded_features = json.load(f)\n",
    "\n",
    "# Test prediction with loaded model\n",
    "test_prediction = loaded_model.predict_proba(X_test.iloc[[0]])\n",
    "print(f\"\\nâœ“ Model loaded successfully!\")\n",
    "print(f\"Test prediction: {test_prediction[0][1]:.2%} probability of churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Making Predictions on New Data\n",
    "\n",
    "### Exercise 9: Create a prediction function for production use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_churn(customer_data, model, encoders, feature_names):\n",
    "    \"\"\"\n",
    "    Predict churn probability for new customer data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    customer_data : dict\n",
    "        Dictionary containing customer features\n",
    "    model : XGBoost model\n",
    "        Trained model\n",
    "    encoders : dict\n",
    "        Label encoders for categorical features\n",
    "    feature_names : list\n",
    "        List of feature names in correct order\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Prediction results with probability and recommendation\n",
    "    \"\"\"\n",
    "    # Create DataFrame from input\n",
    "    df = pd.DataFrame([customer_data])\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    for col, encoder in encoders.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = encoder.transform(df[col])\n",
    "    \n",
    "    # Ensure correct feature order\n",
    "    df = df[feature_names]\n",
    "    \n",
    "    # Make prediction\n",
    "    churn_prob = model.predict_proba(df)[0][1]\n",
    "    churn_pred = int(churn_prob > 0.5)\n",
    "    \n",
    "    # Generate recommendation\n",
    "    if churn_prob > 0.7:\n",
    "        risk_level = \"HIGH\"\n",
    "        recommendation = \"Immediate intervention required: Offer retention package or reach out to customer\"\n",
    "    elif churn_prob > 0.4:\n",
    "        risk_level = \"MEDIUM\"\n",
    "        recommendation = \"Monitor closely: Consider proactive engagement or special offers\"\n",
    "    else:\n",
    "        risk_level = \"LOW\"\n",
    "        recommendation = \"Maintain current engagement: Customer is likely to stay\"\n",
    "    \n",
    "    return {\n",
    "        'churn_probability': float(churn_prob),\n",
    "        'will_churn': bool(churn_pred),\n",
    "        'risk_level': risk_level,\n",
    "        'recommendation': recommendation\n",
    "    }\n",
    "\n",
    "# Test with a sample customer\n",
    "sample_customer = {\n",
    "    'age': 35,\n",
    "    'tenure_months': 8,\n",
    "    'monthly_charges': 85.50,\n",
    "    'total_charges': 684.0,\n",
    "    'num_products': 2,\n",
    "    'support_calls': 4,\n",
    "    'contract_type': 'Month-to-month',\n",
    "    'payment_method': 'Electronic check',\n",
    "    'has_tech_support': 0,\n",
    "    'paperless_billing': 1,\n",
    "    'auto_payment': 0\n",
    "}\n",
    "\n",
    "result = predict_churn(sample_customer, loaded_model, loaded_encoders, loaded_features)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHURN PREDICTION FOR NEW CUSTOMER\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nCustomer Profile:\")\n",
    "for key, value in sample_customer.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nPrediction Results:\")\n",
    "print(f\"  Churn Probability: {result['churn_probability']:.2%}\")\n",
    "print(f\"  Will Churn: {result['will_churn']}\")\n",
    "print(f\"  Risk Level: {result['risk_level']}\")\n",
    "print(f\"\\nRecommendation: {result['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: Cross-Validation\n",
    "\n",
    "### Exercise 10: Validate model robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Perform cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Performing 5-Fold Cross-Validation...\\n\")\n",
    "cv_scores = cross_val_score(\n",
    "    best_model,\n",
    "    X_encoded,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Cross-Validation Scores:\")\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"  Fold {i}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nMean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Visualize CV scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, 6), cv_scores, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "plt.axhline(cv_scores.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {cv_scores.mean():.4f}')\n",
    "plt.xlabel('Fold', fontsize=12)\n",
    "plt.ylabel('AUC Score', fontsize=12)\n",
    "plt.title('Cross-Validation Scores', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.ylim([cv_scores.min() - 0.01, cv_scores.max() + 0.01])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Data Preparation**\n",
    "   - Generated realistic datasets with various feature types\n",
    "   - Handled missing values (XGBoost does this automatically!)\n",
    "   - Encoded categorical variables\n",
    "\n",
    "2. **Model Building**\n",
    "   - Built baseline XGBoost models\n",
    "   - Used both scikit-learn API and native XGBoost API\n",
    "   - Implemented early stopping to prevent overfitting\n",
    "\n",
    "3. **Parameter Tuning**\n",
    "   - Understood impact of learning_rate, max_depth, etc.\n",
    "   - Performed grid search for optimal hyperparameters\n",
    "   - Balanced model complexity vs. performance\n",
    "\n",
    "4. **Model Interpretation**\n",
    "   - Analyzed feature importance\n",
    "   - Used SHAP values for prediction explanations\n",
    "   - Understood what drives model decisions\n",
    "\n",
    "5. **Production Deployment**\n",
    "   - Saved and loaded models\n",
    "   - Created prediction functions\n",
    "   - Validated with cross-validation\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Try with your own dataset\n",
    "2. Experiment with advanced features like custom objectives\n",
    "3. Compare XGBoost with other algorithms (Random Forest, LightGBM)\n",
    "4. Deploy your model as a web service\n",
    "5. Monitor model performance in production\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- XGBoost Documentation: https://xgboost.readthedocs.io/\n",
    "- Kaggle Competitions: Practice with real datasets\n",
    "- SHAP Documentation: https://shap.readthedocs.io/\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've completed the XGBoost tutorial! ðŸŽ‰**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Quick Reference Guide\n",
    "\n",
    "### Key XGBoost Parameters:\n",
    "\n",
    "```python\n",
    "xgb.XGBClassifier(\n",
    "    # Tree Parameters\n",
    "    max_depth=6,              # Maximum tree depth (3-10 typical)\n",
    "    min_child_weight=1,       # Minimum sum of instance weight in a child\n",
    "    gamma=0,                  # Minimum loss reduction for split\n",
    "    \n",
    "    # Boosting Parameters\n",
    "    learning_rate=0.1,        # Step size (0.01-0.3 typical)\n",
    "    n_estimators=100,         # Number of trees\n",
    "    \n",
    "    # Sampling Parameters\n",
    "    subsample=0.8,            # Fraction of samples per tree\n",
    "    colsample_bytree=0.8,     # Fraction of features per tree\n",
    "    colsample_bylevel=1.0,    # Fraction of features per level\n",
    "    \n",
    "    # Regularization\n",
    "    reg_alpha=0,              # L1 regularization\n",
    "    reg_lambda=1,             # L2 regularization\n",
    "    \n",
    "    # Other\n",
    "    random_state=42,\n",
    "    n_jobs=-1                 # Use all CPU cores\n",
    ")\n",
    "```\n",
    "\n",
    "### Common Patterns:\n",
    "\n",
    "**Overfitting?** â†’ Increase regularization (reg_alpha, reg_lambda) or decrease max_depth\n",
    "\n",
    "**Underfitting?** â†’ Increase max_depth or n_estimators\n",
    "\n",
    "**Slow training?** â†’ Decrease n_estimators, use subsample < 1.0\n",
    "\n",
    "**Imbalanced data?** â†’ Use scale_pos_weight parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Create a simple 2D dataset with normal points clustered together\n",
    "# and a few outliers\n",
    "np.random.seed(42)\n",
    "\n",
    "# Normal data: cluster around (5, 5)\n",
    "normal_points = np.random.normal(loc=5, scale=1, size=(20, 2))\n",
    "\n",
    "# Outliers: far from the cluster\n",
    "outliers = np.array([\n",
    "    [1, 1],   # Far from cluster\n",
    "    [9, 9],   # Far from cluster\n",
    "    [2, 8]    # Far from cluster\n",
    "])\n",
    "\n",
    "# Combine all points\n",
    "all_points = np.vstack([normal_points, outliers])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RANDOM CUT FOREST - SIMPLE EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nWe have {len(normal_points)} normal points clustered around (5,5)\")\n",
    "print(f\"and {len(outliers)} outlier points: {outliers.tolist()}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Simulate building ONE tree in the forest\n",
    "print(\"\\nSTEP 1: Create a bounding box around ALL data\")\n",
    "print(\"-\" * 60)\n",
    "x_min, y_min = all_points.min(axis=0)\n",
    "x_max, y_max = all_points.max(axis=0)\n",
    "print(f\"Bounding box: X=[{x_min:.2f}, {x_max:.2f}], Y=[{y_min:.2f}, {y_max:.2f}]\")\n",
    "\n",
    "# Show the cutting process\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Recursively cut the space until each point is isolated\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nCut 1: Random vertical cut at X=3.0\")\n",
    "print(\"  - Left side: 2 outlier points (1,1) and (2,8)\")\n",
    "print(\"  - Right side: 21 points (20 normal + 1 outlier at 9,9)\")\n",
    "print(\"  â†’ Outliers separated from main cluster!\")\n",
    "\n",
    "print(\"\\nCut 2: On the LEFT side, horizontal cut at Y=5.0\")\n",
    "print(\"  - Bottom: 1 point at (1,1)\")\n",
    "print(\"  - Top: 1 point at (2,8)\")\n",
    "print(\"  â†’ Outlier at (1,1) is NOW ISOLATED after 2 cuts!\")\n",
    "print(\"  â†’ Outlier at (2,8) is NOW ISOLATED after 2 cuts!\")\n",
    "\n",
    "print(\"\\nCut 3: On the right side, random vertical cut at X=8.5\")\n",
    "print(\"  - Left: 20 normal points\")\n",
    "print(\"  - Right: 1 outlier at (9,9)\")\n",
    "print(\"  â†’ Outlier at (9,9) is NOW ISOLATED after 2 cuts!\")\n",
    "\n",
    "print(\"\\nCuts 4-23: Keep cutting until all 20 normal points are isolated\")\n",
    "print(\"  â†’ Normal points require 10-18 cuts on average to isolate\")\n",
    "\n",
    "# Calculate depth scores\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Calculate anomaly scores\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nAnomaly Score = 1 / Average Depth Across All Trees\")\n",
    "print(\"\\nFor this single tree:\")\n",
    "print(\"  - Outlier at (1,1): depth=2  â†’ score = 0.5 (HIGH)\")\n",
    "print(\"  - Outlier at (2,8): depth=2  â†’ score = 0.5 (HIGH)\")\n",
    "print(\"  - Outlier at (9,9): depth=2  â†’ score = 0.5 (HIGH)\")\n",
    "print(\"  - Normal points:    depth=10-18 â†’ score = 0.05-0.10 (LOW)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Points that are ISOLATED QUICKLY (few cuts) = HIGH anomaly score\")\n",
    "print(\"Points that require MANY CUTS to isolate = LOW anomaly score\")\n",
    "print(\"\\nOutliers are far from other points, so they get isolated early!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Original data\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(normal_points[:, 0], normal_points[:, 1], \n",
    "           c='blue', s=100, alpha=0.6, label='Normal Points')\n",
    "ax1.scatter(outliers[:, 0], outliers[:, 1], \n",
    "           c='red', s=200, marker='s', label='Outliers', edgecolors='black', linewidths=2)\n",
    "ax1.set_xlabel('X', fontsize=12)\n",
    "ax1.set_ylabel('Y', fontsize=12)\n",
    "ax1.set_title('Original Data: Normal Points (blue) vs Outliers (red)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "\n",
    "# Plot 2: Show the cutting process\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(normal_points[:, 0], normal_points[:, 1], \n",
    "           c='blue', s=100, alpha=0.6, label='Normal (depth: 10-18)')\n",
    "ax2.scatter(outliers[:, 0], outliers[:, 1], \n",
    "           c='red', s=200, marker='s', label='Outliers (depth: 2)', \n",
    "           edgecolors='black', linewidths=2)\n",
    "\n",
    "# Draw the cuts\n",
    "ax2.axvline(x=3.0, color='green', linewidth=2, linestyle='--', label='Cut 1 (vertical)')\n",
    "ax2.axhline(y=5.0, xmax=0.3, color='orange', linewidth=2, linestyle='--', label='Cut 2 (horizontal)')\n",
    "\n",
    "# Annotate outliers with their depths\n",
    "ax2.annotate('Depth=2', xy=(1, 1), xytext=(0.5, 2.5),\n",
    "            fontsize=10, fontweight='bold', color='darkred',\n",
    "            arrowprops=dict(arrowstyle='->', color='darkred', lw=2))\n",
    "ax2.annotate('Depth=2', xy=(2, 8), xytext=(0.5, 6.5),\n",
    "            fontsize=10, fontweight='bold', color='darkred',\n",
    "            arrowprops=dict(arrowstyle='->', color='darkred', lw=2))\n",
    "ax2.annotate('Depth=2\\n(isolated from\\nmain cluster)', xy=(9, 9), xytext=(6.5, 8),\n",
    "            fontsize=10, fontweight='bold', color='darkred',\n",
    "            arrowprops=dict(arrowstyle='->', color='darkred', lw=2))\n",
    "\n",
    "ax2.set_xlabel('X', fontsize=12)\n",
    "ax2.set_ylabel('Y', fontsize=12)\n",
    "ax2.set_title('Random Cuts Isolate Outliers Faster', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=9, loc='lower right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rcf_visualization.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\nâœ“ Visualization saved!\")\n",
    "\n",
    "# Now show what happens with multiple trees\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WITH MULTIPLE TREES (e.g., 100 trees)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nEach tree makes different random cuts, so we average the scores:\")\n",
    "print(\"  - Outlier scores: consistently HIGH across all trees\")\n",
    "print(\"  - Normal scores: consistently LOW across all trees\")\n",
    "print(\"  â†’ Averaging reduces noise and improves detection!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-ml-engineer-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
