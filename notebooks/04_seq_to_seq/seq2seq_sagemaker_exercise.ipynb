{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker Sequence-to-Sequence Exercise\n",
    "\n",
    "This notebook walks you through training Amazon SageMaker's **Sequence-to-Sequence (Seq2Seq)** algorithm for machine translation.\n",
    "\n",
    "## What You'll Learn\n",
    "1. How to prepare data in Seq2Seq's required RecordIO-Protobuf format\n",
    "2. How to configure and train a Seq2Seq model on SageMaker\n",
    "3. How to deploy and query the model for translations\n",
    "4. How to evaluate translation quality\n",
    "\n",
    "## What is Sequence-to-Sequence?\n",
    "Seq2Seq is a supervised learning algorithm that transforms an input sequence of tokens into an output sequence of tokens. It uses an **encoder-decoder architecture** with attention mechanisms.\n",
    "\n",
    "**Common Use Cases:**\n",
    "- **Machine Translation**: Convert text from one language to another\n",
    "- **Text Summarization**: Condense longer text into shorter summaries\n",
    "- **Speech-to-Text**: Convert audio sequences to text\n",
    "\n",
    "**Architecture Options:**\n",
    "- **RNN-based**: Uses LSTM or GRU cells with attention\n",
    "- **CNN-based**: Uses convolutional layers (faster training)\n",
    "\n",
    "## Prerequisites\n",
    "- SageMaker notebook instance or Studio, or local environment with AWS credentials\n",
    "- IAM role with S3 and SageMaker permissions\n",
    "- **GPU instance required** (P2, P3, G4dn, or G5 family)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import struct\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS session from environment variables\n",
    "aws_profile = os.getenv('AWS_PROFILE')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE_ARN')\n",
    "\n",
    "if aws_profile:\n",
    "    boto3.setup_default_session(profile_name=aws_profile, region_name=aws_region)\n",
    "else:\n",
    "    boto3.setup_default_session(region_name=aws_region)\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Use environment variable for role, or fall back to execution role if running in SageMaker\n",
    "if sagemaker_role:\n",
    "    role = sagemaker_role\n",
    "else:\n",
    "    role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Profile: {aws_profile or 'default'}\")\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "PREFIX = \"seq2seq-translation\"\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_SAMPLES = 5000\n",
    "VAL_RATIO = 0.1\n",
    "MAX_SEQ_LEN = 50\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Generate Synthetic Translation Data\n",
    "\n",
    "We'll create a synthetic \"translation\" dataset that maps simple English patterns to a transformed version. This simulates a translation task without requiring a real bilingual corpus.\n",
    "\n",
    "**Note:** For real applications, you would use actual parallel corpora (e.g., English-German, English-French)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_translation_data(num_samples=5000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic \"translation\" pairs.\n",
    "    \n",
    "    We create simple English sentences and transform them\n",
    "    into a \"target language\" using consistent rules:\n",
    "    - Word order changes (subject-verb-object -> subject-object-verb)\n",
    "    - Word substitutions\n",
    "    - Prefix/suffix additions\n",
    "    \n",
    "    This simulates the structure of real translation without\n",
    "    requiring a bilingual corpus.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Vocabulary components\n",
    "    subjects = ['the cat', 'the dog', 'the bird', 'the man', 'the woman', \n",
    "                'the child', 'the teacher', 'the doctor', 'the student', 'the artist']\n",
    "    verbs = ['sees', 'likes', 'wants', 'finds', 'takes', \n",
    "             'helps', 'knows', 'loves', 'needs', 'gives']\n",
    "    objects = ['the ball', 'the book', 'the food', 'the water', 'the house',\n",
    "               'the car', 'the flower', 'the key', 'the phone', 'the pen']\n",
    "    adjectives = ['big', 'small', 'red', 'blue', 'old', 'new', 'fast', 'slow', 'good', 'bad']\n",
    "    adverbs = ['quickly', 'slowly', 'carefully', 'happily', 'sadly']\n",
    "    \n",
    "    # Translation mappings (simulating word-level translation)\n",
    "    word_map = {\n",
    "        'the': 'el', 'cat': 'gato', 'dog': 'perro', 'bird': 'pajaro',\n",
    "        'man': 'hombre', 'woman': 'mujer', 'child': 'nino', 'teacher': 'maestro',\n",
    "        'doctor': 'medico', 'student': 'estudiante', 'artist': 'artista',\n",
    "        'sees': 've', 'likes': 'gusta', 'wants': 'quiere', 'finds': 'encuentra',\n",
    "        'takes': 'toma', 'helps': 'ayuda', 'knows': 'conoce', 'loves': 'ama',\n",
    "        'needs': 'necesita', 'gives': 'da',\n",
    "        'ball': 'pelota', 'book': 'libro', 'food': 'comida', 'water': 'agua',\n",
    "        'house': 'casa', 'car': 'coche', 'flower': 'flor', 'key': 'llave',\n",
    "        'phone': 'telefono', 'pen': 'pluma',\n",
    "        'big': 'grande', 'small': 'pequeno', 'red': 'rojo', 'blue': 'azul',\n",
    "        'old': 'viejo', 'new': 'nuevo', 'fast': 'rapido', 'slow': 'lento',\n",
    "        'good': 'bueno', 'bad': 'malo',\n",
    "        'quickly': 'rapidamente', 'slowly': 'lentamente', 'carefully': 'cuidadosamente',\n",
    "        'happily': 'felizmente', 'sadly': 'tristemente'\n",
    "    }\n",
    "    \n",
    "    source_sentences = []\n",
    "    target_sentences = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Generate source sentence with random structure\n",
    "        structure = np.random.choice(['SVO', 'SVO_adj', 'SVO_adv', 'SVO_adj_adv'])\n",
    "        \n",
    "        subj = np.random.choice(subjects)\n",
    "        verb = np.random.choice(verbs)\n",
    "        obj = np.random.choice(objects)\n",
    "        adj = np.random.choice(adjectives)\n",
    "        adv = np.random.choice(adverbs)\n",
    "        \n",
    "        if structure == 'SVO':\n",
    "            source = f\"{subj} {verb} {obj}\"\n",
    "        elif structure == 'SVO_adj':\n",
    "            source = f\"{subj} {verb} the {adj} {obj.split()[-1]}\"\n",
    "        elif structure == 'SVO_adv':\n",
    "            source = f\"{subj} {adv} {verb} {obj}\"\n",
    "        else:\n",
    "            source = f\"{subj} {adv} {verb} the {adj} {obj.split()[-1]}\"\n",
    "        \n",
    "        # Translate to target\n",
    "        target_words = []\n",
    "        for word in source.split():\n",
    "            target_words.append(word_map.get(word, word))\n",
    "        target = ' '.join(target_words)\n",
    "        \n",
    "        source_sentences.append(source)\n",
    "        target_sentences.append(target)\n",
    "    \n",
    "    return source_sentences, target_sentences, word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "print(\"Generating synthetic translation data...\")\n",
    "source_sentences, target_sentences, word_map = generate_translation_data(NUM_SAMPLES, RANDOM_STATE)\n",
    "\n",
    "print(f\"\\nGenerated {len(source_sentences)} sentence pairs\")\n",
    "print(f\"\\nSample pairs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Source: {source_sentences[i]}\")\n",
    "    print(f\"  Target: {target_sentences[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "np.random.seed(RANDOM_STATE)\n",
    "indices = np.random.permutation(len(source_sentences))\n",
    "val_size = int(len(source_sentences) * VAL_RATIO)\n",
    "\n",
    "val_idx = indices[:val_size]\n",
    "train_idx = indices[val_size:]\n",
    "\n",
    "train_source = [source_sentences[i] for i in train_idx]\n",
    "train_target = [target_sentences[i] for i in train_idx]\n",
    "val_source = [source_sentences[i] for i in val_idx]\n",
    "val_target = [target_sentences[i] for i in val_idx]\n",
    "\n",
    "print(f\"Training set: {len(train_source)} pairs\")\n",
    "print(f\"Validation set: {len(val_source)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Build Vocabularies\n",
    "\n",
    "Seq2Seq requires:\n",
    "- Source vocabulary (`vocab.src.json`): Maps source language tokens to integers\n",
    "- Target vocabulary (`vocab.trg.json`): Maps target language tokens to integers\n",
    "\n",
    "**Special Tokens:**\n",
    "- `<pad>` (0): Padding token\n",
    "- `<unk>` (1): Unknown token\n",
    "- `<s>` (2): Start of sequence\n",
    "- `</s>` (3): End of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(sentences, min_freq=1):\n",
    "    \"\"\"\n",
    "    Build vocabulary from sentences.\n",
    "    \n",
    "    Returns a dictionary mapping tokens to integer IDs.\n",
    "    Special tokens are reserved at the beginning.\n",
    "    \"\"\"\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        word_counts.update(sentence.lower().split())\n",
    "    \n",
    "    # Start with special tokens\n",
    "    vocab = {\n",
    "        '<pad>': 0,\n",
    "        '<unk>': 1,\n",
    "        '<s>': 2,\n",
    "        '</s>': 3\n",
    "    }\n",
    "    \n",
    "    # Add words that meet minimum frequency\n",
    "    idx = 4\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Build vocabularies\n",
    "source_vocab = build_vocabulary(train_source)\n",
    "target_vocab = build_vocabulary(train_target)\n",
    "\n",
    "print(f\"Source vocabulary size: {len(source_vocab)}\")\n",
    "print(f\"Target vocabulary size: {len(target_vocab)}\")\n",
    "\n",
    "print(f\"\\nSample source vocab: {dict(list(source_vocab.items())[:10])}\")\n",
    "print(f\"Sample target vocab: {dict(list(target_vocab.items())[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, vocab, max_len=None):\n",
    "    \"\"\"\n",
    "    Convert sentence to list of integer token IDs.\n",
    "    \"\"\"\n",
    "    tokens = [vocab.get(word, vocab['<unk>']) for word in sentence.lower().split()]\n",
    "    \n",
    "    # Truncate if needed\n",
    "    if max_len and len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test tokenization\n",
    "sample_source = train_source[0]\n",
    "sample_tokens = tokenize_sentence(sample_source, source_vocab)\n",
    "print(f\"Original: {sample_source}\")\n",
    "print(f\"Tokenized: {sample_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Data in RecordIO-Protobuf Format\n",
    "\n",
    "SageMaker Seq2Seq requires data in RecordIO-Protobuf format with integer-encoded tokens.\n",
    "\n",
    "Each record contains:\n",
    "- Source sequence (integer tokens)\n",
    "- Target sequence (integer tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use SageMaker's built-in serialization utilities\n",
    "import io\n",
    "import struct\n",
    "\n",
    "def write_recordio(data, filename):\n",
    "    \"\"\"\n",
    "    Write data to RecordIO format.\n",
    "    \n",
    "    Each record is: [4-byte magic number][4-byte length][data][padding]\n",
    "    \"\"\"\n",
    "    # RecordIO magic number\n",
    "    RECORDIO_MAGIC = 0xCED7230A\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        for record in data:\n",
    "            # Serialize the record\n",
    "            record_bytes = record.encode('utf-8') if isinstance(record, str) else record\n",
    "            \n",
    "            # Write magic number and length\n",
    "            f.write(struct.pack('I', RECORDIO_MAGIC))\n",
    "            f.write(struct.pack('I', len(record_bytes)))\n",
    "            \n",
    "            # Write data\n",
    "            f.write(record_bytes)\n",
    "            \n",
    "            # Pad to 4-byte boundary\n",
    "            padding = (4 - len(record_bytes) % 4) % 4\n",
    "            f.write(b'\\x00' * padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Seq2Seq, we need to use the protobuf format\n",
    "# Let's use a simpler approach with the integer token files\n",
    "\n",
    "def create_seq2seq_data(source_sentences, target_sentences, source_vocab, target_vocab, max_len=50):\n",
    "    \"\"\"\n",
    "    Create parallel integer-encoded sequences for Seq2Seq.\n",
    "    \"\"\"\n",
    "    source_data = []\n",
    "    target_data = []\n",
    "    \n",
    "    for src, tgt in zip(source_sentences, target_sentences):\n",
    "        src_tokens = tokenize_sentence(src, source_vocab, max_len)\n",
    "        tgt_tokens = tokenize_sentence(tgt, target_vocab, max_len)\n",
    "        \n",
    "        source_data.append(src_tokens)\n",
    "        target_data.append(tgt_tokens)\n",
    "    \n",
    "    return source_data, target_data\n",
    "\n",
    "# Create tokenized data\n",
    "train_source_tokens, train_target_tokens = create_seq2seq_data(\n",
    "    train_source, train_target, source_vocab, target_vocab, MAX_SEQ_LEN\n",
    ")\n",
    "val_source_tokens, val_target_tokens = create_seq2seq_data(\n",
    "    val_source, val_target, source_vocab, target_vocab, MAX_SEQ_LEN\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_source_tokens)}\")\n",
    "print(f\"Validation samples: {len(val_source_tokens)}\")\n",
    "print(f\"\\nSample tokenized pair:\")\n",
    "print(f\"  Source tokens: {train_source_tokens[0]}\")\n",
    "print(f\"  Target tokens: {train_target_tokens[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Save vocabularies in JSON format (required by Seq2Seq)\n",
    "with open('data/vocab.src.json', 'w') as f:\n",
    "    json.dump(source_vocab, f, indent=2)\n",
    "\n",
    "with open('data/vocab.trg.json', 'w') as f:\n",
    "    json.dump(target_vocab, f, indent=2)\n",
    "\n",
    "print(\"Vocabularies saved:\")\n",
    "print(f\"  - data/vocab.src.json ({len(source_vocab)} tokens)\")\n",
    "print(f\"  - data/vocab.trg.json ({len(target_vocab)} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SageMaker Seq2Seq, we need to create RecordIO-Protobuf files\n",
    "# Using MXNet's recordio format\n",
    "\n",
    "try:\n",
    "    import mxnet as mx\n",
    "    MXNET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MXNET_AVAILABLE = False\n",
    "    print(\"MXNet not available. Using alternative format.\")\n",
    "\n",
    "def write_seq2seq_recordio(source_tokens, target_tokens, filename):\n",
    "    \"\"\"\n",
    "    Write Seq2Seq data to RecordIO format using MXNet.\n",
    "    \n",
    "    Each record contains source and target sequences as integer arrays.\n",
    "    \"\"\"\n",
    "    if not MXNET_AVAILABLE:\n",
    "        # Fallback: write as JSON lines for batch transform\n",
    "        with open(filename.replace('.rec', '.jsonl'), 'w') as f:\n",
    "            for src, tgt in zip(source_tokens, target_tokens):\n",
    "                record = {'source': src, 'target': tgt}\n",
    "                f.write(json.dumps(record) + '\\n')\n",
    "        return\n",
    "    \n",
    "    record_writer = mx.recordio.MXRecordIO(filename, 'w')\n",
    "    \n",
    "    for src, tgt in zip(source_tokens, target_tokens):\n",
    "        # Pack source and target as numpy arrays\n",
    "        src_array = np.array(src, dtype=np.int32)\n",
    "        tgt_array = np.array(tgt, dtype=np.int32)\n",
    "        \n",
    "        # Create header with shape info\n",
    "        header = mx.recordio.IRHeader(0, [len(src), len(tgt)], 0, 0)\n",
    "        \n",
    "        # Pack the record\n",
    "        packed = mx.recordio.pack(\n",
    "            header,\n",
    "            np.concatenate([src_array, tgt_array]).tobytes()\n",
    "        )\n",
    "        record_writer.write(packed)\n",
    "    \n",
    "    record_writer.close()\n",
    "\n",
    "# Write training and validation data\n",
    "write_seq2seq_recordio(train_source_tokens, train_target_tokens, 'data/train.rec')\n",
    "write_seq2seq_recordio(val_source_tokens, val_target_tokens, 'data/val.rec')\n",
    "\n",
    "if MXNET_AVAILABLE:\n",
    "    print(\"RecordIO files created:\")\n",
    "    print(f\"  - data/train.rec ({os.path.getsize('data/train.rec') / 1024:.1f} KB)\")\n",
    "    print(f\"  - data/val.rec ({os.path.getsize('data/val.rec') / 1024:.1f} KB)\")\n",
    "else:\n",
    "    print(\"JSON Lines files created (MXNet not available):\")\n",
    "    print(f\"  - data/train.jsonl\")\n",
    "    print(f\"  - data/val.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Upload training data\n",
    "if MXNET_AVAILABLE:\n",
    "    s3_client.upload_file('data/train.rec', BUCKET_NAME, f\"{PREFIX}/train/train.rec\")\n",
    "    s3_client.upload_file('data/val.rec', BUCKET_NAME, f\"{PREFIX}/validation/val.rec\")\n",
    "else:\n",
    "    s3_client.upload_file('data/train.jsonl', BUCKET_NAME, f\"{PREFIX}/train/train.jsonl\")\n",
    "    s3_client.upload_file('data/val.jsonl', BUCKET_NAME, f\"{PREFIX}/validation/val.jsonl\")\n",
    "\n",
    "# Upload vocabularies\n",
    "s3_client.upload_file('data/vocab.src.json', BUCKET_NAME, f\"{PREFIX}/vocab/vocab.src.json\")\n",
    "s3_client.upload_file('data/vocab.trg.json', BUCKET_NAME, f\"{PREFIX}/vocab/vocab.trg.json\")\n",
    "\n",
    "train_s3_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/train\"\n",
    "val_s3_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/validation\"\n",
    "vocab_s3_uri = f\"s3://{BUCKET_NAME}/{PREFIX}/vocab\"\n",
    "\n",
    "print(\"Data uploaded to S3:\")\n",
    "print(f\"  Train: {train_s3_uri}\")\n",
    "print(f\"  Validation: {val_s3_uri}\")\n",
    "print(f\"  Vocab: {vocab_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 5: Configure and Train the Seq2Seq Model\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "**Encoder/Decoder Architecture**\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `encoder_type` | Encoder architecture: `rnn` or `cnn` | rnn |\n",
    "| `decoder_type` | Decoder architecture: `rnn` or `cnn` | rnn |\n",
    "| `num_layers_encoder` | Number of encoder layers | 1 |\n",
    "| `num_layers_decoder` | Number of decoder layers | 1 |\n",
    "| `rnn_num_hidden` | Hidden units in RNN layers | 1024 |\n",
    "| `rnn_cell_type` | RNN cell type: `lstm` or `gru` | lstm |\n",
    "\n",
    "**Attention Mechanism**\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `rnn_attention_type` | Attention type: `dot`, `mlp`, `bilinear`, `fixed` | mlp |\n",
    "| `rnn_attention_num_hidden` | Hidden units in attention layer | rnn_num_hidden |\n",
    "\n",
    "**Embedding**\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `num_embed_source` | Source embedding dimension | 512 |\n",
    "| `num_embed_target` | Target embedding dimension | 512 |\n",
    "| `embed_dropout_source` | Source embedding dropout | 0 |\n",
    "| `embed_dropout_target` | Target embedding dropout | 0 |\n",
    "\n",
    "**Training**\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `batch_size` | Mini-batch size | 64 |\n",
    "| `learning_rate` | Initial learning rate | 0.0003 |\n",
    "| `optimizer_type` | Optimizer: `adam`, `sgd`, `rmsprop` | adam |\n",
    "| `max_num_epochs` | Maximum training epochs | 10 |\n",
    "| `clip_gradient` | Gradient clipping threshold | 1 |\n",
    "\n",
    "**Sequence Handling**\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `max_seq_len_source` | Maximum source sequence length | 100 |\n",
    "| `max_seq_len_target` | Maximum target sequence length | 100 |\n",
    "| `bucketing_enabled` | Enable sequence length bucketing | true |\n",
    "\n",
    "**Inference**\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `beam_size` | Beam search width for decoding | 5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Seq2Seq container image\n",
    "seq2seq_image = retrieve(\n",
    "    framework='seq2seq',\n",
    "    region=region,\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "print(f\"Seq2Seq Image URI: {seq2seq_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the estimator\n",
    "# NOTE: Seq2Seq requires GPU instances (P2, P3, G4dn, G5)\n",
    "seq2seq_estimator = Estimator(\n",
    "    image_uri=seq2seq_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',  # GPU required!\n",
    "    output_path=f's3://{BUCKET_NAME}/{PREFIX}/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='seq2seq-translation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "hyperparameters = {\n",
    "    # Encoder configuration\n",
    "    \"encoder_type\": \"rnn\",\n",
    "    \"num_layers_encoder\": 2,\n",
    "    \"rnn_num_hidden\": 256,\n",
    "    \"rnn_cell_type\": \"lstm\",\n",
    "    \n",
    "    # Decoder configuration\n",
    "    \"decoder_type\": \"rnn\",\n",
    "    \"num_layers_decoder\": 2,\n",
    "    \"rnn_attention_type\": \"mlp\",\n",
    "    \n",
    "    # Embedding\n",
    "    \"num_embed_source\": 256,\n",
    "    \"num_embed_target\": 256,\n",
    "    \"embed_dropout_source\": 0.1,\n",
    "    \"embed_dropout_target\": 0.1,\n",
    "    \n",
    "    # Sequence handling\n",
    "    \"max_seq_len_source\": MAX_SEQ_LEN,\n",
    "    \"max_seq_len_target\": MAX_SEQ_LEN,\n",
    "    \"bucketing_enabled\": \"true\",\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"optimizer_type\": \"adam\",\n",
    "    \"max_num_epochs\": 10,\n",
    "    \"clip_gradient\": 1.0,\n",
    "    \n",
    "    # Checkpointing\n",
    "    \"checkpoint_frequency_num_batches\": 500,\n",
    "    \"checkpoint_threshold\": 3,\n",
    "    \n",
    "    # Inference\n",
    "    \"beam_size\": 5,\n",
    "}\n",
    "\n",
    "seq2seq_estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "print(\"Hyperparameters configured:\")\n",
    "for k, v in hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data channels - Seq2Seq requires train, validation, and vocab channels\n",
    "data_channels = {\n",
    "    \"train\": train_s3_uri,\n",
    "    \"validation\": val_s3_uri,\n",
    "    \"vocab\": vocab_s3_uri\n",
    "}\n",
    "\n",
    "print(\"Starting training job...\")\n",
    "print(\"NOTE: This requires a GPU instance and will take 15-30 minutes.\\n\")\n",
    "\n",
    "# Start training\n",
    "seq2seq_estimator.fit(inputs=data_channels, wait=True, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "training_job_name = seq2seq_estimator.latest_training_job.name\n",
    "print(f\"Training job completed: {training_job_name}\")\n",
    "print(f\"Model artifacts: {seq2seq_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Step 6: Deploy the Model\n",
    "\n",
    "Unlike batch-oriented algorithms, Seq2Seq is typically deployed for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy to an endpoint\n",
    "print(\"Deploying model to endpoint...\")\n",
    "print(\"This will take approximately 5-7 minutes.\\n\")\n",
    "\n",
    "predictor = seq2seq_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',  # CPU is fine for inference\n",
    "    endpoint_name=f'seq2seq-translation-{datetime.now().strftime(\"%Y%m%d%H%M\")}'\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint deployed: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Step 7: Make Predictions\n",
    "\n",
    "Seq2Seq accepts JSON input with the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Configure predictor\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "def translate(text):\n",
    "    \"\"\"\n",
    "    Translate text using the deployed model.\n",
    "    \"\"\"\n",
    "    # Seq2Seq expects the source text as space-separated tokens\n",
    "    request = {\"instances\": [{\"data\": text}]}\n",
    "    response = predictor.predict(request)\n",
    "    return response\n",
    "\n",
    "# Test translations\n",
    "test_sentences = [\n",
    "    \"the cat sees the ball\",\n",
    "    \"the dog likes the food\",\n",
    "    \"the woman helps the child\",\n",
    "    \"the teacher quickly finds the book\",\n",
    "    \"the artist loves the big flower\"\n",
    "]\n",
    "\n",
    "print(\"Translation Results:\")\n",
    "print(\"=\" * 60)\n",
    "for sentence in test_sentences:\n",
    "    result = translate(sentence)\n",
    "    print(f\"Source:  {sentence}\")\n",
    "    print(f\"Target:  {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Translation Quality\n",
    "\n",
    "### Understanding Translation Metrics\n",
    "\n",
    "**BLEU Score (Bilingual Evaluation Understudy)**\n",
    "- Standard metric for machine translation\n",
    "- Measures n-gram overlap between prediction and reference\n",
    "- Range: 0-100 (higher is better)\n",
    "- Interpretation:\n",
    "  - < 10: Almost useless\n",
    "  - 10-19: Hard to understand\n",
    "  - 20-29: Clear meaning, grammatical errors\n",
    "  - 30-40: Understandable, good quality\n",
    "  - 40-50: High quality\n",
    "  - 50-60: Very high quality\n",
    "  - > 60: Better than human (rare)\n",
    "\n",
    "**Exact Match**\n",
    "- Percentage of predictions exactly matching reference\n",
    "- Very strict metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def calculate_bleu(reference, candidate, max_n=4):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score between reference and candidate.\n",
    "    \"\"\"\n",
    "    ref_tokens = reference.lower().split()\n",
    "    cand_tokens = candidate.lower().split()\n",
    "    \n",
    "    # Calculate n-gram precisions\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_ngrams = Counter([tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens)-n+1)])\n",
    "        cand_ngrams = Counter([tuple(cand_tokens[i:i+n]) for i in range(len(cand_tokens)-n+1)])\n",
    "        \n",
    "        matches = sum((ref_ngrams & cand_ngrams).values())\n",
    "        total = sum(cand_ngrams.values())\n",
    "        \n",
    "        if total > 0:\n",
    "            precisions.append(matches / total)\n",
    "        else:\n",
    "            precisions.append(0)\n",
    "    \n",
    "    # Brevity penalty\n",
    "    if len(cand_tokens) == 0:\n",
    "        return 0\n",
    "    bp = min(1, math.exp(1 - len(ref_tokens) / len(cand_tokens)))\n",
    "    \n",
    "    # Geometric mean of precisions\n",
    "    if all(p > 0 for p in precisions):\n",
    "        log_precisions = [math.log(p) for p in precisions]\n",
    "        bleu = bp * math.exp(sum(log_precisions) / len(log_precisions))\n",
    "    else:\n",
    "        bleu = 0\n",
    "    \n",
    "    return bleu * 100  # Return as percentage\n",
    "\n",
    "# Evaluate on validation set\n",
    "bleu_scores = []\n",
    "exact_matches = 0\n",
    "\n",
    "print(\"Evaluating on validation set...\")\n",
    "for i, (src, ref) in enumerate(zip(val_source[:100], val_target[:100])):\n",
    "    try:\n",
    "        result = translate(src)\n",
    "        pred = result.get('predictions', [{}])[0].get('target', '')\n",
    "        \n",
    "        bleu = calculate_bleu(ref, pred)\n",
    "        bleu_scores.append(bleu)\n",
    "        \n",
    "        if pred.lower().strip() == ref.lower().strip():\n",
    "            exact_matches += 1\n",
    "        \n",
    "        if i < 5:\n",
    "            print(f\"\\nSource: {src}\")\n",
    "            print(f\"Reference: {ref}\")\n",
    "            print(f\"Predicted: {pred}\")\n",
    "            print(f\"BLEU: {bleu:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        continue\n",
    "\n",
    "if bleu_scores:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Average BLEU: {np.mean(bleu_scores):.2f}\")\n",
    "    print(f\"Median BLEU: {np.median(bleu_scores):.2f}\")\n",
    "    print(f\"Exact Match: {exact_matches}/{len(bleu_scores)} ({100*exact_matches/len(bleu_scores):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Step 9: Clean Up Resources\n",
    "\n",
    "**IMPORTANT**: Always delete endpoints when done to avoid ongoing charges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "print(f\"Deleting endpoint: {predictor.endpoint_name}\")\n",
    "predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, clean up S3 data\n",
    "# Uncomment the following lines if you want to delete the S3 data\n",
    "\n",
    "# import boto3\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket = s3.Bucket(BUCKET_NAME)\n",
    "# bucket.objects.filter(Prefix=PREFIX).delete()\n",
    "# print(f\"Deleted all objects under s3://{BUCKET_NAME}/{PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **Data Format**: Seq2Seq requires RecordIO-Protobuf format with integer-encoded tokens, plus JSON vocabulary files\n",
    "\n",
    "2. **Three Required Channels**:\n",
    "   - `train`: Training data\n",
    "   - `validation`: Validation data\n",
    "   - `vocab`: Source and target vocabulary files (`vocab.src.json`, `vocab.trg.json`)\n",
    "\n",
    "3. **Key Hyperparameters**:\n",
    "   - `encoder_type`, `decoder_type`: Architecture choice (rnn or cnn)\n",
    "   - `rnn_num_hidden`: Hidden units (model capacity)\n",
    "   - `num_layers_encoder`, `num_layers_decoder`: Depth\n",
    "   - `rnn_attention_type`: Attention mechanism\n",
    "   - `beam_size`: Decoding beam width\n",
    "\n",
    "4. **Hardware Requirements**:\n",
    "   - **GPU required** for training (P2, P3, G4dn, G5)\n",
    "   - CPU can be used for inference\n",
    "\n",
    "5. **Evaluation**: Use BLEU score for translation quality\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Use Case | Description |\n",
    "|----------|-------------|\n",
    "| Machine Translation | Translate text between languages |\n",
    "| Text Summarization | Generate summaries from documents |\n",
    "| Question Answering | Generate answers from context |\n",
    "| Chatbots | Generate responses to user queries |\n",
    "| Code Generation | Generate code from descriptions |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different encoder/decoder architectures (CNN is faster)\n",
    "- Experiment with attention types (dot, mlp, bilinear)\n",
    "- Use larger hidden dimensions for complex tasks\n",
    "- Add more training data for better generalization\n",
    "- Try transfer learning from pre-trained models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
